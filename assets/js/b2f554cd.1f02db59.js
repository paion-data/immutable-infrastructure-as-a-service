"use strict";(self.webpackChunkhashicorp_aws=self.webpackChunkhashicorp_aws||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"testing-hashicorp-terraform","metadata":{"permalink":"/blog/testing-hashicorp-terraform","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/testing-hashicorp-terraform/index.md","source":"@site/blog/testing-hashicorp-terraform/index.md","title":"Testing HashiCorp Terraform","description":"How do we know if we can run terraform apply to our infrastructure without negatively affecting critical business","date":"2024-07-10T08:56:43.000Z","formattedDate":"July 10, 2024","tags":[{"label":"HashiCorp","permalink":"/blog/tags/hashi-corp"},{"label":"Terraform","permalink":"/blog/tags/terraform"}],"readingTime":1.885,"hasTruncateMarker":false,"authors":[{"name":"ROSEMARY WANG","url":"https://www.hashicorp.com/blog/testing-hashicorp-terraform","imageURL":"https://www.hashicorp.com/_next/static/media/terraform_on-dark.8e9a65a1.svg","key":"rose-wang"}],"frontMatter":{"slug":"testing-hashicorp-terraform","title":"Testing HashiCorp Terraform","authors":["rose-wang"],"tags":["HashiCorp","Terraform"]},"unlisted":false,"nextItem":{"title":"Bumping Semantic Version by GitHub Tag","permalink":"/blog/version-bump"}},"content":"How do we know if we can run `terraform apply` to our infrastructure without negatively affecting critical business\\napplications? We can run `terraform validate` and `terraform plan` to check our configuration, but will that be enough?\\nWhether we\'ve updated some HashiCorp Terraform configuration or a new version of a module, we want to catch errors\\nquickly before we apply any changes to production infrastructure.\\n\\nIn this post, We will discuss some testing strategies for HashiCorp Terraform configuration and modules so that we can\\n`terraform apply` with greater confidence.\\n\\nThe Testing Pyramid\\n-------------------\\n\\nIn theory, we might decide to align our infrastructure testing strategy with the test pyramid, which groups tests by\\ntype, scope, and granularity. The testing pyramid suggests that we write fewer tests in the categories at the top of the\\npyramid, and more at the bottom. Those on the pyramid take more time to run and cost more due to the higher number of\\nresources we have to configure and create.\\n\\n![Error loading pyramid.png](./pyramid.png)\\n\\nIn reality, our tests may not perfectly align with the pyramid shape. The pyramid offers a common framework to describe\\nwhat scope a test can cover to verify configuration and infrastructure resources. We\'ll start at the bottom of the\\npyramid with unit tests and work the way up the pyramid to end-to-end tests.\\n\\n:::note\\n\\nhashicorp-aws does not merit any manual testing; so it is not discussed here\\n\\n:::\\n\\nLinting and Formatting\\n----------------------\\n\\nWhile not on the test pyramid, we often encounter tests to verify the hygiene of your Terraform configuration. Use\\n`terraform fmt -check` and terraform validate to format and validate the correctness of our Terraform configuration.\\n\\nWhen we collaborate on Terraform, we may consider testing the Terraform configuration for a set of standards and best\\npractices. Build or use a linting tool to analyze our Terraform configuration for specific best practices and patterns.\\nFor example, a linter can verify that our teammate defines a Terraform variable for an instance type instead of\\nhard-coding the value.\\n\\nUnit Tests\\n----------\\n\\nAt the bottom of the pyramid, unit tests verify individual resources and configurations for expected values. They should\\nanswer the question, \u201cDoes my configuration or plan contain the correct metadata?\u201d Traditionally, unit tests should run\\nindependently, without external resources or API calls."},{"id":"version-bump","metadata":{"permalink":"/blog/version-bump","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-19-version-bump/index.md","source":"@site/blog/2024-04-19-version-bump/index.md","title":"Bumping Semantic Version by GitHub Tag","description":"We offer a convenient versioning management approach for releasing software on GitHub.","date":"2024-04-19T00:00:00.000Z","formattedDate":"April 19, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Frontend","permalink":"/blog/tags/frontend"}],"readingTime":0.81,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"version-bump","title":"Bumping Semantic Version by GitHub Tag","authors":["jiaqi"],"tags":["CI/CD","GitHub","Frontend"]},"unlisted":false,"prevItem":{"title":"Testing HashiCorp Terraform","permalink":"/blog/testing-hashicorp-terraform"},"nextItem":{"title":"Cypress E2E Tests","permalink":"/blog/cypress-e2e"}},"content":"We offer a convenient versioning management approach for releasing software on GitHub.\\n\\n\x3c!--truncate--\x3e\\n\\n:::warning\\n\\nThe GitHub repository must grant *write* access to this workflow permission, i.e. navigate to **Settings** ->\\n**Actions** -> **General**; under *Workflow permissions* make sure **Read and write permissions** is checked:\\n\\n![Error loading permissions.png](./permission.png)\\n\\n:::\\n\\n1. Manually create the first tag:\\n\\n   ```shell\\n   git tag -a v0.0.1 -m \\"v0.0.1\\"\\n   git push origin v0.0.1\\n   ```\\n\\n2. Create a pull request that adds the following job to GitHub Action\\n\\n  ```yaml\\n  \\"on\\":\\n    pull_request:\\n    push:\\n      branches:\\n        - master\\n\\n  jobs:\\n    push-release-tag:\\n      if: github.ref == \'refs/heads/master\'\\n      uses: QubitPi/hashicorp-aws/.github/workflows/version-bump.yml@master\\n      with:\\n        user: QubitPi\\n        email: jack20220723@gmail.com\\n  ```\\n\\n3. When the pull request is merged, the version bump action will automatically create and push a new version tag of\\n   `MAJOR`.`MINOR`.(`PATCH` + 1)\\n\\n:::tip\\n\\nBumping the `MAJOR` or `MINOR` version still needs to be done manually using `git tag -a vx.x.x -m \\"vx.x.x\\"` command\\ngiven the assumption that agile software development will change patch version most frequently and almost always\\n\\n:::"},{"id":"cypress-e2e","metadata":{"permalink":"/blog/cypress-e2e","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-14-cypress-e2e.md","source":"@site/blog/2024-04-14-cypress-e2e.md","title":"Cypress E2E Tests","description":"This action assumes the yarn package manager is used.","date":"2024-04-14T00:00:00.000Z","formattedDate":"April 14, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Frontend","permalink":"/blog/tags/frontend"}],"readingTime":1.865,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"cypress-e2e","title":"Cypress E2E Tests","authors":["jiaqi"],"tags":["CI/CD","GitHub","Frontend"]},"unlisted":false,"prevItem":{"title":"Bumping Semantic Version by GitHub Tag","permalink":"/blog/version-bump"},"nextItem":{"title":"Setting up JDK in GitHub Actions","permalink":"/blog/jdk-setup"}},"content":":::info\\n\\nThis action assumes the [yarn package manager](https://yarnpkg.com/) is used.\\n\\n:::\\n\\nCypress E2E action offers an easy way to automate, customize, and execute **parallel** end-to-end tests. The action\\nprovides\\n\\n- dependency installation via **yarn**,\\n- scanning of test specs,\\n- running each spec in parallel, and\\n- upload test screenshots and video on test failure.\\n\\nThe example below is a very simple setup:\\n\\n1. Install Cypress with `yarn add cypress --dev`\\n2. Initialize Cypress with `yarn run cypress open`\\n3. [Support TypeScript](https://qubitpi.github.io/cypress-documentation/guides/tooling/typescript-support/)\\n4. Put all **.spec.cy.ts** test files under \\"cypress/e2e\\" directory\\n5. Install [wait-on]: `yarn add -D wait-on`\\n6. Add the following script command to `package.json`:\\n\\n   ```json\\n   {\\n     ...\\n\\n     \\"scripts\\": {\\n       \\"e2e\\": \\"cypress run --browser chrome\\",\\n       \\"wait-on-dev\\": \\"wait-on http-get://localhost:3000/\\",\\n       \\"wait-on-prod\\": \\"wait-on http-get://localhost:3000/\\"\\n     },\\n\\n     ...\\n   }\\n   ```\\n\\n   :::info\\n\\n   Note that we assume the UI is running at port 3000. Please adjust it accordingly if it\'s running at a different port.\\n\\n   :::\\n\\n8. Use Cypress E2E Tests workflow:\\n\\n   ```yaml\\n   ---\\n   name: CI/CD\\n\\n   \\"on\\":\\n     pull_request:\\n     push:\\n       branches:\\n         - master\\n\\n     e2e-tests:\\n       name: Unit Tests\\n       needs: unit-tests\\n       uses: QubitPi/hashicorp-aws/.github/workflows/cypress-e2e.yml@master\\n   ```\\n\\n   In the example above, the node 18 is used in the CI/CDed project by default. A list of custom node versions can be\\n   used to replace the default. For example, to run E2E tests in node 16, 18, and 20, simply use **node-versions**\\n   parameter:\\n\\n   ```yaml\\n   ---\\n     e2e-tests:\\n       name: Unit Tests\\n       needs: unit-tests\\n       uses: QubitPi/hashicorp-aws/.github/workflows/cypress-e2e.yml@master\\n       with:\\n         node-versions: \'[\\"16\\", \\"18\\", \\"20\\"]\'\\n   ```\\n\\n:::tip\\n\\nInside the `cypress-e2e` workflow, each [Cypress spec] is tested in 2 modes:\\n\\n1. **yarn-start**: the web app is started using `yarn start`\\n2. **server**: a production build is generated first using `yarn build` and then the web app is started with\\n   `yarn serve`\\n\\nThe reason we run the same E2E in 2 separate modes is that we assume E2E testing consists of 2 logical parts:\\n\\n1. The logical tests defined by Cypress spec files\\n2. The same tests in the context of integration of web app logic and the production runtime github-actions-core\\n\\nThe app may work perfectly fine in E2E, but it\'s a different question when the same app is packaged up using,\\nfor example, [webpack](https://webpack.js.org/). The later could also be interpreted as integration tests against\\nwebpack configuration which makes the tests more comprehensive\\n\\n:::\\n\\n[wait-on]: https://www.npmjs.com/package/wait-on"},{"id":"jdk-setup","metadata":{"permalink":"/blog/jdk-setup","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-14-jdk-setup.md","source":"@site/blog/2024-04-14-jdk-setup.md","title":"Setting up JDK in GitHub Actions","description":"Installing JDK 17","date":"2024-04-14T00:00:00.000Z","formattedDate":"April 14, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Backend","permalink":"/blog/tags/backend"},{"label":"Java","permalink":"/blog/tags/java"}],"readingTime":0.335,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"jdk-setup","title":"Setting up JDK in GitHub Actions","authors":["jiaqi"],"tags":["CI/CD","GitHub","Backend","Java"]},"unlisted":false,"prevItem":{"title":"Cypress E2E Tests","permalink":"/blog/cypress-e2e"},"nextItem":{"title":"NPM Release action","permalink":"/blog/npm-release"}},"content":"Installing JDK 17\\n-----------------\\n\\nThe standard [actions/setup-java](https://github.com/actions/setup-java) requires us to specify JDK distributions other\\nthan JDK version. Looking up JDK distributions wastes user\'s time and gives opportunities to error.\\n\\nWe offer a no-config action that installs JDK 17 by default. The usage is as follows:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  tests:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Set up JDK\\n        uses: QubitPi/hashicorp-aws/.github/actions/jdk-setup@master\\n```"},{"id":"npm-release","metadata":{"permalink":"/blog/npm-release","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-14-npm-release.md","source":"@site/blog/2024-04-14-npm-release.md","title":"NPM Release action","description":"This action works for both [npm] and [yarn] package managers","date":"2024-04-14T00:00:00.000Z","formattedDate":"April 14, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Frontend","permalink":"/blog/tags/frontend"},{"label":"NPM","permalink":"/blog/tags/npm"}],"readingTime":0.52,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"npm-release","title":"NPM Release action","authors":["jiaqi"],"tags":["CI/CD","GitHub","Frontend","NPM"]},"unlisted":false,"prevItem":{"title":"Setting up JDK in GitHub Actions","permalink":"/blog/jdk-setup"},"nextItem":{"title":"UI Unit Test","permalink":"/blog/ui-unit-test"}},"content":":::tip\\n\\nThis action works for both [npm] and [yarn] package managers\\n\\n:::\\n\\nThe NPM release action bundles up a React/Vue package and publishes it to [npm registry][npm].\\n\\nTo use the release action,\\n[create a GitHub Secret](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository)\\nfor [npm token](https://docs.npmjs.com/creating-and-viewing-access-tokens), which will be used to authenticate against\\nNPM in the action. Then use the following template in CI/CD:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\nenv:\\n  NODE_VERSION: 18\\n\\njobs:\\n  publish:\\n    name: Publish Package to NPM\\n    if: github.ref == \'refs/heads/master\'\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: QubitPi/hashicorp-aws/.github/actions/npm-release.yml@master\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          npm-token: ${{ env.NPM_TOKEN }}\\n          user: Qubitpi\\n          email: jack20220723@gmail.com\\n```\\n\\n[npm]: https://www.npmjs.com/\\n[yarn]: https://yarnpkg.com/"},{"id":"ui-unit-test","metadata":{"permalink":"/blog/ui-unit-test","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-14-ui-unit-test.md","source":"@site/blog/2024-04-14-ui-unit-test.md","title":"UI Unit Test","description":"The UI unit test action runs unit tests and assumes the yarn package manager and requires a test script to be","date":"2024-04-14T00:00:00.000Z","formattedDate":"April 14, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Frontend","permalink":"/blog/tags/frontend"}],"readingTime":0.54,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"ui-unit-test","title":"UI Unit Test","authors":["jiaqi"],"tags":["CI/CD","GitHub","Frontend"]},"unlisted":false,"prevItem":{"title":"NPM Release action","permalink":"/blog/npm-release"},"nextItem":{"title":"UI Code Style","permalink":"/blog/ui-code-style"}},"content":"The UI unit test action runs unit tests and assumes the **yarn** package manager and requires a `test` script to be\\ndefined in projects `package.json` file. For example, the following uses [Jest] as the unit test runner:\\n\\n```json\\n{\\n  \\"scripts\\": {\\n    \\"test\\": \\"jest\\"\\n  }\\n}\\n```\\n\\nTo use this action, import it in the following way:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\n  unit-tests:\\n    name: Unit Tests\\n    uses: QubitPi/hashicorp-aws/.github/workflows/ui-unit-test.yml@master\\n    with:\\n      node-version: 18\\n```\\n\\n:::tip\\n\\nIn the example above, the node 18 is used in the CI/CDed project.\\n\\n:::\\n\\n:::tip\\n\\nThe example above uses Node version 18, which is specified in `NODE_VERSION`\\n[environment variable](https://docs.github.com/en/actions/learn-github-actions/variables#defining-environment-variables-for-a-single-workflow)\\n\\n:::\\n\\n[Jest]: https://jestjs.io/"},{"id":"ui-code-style","metadata":{"permalink":"/blog/ui-code-style","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-13-ui-code-style.md","source":"@site/blog/2024-04-13-ui-code-style.md","title":"UI Code Style","description":"In Frontend dev realm, there are lots of code style checker. Assembling all of them together takes efforts and pains.","date":"2024-04-13T00:00:00.000Z","formattedDate":"April 13, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Frontend","permalink":"/blog/tags/frontend"}],"readingTime":1.42,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"ui-code-style","title":"UI Code Style","authors":["jiaqi"],"tags":["CI/CD","GitHub","Frontend"]},"unlisted":false,"prevItem":{"title":"UI Unit Test","permalink":"/blog/ui-unit-test"},"nextItem":{"title":"Performing Style Check on YAML & Markdown Files and Link Check","permalink":"/blog/yml-and-md-style-checks"}},"content":"In Frontend dev realm, there are lots of code style checker. Assembling all of them together takes efforts and pains.\\nThis action runs the following two code style checker specifically for frontend dev:\\n\\n1. [Prettier]\\n2. [ESLint]\\n\\nThis action assume [ESLint], [typescript-eslint], and [Prettier] have been installed, which can be done with:\\n\\n```bash\\nyarn add --dev @typescript-eslint/parser @typescript-eslint/eslint-plugin eslint typescript\\nyarn add --dev --exact prettier\\n```\\n\\nHere is an example usage of the action:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\n  code-style:\\n    name: React & TS Code Style Check\\n    uses: QubitPi/hashicorp-aws/.github/workflows/ui-code-style.yml@master\\n    with:\\n      node-version: 18\\n```\\n\\n:::tip\\n\\nIn the example above, the node 18 is used in the CI/CDed project.\\n\\n:::\\n\\nThe configurations of Prettier and ESLint can be done regularly by following their respective documentations. For\\nexample, the [.prettierrc.json](https://qubitpi.github.io/prettier/docs/en/configuration) and\\n[.prettierignore](https://qubitpi.github.io/prettier/docs/en/ignore.html) can be placed at the project root with the\\nfollowing contents:\\n\\n```json title=\\".prettierrc.json\\"\\n{\\n  \\"tabWidth\\": 2,\\n  \\"useTabs\\": false,\\n  \\"printWidth\\": 120\\n}\\n```\\n\\n```ignore title=\\".prettierignore\\"\\n*.md\\n*.mdx\\nbuild\\ncoverage\\nnode_modules\\n```\\n\\n:::tip\\n\\nWe can fix it by formatting all files at the root of project with:\\n\\n```bash\\nyarn prettier . --write\\n```\\n\\n:::\\n\\nInitial ESLint configuration template can be generated with\\n\\n```bash\\nyarn run eslint --init # https://dev.to/maithanhdanh/configuration-for-eslint-b47\\n```\\n\\n:::info[Prettier & ESLint Conflict]\\n\\nLinters usually contain not only code quality rules, but also stylistic rules. Most stylistic rules are unnecessary\\nwhen using Prettier, but worse - they might conflict with Prettier! Use Prettier for code formatting concerns, and\\nlinters for code-quality concerns, as outlined in\\n[Prettier vs. Linters](https://qubitpi.github.io/prettier/docs/en/comparison).\\n\\nLuckily it\'s easy to turn off rules that conflict or are unnecessary with Prettier, by using these pre-made configs:\\n\\n- [eslint-config-prettier](https://github.com/prettier/eslint-config-prettier)\\n\\n```bash\\nyarn add --dev eslint-config-prettier\\n```\\n\\n:::\\n\\n[ESLint]: https://eslint.org/\\n\\n[Prettier]: https://prettier.io/\\n\\n[typescript-eslint]: https://typescript-eslint.io/"},{"id":"yml-and-md-style-checks","metadata":{"permalink":"/blog/yml-and-md-style-checks","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-04-08-yml-md-style-and-link-checks.mdx","source":"@site/blog/2024-04-08-yml-md-style-and-link-checks.mdx","title":"Performing Style Check on YAML & Markdown Files and Link Check","description":"Inspired by Sous Chefs, hashicorp-aws","date":"2024-04-08T00:00:00.000Z","formattedDate":"April 8, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"},{"label":"Code Style","permalink":"/blog/tags/code-style"}],"readingTime":2.51,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"yml-and-md-style-checks","title":"Performing Style Check on YAML & Markdown Files and Link Check","authors":["jiaqi"],"tags":["CI/CD","GitHub","Code Style"]},"unlisted":false,"prevItem":{"title":"UI Code Style","permalink":"/blog/ui-code-style"},"nextItem":{"title":"Reusable GitHub Action - Posting GitHub Secrets","permalink":"/blog/github-secret-action"}},"content":"Inspired by [Sous Chefs](https://github.com/sous-chefs/.github/blob/main/.github/workflows/lint-unit.yml), hashicorp-aws\\noffers a [reusable workflow](https://docs.github.com/en/actions/using-workflows/reusing-workflows) that performs the\\nfollowing code style checks:\\n\\n1. [YAML file style check]\\n2. [Markdown file style check]\\n3. [Broken link check]\\n\\nExample Usage:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  yml-md-style-and-link-checks:\\n    uses: QubitPi/hashicorp-aws/.github/workflows/yml-md-style-and-link-checks.yml@master\\n```\\n\\n:::tip\\n\\nThe example above is all we need to run the 3 checks. The workflow has default configurations, which can be\\n[overridden](#optional-overriding-default-configurations)\\n\\nThe configurations of the composing checks can be configured regularly by following their respective GitHub Actions\\ndocumentations. The following sections discusses the configuration by example.\\n\\n:::\\n\\n(Optional) Overriding Default Configurations\\n--------------------------------------------\\n\\n### YAML File Style Check\\n\\nThe default YAML style configurations is\\n\\n```yaml\\n---\\nextends: default\\nrules:\\n  line-length:\\n    max: 256\\n    level: warning\\n  document-start: disable\\n  braces:\\n    forbid: false\\n    min-spaces-inside: 0\\n    max-spaces-inside: 1\\n    min-spaces-inside-empty: -1\\n    max-spaces-inside-empty: -1\\n```\\n\\nTo override the default configuration, create a file named **.yamllint** at the root of the downstream project and\\nconfigure the workflow with `use-custom-yamllint-config-file` option set to `true`. For example\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  yml-md-style-and-link-checks:\\n    uses: QubitPi/hashicorp-aws/.github/workflows/yml-md-style-and-link-checks.yml@master\\n    with:\\n      use-custom-yamllint-config-file: true\\n```\\n\\n:::tip\\n\\nMore configuration options can be found at [yamllint documentation](https://yamllint.readthedocs.io/en/stable/)\\n\\n:::\\n\\n### Markdown File Style Check\\n\\nThe configurations of markdown file style check are splitted into 2 config files whose default configurations\\nare\\n\\nimport Tabs from \'@theme/Tabs\';\\nimport TabItem from \'@theme/TabItem\';\\n\\n<Tabs>\\n  <TabItem value=\\".mdlrc\\" label=\\".mdlrc\\" default>\\n    ```\\n    rules \\"~MD002\\", \\"~MD003\\", \\"~MD005\\", \\"~MD007\\", \\"~MD013\\", \\"~MD022\\", \\"~MD024\\", \\"~MD029\\", \\"~MD033\\", \\"~MD034\\", \\"~MD036\\", \\"~MD041\\"\\n    style \\"#{File.dirname(__FILE__)}/markdownlint.rb\\"\\n    ```\\n\\n:::tip\\n\\nIn the example above, the first line above *excludes*\\n[specified rules](https://github.com/markdownlint/markdownlint/blob/main/docs/RULES.md). The second line specifies the\\nrule configuration file (`markdownlint.rb`). For more native config options, please refer to\\n[its documentations](https://github.com/markdownlint/markdownlint/blob/main/docs/configuration.md)\\n\\n:::\\n\\n  </TabItem>\\n  <TabItem value=\\"markdownlint.rb\\" label=\\"markdownlint.rb\\">\\n\\n    We may need to adjust certain settings of some single rule by having another file named `markdownlint.rb`:\\n\\n    ```ruby\\n    all\\n    rule \'MD003\', style: :setext_with_atx\\n    rule \'MD004\', style: :sublist\\n    rule \'MD013\', line_length: 120\\n    rule \'MD029\', style: :ordered\\n    ```\\n\\n:::tip\\n\\nMore info about rule config can be found in\\n[its documentation](https://github.com/markdownlint/markdownlint/blob/main/docs/creating_rules.md) and\\n[its comprehensive rule list](https://github.com/markdownlint/markdownlint/blob/main/docs/RULES.md)\\n\\n:::\\n  </TabItem>\\n</Tabs>\\n\\nCreate files named `.mdlrc` and `markdownlint.rb` at the root of the project and add `use-custom-mdlrc-config-file` and\\n`use-custom-markdownlint-config-file` options to the workflow file like so:\\n\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  yml-md-style-and-link-checks:\\n    uses: QubitPi/hashicorp-aws/.github/workflows/yml-md-style-and-link-checks.yml@master\\n    with:\\n      use-custom-mdlrc-config-file: true\\n      use-custom-markdownlint-config-file: true\\n```\\n\\n### Broken Link Check\\n\\nThe [Broken link check] pretty much configures everything for us, so we don\'t need to configure anything unless we\\nneed to exclude\\n[links](https://github.com/lycheeverse/lychee-action#excluding-links-from-getting-checked) or\\n[file](https://github.com/lycheeverse/lychee-action) by regular expression. hashicorp-aws defaults to exclude all\\nrelative file links with the following default:\\n\\n```ignore title=\\".lycheeignore\\"\\nfile:///*\\n```\\n\\n:::info\\n\\nThe ignore rule in the example above skips checks of all relative links among files. This is common in\\n[Docusaurus](https://docusaurus.io/docs/markdown-features/links)-based documentation\\n\\n:::\\n\\nIf we don\'t need such default, we would simply create a `.lycheeignore` file at our project root and setting\\n`use-custom-lycheeignore-file` to `true`:\\n\\n```yaml\\nname: CI/CD\\n\\n\\"on\\":\\n  pull_request:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  yml-md-style-and-link-checks:\\n    uses: QubitPi/hashicorp-aws/.github/workflows/yml-md-style-and-link-checks.yml@master\\n    with:\\n      use-custom-lycheeignore-file: true\\n```\\n\\n[YAML file style check]: https://github.com/actionshub/yamllint\\n[Markdown file style check]: https://github.com/actionshub/markdownlint\\n[Broken link check]: https://github.com/lycheeverse/lychee-action"},{"id":"github-secret-action","metadata":{"permalink":"/blog/github-secret-action","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-02-23-github-secret/index.md","source":"@site/blog/2024-02-23-github-secret/index.md","title":"Reusable GitHub Action - Posting GitHub Secrets","description":"Overview","date":"2024-02-23T00:00:00.000Z","formattedDate":"February 23, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"}],"readingTime":1.77,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"github-secret-action","title":"Reusable GitHub Action - Posting GitHub Secrets","authors":["jiaqi"],"tags":["CI/CD","GitHub"]},"unlisted":false,"prevItem":{"title":"Performing Style Check on YAML & Markdown Files and Link Check","permalink":"/blog/yml-and-md-style-checks"},"nextItem":{"title":"Sending GitHub Action Results to Slack Channel","permalink":"/blog/github-slack-notification"}},"content":"Overview\\n--------\\n\\nBeing a strong proponent of Immutable Infrastructure, [hashicorp-aws] is constantly pushing the limits of its ability\\nin various use cases, one of which is the _Configuration Management_\\n\\nTraditional configuration management includes Chef, Puppet, and Ansible. They all assume mutable infrastructure being\\npresent. For example, Chef has a major component responsible for jumping into a VM, checking if config has been mutated\\nbefore apply any operations.\\n\\nWith the adoption of Immutable infrastructure, we initially stored and managed our configuration, such as SSL\\ncertificate or AWS SECRET ACCESS KEY directly in GitHub Secrets. This has the disadvantage of not being able to see\\ntheir values after creation, making it very hard to manage.\\n\\nThen we moved to a centralized runbook, where everything can easily be seen and modified by authorized team members. In\\nthis approache, CI/CD server will pull down the entire runbook and simply pick up the config files. This, however,\\nexposed a great security risk because illegal usage could simply leak any credentials to public by `cat`ing that\\ncredential file out\\n\\nSo the problem, or what [hashicorp-aws] is trying to solve here, is\\n\\n- being able to keep credentials, whether it\'s string values or values stored in files, **secure**, and\\n- allowing team member to easily **manage** those credentials\\n\\n:::note\\n\\nWe tried HashiCorp Vault but\\n[it doesn\'t support storing file credential](https://discuss.hashicorp.com/t/how-to-store-a-file-content-in-hashicorp-kv-secret-engine-as-value-through-cmd-line-or-script/46895/2),\\n[hashicorp-aws] addressed exactly how file can be managed in this case\\n\\n:::\\n\\nSo this brought us to the alternative way of thinking about Configuration Management in Immutable Infrastructure, which\\nis depicted below:\\n\\n![](github-secret.png)\\n\\nWe still need GitHub Secrets because our tech dev has a deep integratin with it and that\'s the most secure way to pass\\nour organization credentials around.\\n\\nIn addition, we will also keep runbook for config management. The runbook will be hosted separately, not in GitHub\\nSecrets.\\n\\n:::info\\n\\nRunbooks was used in Yahoo that keeps all DevOps credentials in a dedicated GitHub private repo. It\'s been proven to\\nbe an effective way to manage and share a software configurations within a team.\\n\\n:::\\n\\n[hashicorp-aws]\'s github-secret now comes into play to bridge the gap between two componet.\\n\\n[hashicorp-aws]: https://qubitpi.github.io/hashicorp-aws/"},{"id":"github-slack-notification","metadata":{"permalink":"/blog/github-slack-notification","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2024-01-27-github-slack-notification/index.md","source":"@site/blog/2024-01-27-github-slack-notification/index.md","title":"Sending GitHub Action Results to Slack Channel","description":"Sending data into Slack using slack-send.","date":"2024-01-27T00:00:00.000Z","formattedDate":"January 27, 2024","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"Slack","permalink":"/blog/tags/slack"},{"label":"Team Efficiency","permalink":"/blog/tags/team-efficiency"}],"readingTime":2.125,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"github-slack-notification","title":"Sending GitHub Action Results to Slack Channel","authors":["jiaqi"],"tags":["CI/CD","Slack","Team Efficiency"]},"unlisted":false,"prevItem":{"title":"Reusable GitHub Action - Posting GitHub Secrets","permalink":"/blog/github-secret-action"},"nextItem":{"title":"Using a GitHub Action Matrix to Define Variations for Each Job","permalink":"/blog/github-matrix"}},"content":"Sending data into Slack using [slack-send](https://github.com/marketplace/actions/slack-send).\\n\\n\x3c!--truncate--\x3e\\n\\nslack-send\\n----------\\n\\nThis post discusses practical guide for sending data to Slack via a **Slack Incoming Webhook URL**. The reason we prefer\\nwebhook approach is to preserve the privacy of team member. The chatbot invite approach potentially\\nallow all Slack member to touch a private app of an individual\\n\\n1. Follow the [setup](https://github.com/slackapi/slack-github-action?tab=readme-ov-file#setup-2)\\n2. Add `slack-notification` job and a trigger in the last CI/CD job:\\n\\n   ```yaml title=.github/workflows/ci-cd.yml\\n   ---\\n   name: My CI/CD\\n\\n   jobs:\\n     the-last-job:\\n       name: The last CI/CD job in workflow\\n       outputs:\\n         outcome: ${{ job.status }}\\n       continue-on-error: true\\n       runs-on: ubuntu-latest\\n       steps:\\n         ...\\n\\n     slack-notification:\\n       name: Send Slack Notification\\n       if: ${{ always() }}\\n       needs: the-last-job\\n       uses: QubitPi/hashicorp-aws/.github/workflows/slack-notification.yml@master\\n       with:\\n         job-status: ${{ needs.the-last-job.outputs.outcome }}\\n       secrets:\\n         slack-webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}\\n   ```\\n\\n   :::info\\n\\n   Note the key block in `the-last-job`:\\n\\n   ```yaml\\n   outputs:\\n     outcome: ${{ job.status }}\\n   continue-on-error: true\\n   ```\\n\\n   :::\\n\\n### Multi-Terminal-Job Configuration\\n\\nThe config above applies to the case with a single last job. We call this job \\"terminal job\\". The configuration becomes\\na little different when there are multiple terminal jobs such as the one workflow in the figure below:\\n\\n![Error loading slack-notification-multi-terminal-jobs.png](./slack-notification-multi-terminal-jobs.png)\\n\\nIn this case we change the `job-status` input of `slack-notification`:\\n\\n```yaml title=.github/workflows/ci-cd.yml\\n---\\nname: My CI/CD\\n\\njobs:\\n  terminal-job-a:\\n    name: Terminal Job A\\n    outputs:\\n      outcome: ${{ job.status }}\\n    continue-on-error: true\\n    runs-on: ubuntu-latest\\n    steps:\\n      ...\\n\\n  terminal-job-b:\\n    name: Terminal Job B\\n    outputs:\\n      outcome: ${{ job.status }}\\n    continue-on-error: true\\n    runs-on: ubuntu-latest\\n    steps:\\n      ...\\n\\n  slack-notification:\\n    name: Send Slack Notification\\n    if: ${{ always() }}\\n    needs: [terminal-job-a, terminal-job-b]\\n    uses: QubitPi/hashicorp-aws/.github/workflows/slack-notification.yml@master\\n    with:\\n      job-status: ${{ (needs.terminal-job-a.outputs.outcome == \'success\' && needs.terminal-job-b.outputs.outcome == \'success\') && \'success\' || \'failure\' }}\\n    secrets:\\n      slack-webhook-url: ${{ secrets.SLACK_WEBHOOK_URL }}\\n```\\n\\nNote we are using the [ternary expression in GitHub Actions] for the value of `job-status` above.\\n\\nslack-send v.s. Github Slack Integration\\n----------------------------------------\\n\\n[Github Slack Integration] offers \\"on-click\\" experience with almost no configurations like the one above. In addition,\\nit sends notification on GitHub issues open/close, pull requests, GitHub Action workflow _status_, etc. The easier setup\\nand richer notifications makes [Github Slack Integration] a better choice for team-collaboration managing multiple\\nrepositories with a common communication standards\\n\\nslack-send, on the other hands, supports sending any information about GitHub Action workflow, not just _status_. It\\nalso supports custom messages; we can inject emoji or custom pictures in the notification messges, which is a big plus\\nfor those who love personal customizations and focuses on only success-or-fail of each GitHub action run.\\n\\n:::info\\n\\nWith slack-send we also do not need to run `/invite @GitHub` so our channel is completely private.\\n\\n:::\\n\\n[Github Slack Integration]: https://github.com/integrations/slack\\n\\n[ternary expression in GitHub Actions]: https://7tonshark.com/posts/github-actions-ternary-operator/"},{"id":"github-matrix","metadata":{"permalink":"/blog/github-matrix","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2023-11-24-github-matrix.md","source":"@site/blog/2023-11-24-github-matrix.md","title":"Using a GitHub Action Matrix to Define Variations for Each Job","description":"A matrix strategy lets you use variables in a single job definition to automatically create multiple job runs that are","date":"2023-11-24T00:00:00.000Z","formattedDate":"November 24, 2023","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"},{"label":"GitHub","permalink":"/blog/tags/git-hub"}],"readingTime":7.82,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"github-matrix","title":"Using a GitHub Action Matrix to Define Variations for Each Job","authors":["jiaqi"],"tags":["CI/CD","GitHub"]},"unlisted":false,"prevItem":{"title":"Sending GitHub Action Results to Slack Channel","permalink":"/blog/github-slack-notification"},"nextItem":{"title":"Using OpenSSL to encrypt messages and files on Linux","permalink":"/blog/openssl-encrypt"}},"content":"A matrix strategy lets you use variables in a single job definition to automatically create multiple job runs that are\\nbased on the combinations of the variables. For example, you can use a matrix strategy to test your code in multiple\\nversions of a language or on multiple operating systems.\\n\\n\x3c!--truncate--\x3e\\n\\nUsing a Matrix Strategy\\n-----------------------\\n\\nUse `jobs.<job_id>.strategy.matrix` to define a matrix of different job configurations. Within your matrix, define one\\nor more variables followed by an array of values. For example, the following matrix has a variable called `version` with\\nthe value `[10, 12, 14]` and a variable called os with the value `[ubuntu-latest, windows-latest]`:\\n\\n```yaml\\njobs:\\n  example_matrix:\\n    strategy:\\n      matrix:\\n        version: [10, 12, 14]\\n        os: [ubuntu-latest, windows-latest]\\n```\\n\\nA job will run for each possible combination of the variables. In this example, the workflow will run six jobs, one for\\neach combination of the `os` and `version` variables.\\n\\nBy default, GitHub will maximize the number of jobs run in parallel depending on runner availability. The **order of the\\nvariables in the matrix determines the order in which the jobs are created**. The first variable you define will be the\\nfirst job that is created in your workflow run. For example, the above matrix will create the jobs in the following\\norder:\\n\\n- `{version: 10, os: ubuntu-latest}`\\n- `{version: 10, os: windows-latest}`\\n- `{version: 12, os: ubuntu-latest}`\\n- `{version: 12, os: windows-latest}`\\n- `{version: 14, os: ubuntu-latest}`\\n- `{version: 14, os: windows-latest}`\\n\\nA matrix will generate a maximum of **256** jobs per workflow run. This limit applies to both GitHub-hosted and\\nself-hosted runners.\\n\\nThe variables that you define become properties in the matrix context, and you can reference the property in other areas\\nof your workflow file. In this example, you can use `matrix.version` and `matrix.os` to access the current value of\\nversion and os that the job is using. For more information, see \\"[Contexts][Contexts].\\"\\n\\nExample: Using a Single-Dimension Matrix\\n----------------------------------------\\n\\nYou can specify a single variable to create a single-dimension matrix.\\n\\nFor example, the following workflow defines the variable version with the values `[10, 12, 14]`. The workflow will run\\nthree jobs, one for each value in the variable. Each job will access the version value through the `matrix.version`\\ncontext and pass the value as `node-version` to the actions/setup-node action.\\n\\nExample: Using a Multi-dimension Matrix\\n---------------------------------------\\n\\nYou can specify multiple variables to create a multi-dimensional matrix. A job will run for each possible combination of\\nthe variables.\\n\\nFor example, the following workflow specifies two variables:\\n\\n- Two operating systems specified in the `os` variable\\n- Three Node.js versions specified in the `version` variable\\n\\nThe workflow will run six jobs, one for each combination of the `os` and `version` variables. Each job will set the\\n`runs-on` value to the current `os` value and will pass the current version value to the `actions/setup-node` action.\\n\\n```yaml\\njobs:\\n  example_matrix:\\n    strategy:\\n      matrix:\\n        os: [ubuntu-22.04, ubuntu-20.04]\\n        version: [10, 12, 14]\\n    runs-on: ${{ matrix.os }}\\n    steps:\\n      - uses: actions/setup-node@v3\\n        with:\\n          node-version: ${{ matrix.version }}\\n```\\n\\nExample: Using Contexts to Create Matrices\\n------------------------------------------\\n\\nYou can use contexts to create matrices. For more information about contexts, see \\"[Contexts][Contexts].\\"\\n\\nFor example, the following workflow triggers on the `repository_dispatch` event and uses information from the event\\npayload to build the matrix. When a repository dispatch event is created with a payload like the one below, the matrix\\n`version` variable will have a value of `[12, 14, 16]`. For more information about the `repository_dispatch` trigger,\\nsee \\"[Events that trigger workflows][Events that trigger workflows].\\"\\n\\n```json\\n{\\n    \\"event_type\\": \\"test\\",\\n    \\"client_payload\\": {\\n        \\"versions\\": [12, 14, 16]\\n    }\\n}\\n```\\n\\n```yaml\\non:\\n  repository_dispatch:\\n    types:\\n      - test\\n\\njobs:\\n  example_matrix:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        version: ${{ github.event.client_payload.versions }}\\n    steps:\\n      - uses: actions/setup-node@v3\\n        with:\\n          node-version: ${{ matrix.version }}\\n```\\n\\nExpanding or adding matrix configurations\\n-----------------------------------------\\n\\nUse `jobs.<job_id>.strategy.matrix.include` to expand existing matrix configurations or to add new configurations. The\\nvalue of include is a list of objects.\\n\\nFor each object in the `include` list, the key:value pairs in the object will be added to each of the matrix\\ncombinations if none of the key:value pairs overwrite any of the original matrix values. If the object cannot be added\\nto any of the matrix combinations, a new matrix combination will be created instead. Note that the original matrix\\nvalues will not be overwritten, but added matrix values can be overwritten.\\n\\nFor example, this matrix:\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    fruit: [apple, pear]\\n    animal: [cat, dog]\\n    include:\\n      - color: green\\n      - color: pink\\n        animal: cat\\n      - fruit: apple\\n        shape: circle\\n      - fruit: banana\\n      - fruit: banana\\n        animal: cat\\n```\\n\\nwill result in six jobs with the following matrix combinations:\\n\\n- `{fruit: apple, animal: cat, color: pink, shape: circle}`\\n- `{fruit: apple, animal: dog, color: green, shape: circle}`\\n- `{fruit: pear, animal: cat, color: pink}`\\n- `{fruit: pear, animal: dog, color: green}`\\n- `{fruit: banana}`\\n- `{fruit: banana, animal: cat}`\\n\\nfollowing this logic:\\n\\n- `{color: green}` is added to all of the original matrix combinations because it can be added without overwriting any\\n  part of the original combinations.\\n- `{color: pink, animal: cat}` adds `color:pink` only to the original matrix combinations that include `animal: cat`.\\n  This overwrites the color: green that was added by the previous include entry.\\n- `{fruit: apple, shape: circle}` adds shape: circle only to the original matrix combinations that include\\n  `fruit: apple`.\\n- `{fruit: banana}` cannot be added to any original matrix combination without overwriting a value, so it is added as an\\n  additional matrix combination.\\n- `{fruit: banana, animal: cat}` cannot be added to any original matrix combination without overwriting a value, so it\\n  is added as an additional matrix combination. It does not add to the `{fruit: banana}` matrix combination because that\\n  combination was not one of the original matrix combinations.\\n\\nThis looks like kinda \\"what\'re we doing here???\\". But let us just look at the following examples which make it more\\nclear.\\n\\nThe following workflow will run six jobs, one for each combination of `os` and node. When the job for the os value of\\n`windows-latest` and node value of `16` runs, an additional variable called `npm` with the value of `6` will be included\\nin the job.\\n\\n```yaml\\njobs:\\n  example_matrix:\\n    strategy:\\n      matrix:\\n        os: [windows-latest, ubuntu-latest]\\n        node: [12, 14, 16]\\n        include:\\n          - os: windows-latest\\n            node: 16\\n            npm: 6\\n    runs-on: ${{ matrix.os }}\\n    steps:\\n      - uses: actions/setup-node@v3\\n        with:\\n          node-version: ${{ matrix.node }}\\n      - if: ${{ matrix.npm }}\\n        run: npm install -g npm@${{ matrix.npm }}\\n      - run: npm --version\\n```\\n\\nThis matrix will run 10 jobs, one for each combination of `os` and `version` in the matrix, plus a job for the `os`\\nvalue of `windows-latest` and version value of `17`.\\n\\n```yaml\\njobs:\\n  example_matrix:\\n    strategy:\\n      matrix:\\n        os: [macos-latest, windows-latest, ubuntu-latest]\\n        version: [12, 14, 16]\\n        include:\\n          - os: windows-latest\\n            version: 17\\n```\\n\\nIf you don\'t specify any matrix variables, all configurations under `include` will run. For example, the following\\nworkflow would run two jobs, one for each `include` entry. This lets you take advantage of the matrix strategy without\\nhaving a fully populated matrix.\\n\\n```yaml\\njobs:\\n  includes_only:\\n    runs-on: ubuntu-latest\\n    strategy:\\n      matrix:\\n        include:\\n          - site: \\"production\\"\\n            datacenter: \\"site-a\\"\\n          - site: \\"staging\\"\\n            datacenter: \\"site-b\\"\\n```\\n\\nExcluding Matrix Configurations\\n-------------------------------\\n\\nTo remove specific configurations defined in the matrix, use `jobs.<job_id>.strategy.matrix.exclude`. An excluded\\nconfiguration only has to be a partial match for it to be excluded. For example, the following workflow will run nine\\njobs: one job for each of the 12 configurations, minus the one excluded job that matches\\n`{os: macos-latest, version: 12, environment: production}`, and the two excluded jobs that match\\n`{os: windows-latest, version: 16}`.\\n\\n```yaml\\nstrategy:\\n  matrix:\\n    os: [macos-latest, windows-latest]\\n    version: [12, 14, 16]\\n    environment: [staging, production]\\n    exclude:\\n      - os: macos-latest\\n        version: 12\\n        environment: production\\n      - os: windows-latest\\n        version: 16\\nruns-on: ${{ matrix.os }}\\n```\\n\\n> Note: All `include` combinations are processed after `exclude`. This allows you to use `include` to add back\\n> combinations that were previously excluded.\\n\\nHandling Failures\\n-----------------\\n\\nYou can control how job failures are handled with `jobs.<job_id>.strategy.fail-fast` and\\n`jobs.<job_id>.continue-on-error`.\\n\\n`jobs.<job_id>.strategy.fail-fast` applies to the entire matrix. If `jobs.<job_id>.strategy.fail-fast` is set to `true`,\\nGitHub will cancel all in-progress and queued jobs in the matrix if any job in the matrix fails. This property defaults\\nto `true`.\\n\\n`jobs.<job_id>.continue-on-error` applies to a single job. If `jobs.<job_id>.continue-on-error` is `true`, other jobs in\\nthe matrix will continue running even if the job with `jobs.<job_id>.continue-on-error: true` fails.\\n\\nYou can use `jobs.<job_id>.strategy.fail-fast` and `jobs.<job_id>.continue-on-error` together. For example, the\\nfollowing workflow will start four jobs. For each job, continue-on-error is determined by the value of\\n`matrix.experimental`. If any of the jobs with `continue-on-error: false` fail, all jobs that are in progress or queued\\nwill be cancelled. If the job with `continue-on-error: true` fails, the other jobs will not be affected.\\n\\n```yaml\\njobs:\\n  test:\\n    runs-on: ubuntu-latest\\n    continue-on-error: ${{ matrix.experimental }}\\n    strategy:\\n      fail-fast: true\\n      matrix:\\n        version: [6, 7, 8]\\n        experimental: [false]\\n        include:\\n          - version: 9\\n            experimental: true\\n```\\n\\nConfiguring the Maximum Number of Concurrent Jobs\\n-------------------------------------------------\\n\\nBy default, GitHub will maximize the number of jobs run in parallel depending on runner availability. To set the maximum\\nnumber of jobs that can run simultaneously when using a `matrix` job strategy, use\\n`jobs.<job_id>.strategy.max-parallel`.\\n\\nFor example, the following workflow will run a maximum of two jobs at a time, even if there are runners available to run\\nall six jobs at once.\\n\\n```yaml\\njobs:\\n  example_matrix:\\n    strategy:\\n      max-parallel: 2\\n      matrix:\\n        version: [10, 12, 14]\\n        os: [ubuntu-latest, windows-latest]\\n```\\n\\n[Contexts]: https://docs.github.com/en/actions/learn-github-actions/contexts\\n[Events that trigger workflows]: https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#repository_dispatch"},{"id":"openssl-encrypt","metadata":{"permalink":"/blog/openssl-encrypt","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2023-11-23-openssl-encrypt.md","source":"@site/blog/2023-11-23-openssl-encrypt.md","title":"Using OpenSSL to encrypt messages and files on Linux","description":"OpenSSL is a powerful cryptography toolkit. Many of us have already used OpenSSL for creating RSA Private Keys or CSR","date":"2023-11-23T00:00:00.000Z","formattedDate":"November 23, 2023","tags":[{"label":"Security","permalink":"/blog/tags/security"}],"readingTime":4.765,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"openssl-encrypt","title":"Using OpenSSL to encrypt messages and files on Linux","authors":["jiaqi"],"tags":["Security"]},"unlisted":false,"prevItem":{"title":"Using a GitHub Action Matrix to Define Variations for Each Job","permalink":"/blog/github-matrix"},"nextItem":{"title":"Build and Push Docker Images through GitHub Action","permalink":"/blog/dockerhub-github-actions"}},"content":"OpenSSL is a powerful cryptography toolkit. Many of us have already used OpenSSL for creating RSA Private Keys or CSR\\n(Certificate Signing Request). However, did you know that we can use OpenSSL to benchmark our computer speed or that we\\ncan also encrypt files or messages? This post will provide you with some simple to follow tips on how to encrypt\\nmessages and files using OpenSSL.\\n\\n\x3c!--truncate--\x3e\\n\\nEncrypt and Decrypt Messages\\n----------------------------\\n\\nFirst we can start by encrypting simple messages. The following linux command will encrypt a message \\"Welcome to\\nLinuxCareer.com\\" using Base64 Encoding:\\n\\n```bash\\n$ echo \\"Welcome to LinuxCareer.com\\" | openssl enc -base64\\nV2VsY29tZSB0byBMaW51eENhcmVlci5jb20K\\n```\\n\\nThe output of the above command is an encrypted string containing encoded message \\"Welcome to LinuxCareer.com\\". To\\ndecrypt encoded string back to its original message we need to reverse the order and attach -d option for decryption:\\n\\n```bash\\n$ echo \\"V2VsY29tZSB0byBMaW51eENhcmVlci5jb20K\\" | openssl enc -base64 -d\\nWelcome to LinuxCareer.com\\n```\\n\\nThe encryption above is simple to use, however, it lacks an important feature of a password, which should be used for\\nencryption. _The procedure above simply exposes the original password in another plain text in the form of of\\n**echo \\"V2VsY29tZSB0byBMaW51eENhcmVlci5jb20K\\" | openssl enc -base64 -d**, which essentially does nothing about \\"hiding\\nsecrets\\" at all_.\\n\\nWhat will do next is, instead of decrypting using `openssl enc -base64 -d`, decrypting with a **password**. Try to\\ndecrypt the following string with a password \\"pass\\":\\n\\n```bash\\necho \\"U2FsdGVkX181xscMhkpIA6J0qd76N/nSjjTc9NrDUC0CBSLpZQxQ2Db7ipd7kexj\\" | openssl enc -aes-256-cbc -d -a\\n```\\n\\ni.e.\\n\\n```bash\\n$ echo \\"U2FsdGVkX181xscMhkpIA6J0qd76N/nSjjTc9NrDUC0CBSLpZQxQ2Db7ipd7kexj\\" | openssl enc -aes-256-cbc -d -a\\nenter aes-256-cbc decryption password: pass\\nLinuxCareer.com\\n```\\n\\nWe see that the original message \\"LinuxCareer.com\\" got decrypted with the password \\"pass\\". To create an encrypted\\nmessage with a password as the one above we can use the following linux command:\\n\\n```bash\\necho \\"LinuxCareer.com\\" | openssl enc -aes-256-cbc -a\\n```\\n\\ni.e.\\n\\n```bash\\n$ echo \\"LinuxCareer.com\\" | openssl enc -aes-256-cbc -a\\nenter aes-256-cbc encryption password:\\nVerifying - enter aes-256-cbc encryption password:\\nU2FsdGVkX185E3H2me2D+qmCfkEsXDTn8nCn/4sblr8=\\n```\\n\\nIf we wish to store OpenSSL\'s output to a file instead of STDOUT simply use STDOUT redirection `>`. When storing\\nencrypted output to a file we can also omit the `-a` option as we no longer need the output to be ASCII text based:\\n\\n```bash\\necho \\"LinuxCareer.com\\" | openssl enc -aes-256-cbc > openssl.dat\\n```\\n\\n```bash\\n$ echo \\"LinuxCareer.com\\" | openssl enc -aes-256-cbc > openssl.dat\\nenter aes-256-cbc encryption password:\\nVerifying - enter aes-256-cbc encryption password:\\n$ file openssl.dat\\nopenssl.dat: openssl enc\'d data with salted password\\n```\\n\\nTo decrypt the \\"openssl.dat\\" file back to its original message use:\\n\\n```bash\\nopenssl enc -aes-256-cbc -d -in openssl.dat\\n```\\n\\n```bash\\n$ openssl enc -aes-256-cbc -d -in openssl.dat\\nenter aes-256-cbc decryption password:\\nLinuxCareer.com\\n```\\n\\nEncrypt and Decrypt File\\n------------------------\\n\\nTo encrypt files with OpenSSL is as simple as encrypting messages. The only difference is that instead of the echo\\ncommand we use the **-in** option with the actual file we would like to encrypt and **-out** option, which will instruct\\nOpenSSL to store the encrypted file under a given name:\\n\\n> \u26a0\ufe0f Ensure that the encrypted output file is given a different filename than the original plain input file. It is also\\n> recommended to do few encrypt/decrypt test runs on dummy data before encrypting important content.\\n\\n```bash\\nopenssl enc -aes-256-cbc -in /etc/services -out services.dat\\n```\\n\\nTo decrypt back our services file use:\\n\\n```bash\\n$ openssl enc -aes-256-cbc -d -in services.dat > services.txt\\nenter aes-256-cbc decryption password:\\n```\\n\\nEncrypt and Decrypt Directory\\n-----------------------------\\n\\nIn case that we needed to use OpenSSL to encrypt an entire directory we would, first need to create gzip **tarball** and\\nthen encrypt the tarball with the above method or we can do both at the same time by using pipe:\\n\\n```bash\\n$ tar cz /etc | openssl enc -aes-256-cbc -out etc.tar.gz.dat\\ntar: Removing leading `/\' from member names\\nenter aes-256-cbc encryption password:\\nVerifying - enter aes-256-cbc encryption password:\\n```\\n\\nTo decrypt and extract the entire etc/ directory to you current working directory use:\\n\\n```bash\\n$ openssl enc -aes-256-cbc -d -in etc.tar.gz.dat | tar xz\\nenter aes-256-cbc decryption password:\\n```\\n\\n**The method above can be quite useful for automated encrypted backups**.\\n\\nUsing Public and Private keys\\n-----------------------------\\n\\nIn this section we will show how to encrypt and decrypt files using public and private keys. First we need to generate\\nprivate and public keys. This can simply be done by:\\n\\n```bash\\nopenssl genrsa -out private_key.pem 1024\\n```\\n\\nFrom the private key we can then generate public key:\\n\\n```bash\\nopenssl rsa -in private_key.pem -out public_key.pem -outform PEM -pubout\\n```\\n\\nAt this point we should have both private and public key available in our current working directory.\\n\\n```bash\\n$ ls\\nprivate_key.pem  public_key.pem\\n```\\n\\nNext, we create some sample file called \\"encrypt.txt\\" with any arbitrary text:\\n\\n```bash\\n$ echo \\"Welcome to LinuxCareer.com\\" > encrypt.txt\\n$ cat encrypt.txt\\nWelcome to LinuxCareer.com\\n```\\n\\nNow we are ready to encrypt this file with public key:\\n\\n```bash\\nopenssl rsautl -encrypt -inkey public_key.pem -pubin -in encrypt.txt -out encrypt.dat\\n```\\n\\ni.e.\\n\\n```bash\\n$ openssl rsautl -encrypt -inkey public_key.pem -pubin -in encrypt.txt -out encrypt.dat\\n$ ls\\nencrypt.dat  encrypt.txt  private_key.pem  public_key.pem\\n$ file encrypt.dat\\nencrypt.dat: openssl enc\'d data with salted password\\n```\\n\\nAs you can see our new encrypt.dat file is no longer text files. To decrypt this file we need to use private key:\\n\\n```bash\\nopenssl rsautl -decrypt -inkey private_key.pem -in encrypt.dat -out new_encrypt.txt\\n```\\n\\ni.e.\\n\\n```bash\\n$ openssl rsautl -decrypt -inkey private_key.pem -in encrypt.dat -out new_encrypt.txt\\n$ cat new_encrypt.txt\\nWelcome to LinuxCareer.com\\n```\\n\\nThe syntax above is quite intuitive. As you can see we have decrypted a file encrypt.dat to its original form and save\\nit as new_encrypt.txt. You can for example combine this syntax with encrypting directories example above to create\\nautomated encrypted backup script."},{"id":"dockerhub-github-actions","metadata":{"permalink":"/blog/dockerhub-github-actions","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2023-11-20-dockerhub-github-actions.md","source":"@site/blog/2023-11-20-dockerhub-github-actions.md","title":"Build and Push Docker Images through GitHub Action","description":"On every push to GitHub, GitHub Action can","date":"2023-11-20T00:00:00.000Z","formattedDate":"November 20, 2023","tags":[{"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":1.52,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"dockerhub-github-actions","title":"Build and Push Docker Images through GitHub Action","authors":["jiaqi"],"tags":["Docker"]},"unlisted":false,"prevItem":{"title":"Using OpenSSL to encrypt messages and files on Linux","permalink":"/blog/openssl-encrypt"},"nextItem":{"title":"Switching Node.js Versions with NVM","permalink":"/blog/nvm"}},"content":"On every push to GitHub, [GitHub Action](https://github.com/marketplace/actions/build-and-push-docker-images) can\\nauto-trigger the docker image build and push to [Docker Hub](https://hub.docker.com). We will be able to see that each\\npush results in a usable image, which enhances the quality of a docker image a lot.\\n\\n\x3c!--truncate--\x3e\\n\\nGenerate Docker Hub Access Token\\n--------------------------------\\n\\nBefore we start, ensure you can access [Docker Hub](https://hub.docker.com/) from any workflows you create. To do this:\\n\\n1. Add your Docker ID as a secret to GitHub. Navigate to your GitHub repository and click **Settings** > **Secrets** >\\n   **New secret**.\\n2. Create a new secret with the name DOCKERHUB_USERNAME and your Docker ID as value.\\n3. Create a new Personal Access Token (PAT). To create a new token, go to\\n   [Docker Hub Settings](https://hub.docker.com/settings/security) and then click **New Access Token**.\\n\\nDefine CI Workflow on GitHub\\n----------------------------\\n\\n`git checkout` the branch that contains the docker image definition, i.e. Dockerfile, and add a new YAML file to the\\nfollowing path\\n\\n```bash\\n<github-repo>/.github/workflows/<workflow-name>.yml\\n```\\n\\nThe YAML file should contain the following workflow definition:\\n\\n> \ud83d\udccb Change the `<branch-name>` and `<docker-image-name>` below accordingly.\\n\\n```yaml\\n# Builds and pushes XXX image to Docker Hub\\n\\nname: ci\\n\\non:\\n  push:\\n    branches:\\n      - \'<branch-name>\'\\n\\njobs:\\n  docker:\\n    runs-on: ubuntu-latest\\n    steps:\\n      -\\n        name: Checkout\\n        uses: actions/checkout@v3\\n      -\\n        name: Set up QEMU\\n        uses: docker/setup-qemu-action@v2\\n      -\\n        name: Set up Docker Buildx\\n        uses: docker/setup-buildx-action@v2\\n      -\\n        name: Login to DockerHub\\n        uses: docker/login-action@v2\\n        with:\\n          {% raw %}\\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\\n          {% endraw %}\\n      -\\n        name: Build and push\\n        uses: docker/build-push-action@v3\\n        with:\\n          context: .\\n          push: true\\n          {% raw %}\\n          tags: ${{ secrets.DOCKERHUB_USERNAME }}/<docker-image-name>:latest\\n          {% endraw %}\\n```\\n\\nPush the YAML file onto GitHub. Every push to that branch afterwards will trigger the image build and push.\\n\\nBuild Status Badge\\n------------------\\n\\nTo generate real-time badge on image build status, we could use an\\n[approach](https://docs.github.com/en/actions/monitoring-and-troubleshooting-workflows/adding-a-workflow-status-badge)\\nthat GitHub supports out-of-the-box."},{"id":"nvm","metadata":{"permalink":"/blog/nvm","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2023-04-21-nvm/index.md","source":"@site/blog/2023-04-21-nvm/index.md","title":"Switching Node.js Versions with NVM","description":"Node Version Manager is a tool that helps us manage Node versions and is a convenient","date":"2023-04-21T00:00:00.000Z","formattedDate":"April 21, 2023","tags":[{"label":"Node.js","permalink":"/blog/tags/node-js"},{"label":"nvm","permalink":"/blog/tags/nvm"}],"readingTime":2.18,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"nvm","title":"Switching Node.js Versions with NVM","authors":["jiaqi"],"tags":["Node.js","nvm"]},"unlisted":false,"prevItem":{"title":"Build and Push Docker Images through GitHub Action","permalink":"/blog/dockerhub-github-actions"},"nextItem":{"title":"Deploying Jenkins to AWS","permalink":"/blog/jenkins-on-aws"}},"content":"[Node Version Manager](https://github.com/nvm-sh/nvm) is a tool that helps us manage Node versions and is a convenient\\nway to install Node. Think of it as npm or Yarn that helps manage Node packages, but instead of packages, NVM manages\\nNode versions.\\n\\nThis also means you can install multiple Node versions onto your machine at the same time and switch among them if\\nneeded.\\n\\n\x3c!--truncate--\x3e\\n\\nDisplaying a List of Node.js Versions\\n-------------------------------------\\n\\nWe can now view all the versions we downloaded so far with\\n\\n```bash\\nnvm ls\\n```\\n\\nThe list then appears:\\n\\n![Error loading node-versions.png](node-versions.png)\\n\\nThe first three lines show the list of Node versions with the arrow pointing to the 14.18.1 version that is currently in\\nuse; when a version is used, it displays as green.\\n\\nSwitching Among Node.js Versions\\n--------------------------------\\n\\nThe best feature about NVM is the ability to easily switch between different Node versions. Say we must use version\\n16.13.0 and then switch to 12.22.7; we can simply run either `nvm use 12.22.7` or `nvm use 16.13.0` to easily switch\\ninto either version we need.\\n\\nTroubleshooting\\n---------------\\n\\n### Changing Node Version\\n\\n> \u26a0\ufe0f Warning: This answer does not support Windows OS\\n\\nSuppose we would like to down-grade version from 18 to 14, then we can use `n` for node\'s version management like this.\\n[There](https://www.npmjs.com/package/n) is a simple intro for `n`.\\n\\n```bash\\nnpm install -g n\\nn 6.10.3\\n```\\n\\nThis is very easy to use. then you can show your node version:\\n\\n```bash\\nnode -v\\nv6.10.3\\n```\\n\\nThe available node versions can be found on Node\'s [release page](https://nodejs.org/en/about/previous-releases)\\n\\nFor windows [nvm](https://github.com/coreybutler/nvm-windows) is a well-received tool.\\n\\n### \\"npm install\\" Error\\n\\n#### GitHub Operation Times Out\\n\\n```bash\\ngit config --global url.\\"https://\\".insteadOf git://\\n```\\n\\nThis will change all of your urls so that they all start with `https://` which shall be working for you.\\n\\n#### node-sass Version Issue\\n\\nRunning `npm install` gives\\n\\n```bash\\n.node-gyp/18.7.0/include/node/v8-internal.h:646:38: error: no template named \'remove_cv_t\' in namespace \'std\'; did you mean \'remove_cv\'?\\n```\\n\\nWhat you\'re seeing is an error during compilation of node-sass. That\'s a package processing your Sass/SCSS styles, which\\nis written in C++ and only re-packaged as a JavaScript library. The fact it\'s written in C++ means it needs to be\\ncompiled on your device during installation (this is internally done by a tool called node-gyp, which you can spot in\\nyour error output, too).\\n\\n**The problem is node-sass with the specified version in package.json doesn\'t support Node version installed on the\\nmachine**. The [node-sass community](https://github.com/sass/node-sass) needs time to catch up to support it (and\\nthat\'s fair, as it\'s a volunteer-driven project).\\n\\nCase-by-case soulutions would be either upgrading sass versions or [downgrading Node](#change-node-version)"},{"id":"jenkins-on-aws","metadata":{"permalink":"/blog/jenkins-on-aws","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-10-31-jenkins-on-aws/index.md","source":"@site/blog/2022-10-31-jenkins-on-aws/index.md","title":"Deploying Jenkins to AWS","description":"Jenkins is an open-source automation server that integrates with a number of AWS Services, including: AWS CodeCommit,","date":"2022-10-31T00:00:00.000Z","formattedDate":"October 31, 2022","tags":[{"label":"Jenkins","permalink":"/blog/tags/jenkins"}],"readingTime":45.405,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"jenkins-on-aws","title":"Deploying Jenkins to AWS","authors":["jiaqi"],"tags":["Jenkins"]},"unlisted":false,"prevItem":{"title":"Switching Node.js Versions with NVM","permalink":"/blog/nvm"},"nextItem":{"title":"AWS Troubleshooting","permalink":"/blog/aws-troubleshooting"}},"content":"Jenkins is an open-source automation server that integrates with a number of AWS Services, including: AWS CodeCommit,\\nAWS CodeDeploy, Amazon EC2 Spot, and Amazon EC2 Fleet. We can use Amazon Elastic Compute Cloud (Amazon EC2) to deploy a\\nJenkins application on AWS.\\n\\nThis post documents the process of deploying a Jenkins application. We will launch an EC2 instance, install Jenkins on\\nthat instance, and configure Jenkins to automatically spin up Jenkins agents if build abilities need to be augmented\\non the instance.\\n\\n\x3c!--truncate--\x3e\\n\\n:::note Prerequisites\\n\\n1. [Register an AWS account](https://portal.aws.amazon.com/billing/signup#/start), if not having one yet.\\n2. [An Amazon EC2 key pair](#creating-a-key-pair), if we don\'t have one yet\\n\\n:::\\n\\nCreating a Key Pair\\n-------------------\\n\\nCreating a key pair helps ensure that the correct form of authentication is used when we install Jenkins.\\n\\nTo create our key pair:\\n\\n1. Open the [Amazon EC2 console](https://console.aws.amazon.com/ec2/) and sign in.\\n2. In the navigation pane, under **NETWORK & SECURITY**, select **Key Pairs**.\\n3. Select **Create key pair**.\\n4. For **Name**, enter a descriptive name for the key pair. Amazon EC2 associates the public key with the name that we\\n   specify as the **key name**. A key name can include up to 255 ASCII characters. It cannot include leading or trailing\\n   spaces.\\n5. For **File format**, select the format in which to save the private key.\\n    - For OpenSSH compatibility (Linux or Mac OS X), select pem.\\n    - For PuTTY compatibility (Windows), select ppk.\\n6. Select **Create key pair**.\\n7. The private key file downloads automatically. The base file name is the name we specified as the name of our key\\n   pair, and the file name extension is determined by the file format we chose. Save the private key file in a safe\\n   place.\\n\\n   :::caution\\n\\n   This is the only chance for us to save the private key file.\\n\\n   :::\\n\\n8. If we use an SSH client on a macOS or Linux computer to connect to our Linux instance, we would also run the\\n   following command to set the permissions of our private key file so that only we can read it, otherwise we won\'t be\\n   able to connect to our instance using this key pair. For more information, please refer to\\n   [Error: Unprotected private key file](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#troubleshoot-unprotected-key).\\n\\n   ```bash\\n   chmod 400 <key_pair_name>.pem\\n   ```\\n\\nCreating a Security Group\\n-------------------------\\n\\nA security group acts as a firewall that controls the traffic allowed to reach one or more EC2 instances. When we launch\\nan instance, we can assign it one or more security groups. We add rules that control the traffic allowed to reach the\\ninstances in each security group. We can modify a security group\'s rules any time, and the new rules take effect\\n_immediately_.\\n\\nTo create and configure our security group:\\n\\n1. Decide who may access our instance. For example, a single computer or all trusted computers on a network. For a\\n   single computer/user, we can use the public IP address of the computer/user. To find the IP address, use the\\n   [check IP service tool](http://checkip.amazonaws.com/) from AWS3 (this tool works for VPN as well) or search for the\\n   phrase \\"what is my IP address\\" in any search engine.\\n2. Sign in to the [AWS Management Console](https://console.aws.amazon.com/ec2/).\\n3. Open the Amazon EC2 console by selecting **EC2** under **Compute**.\\n\\n   ![Error loading ec2-service.png](ec2-service.png)\\n\\n4. In the left-hand navigation bar, select **Security Groups**, and then select **Create Security Group**.\\n\\n   ![Error loading create-security-group.png](create-security-group.png)\\n\\n5. In **Security group name**, enter **WebServerSG** or any preferred name of our choice, and provide a description.\\n6. Select our VPC from the list. We can use the default VPC.\\n7. On the **Inbound tab**, add the rules as follows:\\n    - Select **Add Rule**, and then select **SSH** from the Type list.\\n    - Under Source, select **Custom**, and in the text box, enter the IP address from step 1.\\n    - Select **Add Rule**, and then select **HTTP** from the Type list.\\n    - Select **Add Rule**, and then select **Custom TCP Rule** from the Type list.\\n    - Under **Port Range**, enter **8080** (Jenkins UI port).\\n8. Select Create.\\n\\nFor more information, refer to\\n[Security Groups](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html) in the Amazon EC2 User\\nGuide for Linux Instances.\\n\\nLaunching an Amazon EC2 Instance\\n--------------------------------\\n\\nTo launch an EC2 instance:\\n\\n1. Sign in to the [AWS Management Console](https://console.aws.amazon.com/ec2/).\\n2. Open the Amazon EC2 console by selecting EC2 under **Compute**.\\n3. From the Amazon EC2 dashboard, select **Launch Instance**.\\n\\n   ![Error loading ec2-launch-instance.png](ec2-launch-instance.png)\\n\\n4. We will run Ubuntu OS in our instance, so let\'s pick up \\"Ubuntu\\" and give it a name such as \\"Jenkins Server\\"\\n\\n   ![Error loading ec2-instance-setup.png](ec2-instance-setup.png)\\n\\n5. On the Choose an Instance Type page, the **t2.micro** instance is selected by default. Verify this instance type is\\n   selected to stay within the free tier.\\n6. Pick up the Key Pair (under \\"Key pair\\" section) and the security group (under \\"Network settings\\") we defined\\n   previously\\n7. Hit \\"Launch Instance\\"\\n8. In the left-hand navigation bar, choose Instances to view the status of our instance. Initially, the status of our\\n   instance is pending. After the status changes to running, our instance is ready for use.\\n\\n   ![Error loading ec2-view-created-instance.png](ec2-view-created-instance.png)\\n\\nInstalling Jenkins on EC2\\n-------------------------\\n\\nNow that the Amazon EC2 instance has been launched, Jenkins can be installed properly.\\n\\nIn this step we will deploy Jenkins on our EC2 instance by completing the following tasks:\\n\\n1. [Connecting to Our Linux Instance](#connecting-to-our-linux-instance)\\n2. [Downloading and Installing Jenkins](#downloading-and-installing-jenkins)\\n3. [Configuring Jenkins](#configuring-jenkins)\\n\\n### Connecting to Our Linux Instance\\n\\nAfter we launch our instance, we can connect to it and use it the same way as our local machine.\\n\\nBefore we connect to our instance, get the public DNS name of the instance using the Amazon EC2 console.\\n\\n![Error loading ec2-public-dns.png](ec2-public-dns.png)\\n\\nUse the `ssh` command to connect to the instance. We will specify the private key (.pem) file and\\n`ec2-user@public_dns_name`.\\n\\n```bash\\nssh -i \\"/path/my-key-pair.pem\\" ubuntu@ec2-198-51-100-1.compute-1.amazonaws.com\\n```\\n\\nWe will receive a response like the following:\\n\\n```bash\\nThe authenticity of host \'ec2-198-51-100-1.compute1.amazonaws.com (10.254.142.33)\' cant be\\nestablished.\\n\\nRSA key fingerprint is 1f:51:ae:28:bf:89:e9:d8:1f:25:5d:37:2d:7d:b8:ca:9f:f5:f1:6f.\\n\\nAre you sure you want to continue connecting\\n(yes/no)?\\n```\\n\\nEnter \\"yes\\". We will receive a response like the following:\\n\\n```bash\\nWarning: Permanently added \'ec2-198-51-100-1.compute1.amazonaws.com\' (RSA) to the list of known hosts.\\n```\\n\\n### Downloading and Installing Jenkins\\n\\n#### Installing Java\\n\\nJenkins requires Java in order to run, yet certain distributions don\'t include this by default and some Java versions\\nare incompatible with Jenkins, because **Jenkins requires Java 11 or 17 since Jenkins 2.357 and LTS 2.361.1**.\\n\\nThere are multiple Java implementations which we can use. [OpenJDK](https://openjdk.java.net/) is the most popular one\\nat the moment, we will use it in this guide.\\n\\nUpdate the Debian apt repositories, install OpenJDK 11, and check the installation with the commands:\\n\\n```bash\\n$ sudo apt update\\n$ sudo apt install openjdk-11-jre\\n$ java -version\\nopenjdk version \\"11.0.12\\" 2021-07-20\\nOpenJDK Runtime Environment (build 11.0.12+7-post-Debian-2)\\nOpenJDK 64-Bit Server VM (build 11.0.12+7-post-Debian-2, mixed mode, sharing)\\n```\\n\\n#### Installing Jenkins\\n\\nOn Debian and Debian-based distributions like Ubuntu, i.e. our EC2 base image, we can install Jenkins through `apt`.\\n\\n##### Long Term Support Release\\n\\nA Jenkins [LTS (Long-Term Support) release](https://www.jenkins.io/download/lts/) is chosen every 12 weeks from the\\nstream of regular releases as the stable release for that time period. It can be installed from the\\n[debian-stable apt repository](https://pkg.jenkins.io/debian-stable/).\\n\\n```bash\\ncurl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo tee \\\\\\n/usr/share/keyrings/jenkins-keyring.asc > /dev/null\\n\\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\\\\nhttps://pkg.jenkins.io/debian-stable binary/ | sudo tee \\\\\\n/etc/apt/sources.list.d/jenkins.list > /dev/null\\n\\nsudo apt-get update\\nsudo apt-get install jenkins\\n```\\n\\nBeginning with Jenkins 2.335 and Jenkins 2.332.1, the package is configured with **systemd** rather than the older\\nSystem V init, so we would use systemd extensively to configure Jenkins next.\\n\\nThe package installation will:\\n\\n- Setup Jenkins as a daemon launched on start. Run **`systemctl cat jenkins`** for more details.\\n- Create a \\"**jenkins**\\" user to run this service. This username is very important because it would be used latter\\n  during [HTTPS/SSL configuration](#enable-ssl-on-jenkins-server-ubuntu)\\n\\n  :::tip\\n\\n  To verify user \\"jenkins\\" has been created, run `cat /etc/passwd`. Every user on a Linux system, whether created as an\\n  account for a real human being or associated with a particular service or system function, is stored in a file called\\n  \\"**/etc/passwd**\\", which contains information about the users on the system. Each line describes a distinct user.\\n\\n  :::\\n\\n- Direct console log output to `systemd-journald`. Run **`journalctl -u jenkins.service -r`** if we are troubleshooting\\n  Jenkins.\\n- Populate `/lib/systemd/system/jenkins.service` with configuration parameters for the launch, e.g `JENKINS_HOME`\\n- Set Jenkins to listen on port 8080. Access this port with our browser to start configuration. Note that this is not\\n  a secure port because it is a HTTP port. We would change this port in a moment for better security.\\n\\n##### Weekly Release\\n\\nAlternatively, a new release is produced weekly to deliver bug fixes and features to users and plugin developers. It can\\nbe installed from the [debian apt repository](https://pkg.jenkins.io/debian/).\\n\\n```bash\\ncurl -fsSL https://pkg.jenkins.io/debian/jenkins.io.key | sudo tee \\\\\\n  /usr/share/keyrings/jenkins-keyring.asc > /dev/null\\n\\necho deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\\\\\n  https://pkg.jenkins.io/debian binary/ | sudo tee \\\\\\n  /etc/apt/sources.list.d/jenkins.list > /dev/null\\n\\nsudo apt-get update\\nsudo apt-get install jenkins\\n```\\n\\nEnable SSL on Jenkins Server (Ubuntu)\\n-------------------------------------\\n\\nIt is very important to secure Jenkins by enabling SSL which runs in a project environment. This section is designed to\\nintroduce the Secure Sockets Layer (SSL) application-level protocol, and particularly the OpenSSL implementation of\\nSSL, in the context of using in Jenkins. After a brief description of exactly what OpenSSL is, and what it is useful\\nfor, the section will further illustrate the practical usage of OpenSSL in Jenkins service for the purpose of serving\\nsecured Jenkins resources from our EC2 instance.\\n\\nWhile previous familiarity with Server Sockets Layer (SSL), or the OpenSSL implementation in particular, is not required\\nfor this section, if desired, the reader is advised to pursue further learning from the [resources](#resources) listed\\nbelow in order to broaden his/her understanding of this powerful security layer.\\n\\n### Resources\\n\\n#### Local System Resources\\n\\n| command           | description                                                             |\\n|-------------------|-------------------------------------------------------------------------|\\n| **`man config`**  | System manual page for the OpenSSL library configuration files          |\\n| **`man gendsa`**  | System manual page for the gendsa DSA private key generator             |\\n| **`man genrsa`**  | System manual page for the genrsa RSA private key generator             |\\n| **`man openssl`** | System manual page for the openssl command-line tool                    |\\n| **`man rand`**    | System manual page for the rand pseudo-random byte generator utility    |\\n| **`man x509`**    | System manual page for the x509 certificate display and signing utility |\\n\\n#### WWW Resources\\n\\n- [OpenSSL Website](http://www.openssl.org/)\\n- [Public Key Infrastructure (X.509) (pkix)](http://www.ietf.org/html.charters/pkix-charter.html)\\n\\n### About OpenSSL\\n\\n**Secure Sockets Layer** is an application-level protocol which was developed by the Netscape Corporation for the\\npurpose of transmitting sensitive information, such as Credit Card details, via the Internet. SSL works by using a\\nprivate key to encrypt data transferred over the SSL-enabled connection, thus thwarting eavesdropping of the\\ninformation. The most popular use of SSL is in conjunction with web browsing (using the HTTP protocol), but many network\\napplications can benefit from using SSL. By convention, URLs that require an SSL connection start with \\"https:\\" instead\\nof \\"http:\\"\\n\\n**OpenSSL** is a robust, commercial-grade implementation of SSL tools, and related general purpose library based upon\\nSSLeay, developed by Eric A. Young and Tim J. Hudson. OpenSSL is available as an Open Source equivalent to commercial\\nimplementations of SSL via an [Apache-style license](http://www.openssl.org/source/license.html).\\n\\n#### About X.509\\n\\nX.509 is a specification for digital certificates published by the International Telecommunications Union -\\nTelecommunication (ITU-T). It specifies information and attributes required for the identification of a person or a\\ncomputer system, and is used for secure management and distribution of digitally signed certificates across secure\\nInternet networks. OpenSSL most commonly uses X.509 certificates.\\n\\n### Installing OpenSSL Toolkit\\n\\nAWS EC2 instance usually have OpenSSL installed and configured properly.\\n\\n### Generating SSL Certificates\\n\\nOnce we have properly generated an X.509-compliant SSL certificate, we may either elect to\\n\\n- sign the certificate by ourselves by [generating a Certificate Authority (CA)](#self-signing-a-certificate), or\\n- have a globally recognized Certificate Authority [sign the certificate](#applying-for-a-certificate-using-certbot). We\\n  can use services such as [Letsencrypt](https://letsencrypt.org/) for valid SSL certificates. But these certificates\\n  have to be renewed every three months.\\n\\nWhen the certificate is signed, it is then ready to be used with the OpenSSL toolkit, or the library to enable encrypted\\nSSL connections to a Lightweight Directory Access Protocol, (LDAP) or Hyper Text Transport Protocol (HTTP) server, for\\nexample. The following sections describe the certificate generation, and signing process for both self-signed, and\\nrecognized CA-signed certificates.\\n\\n> Note that this section has no Jenkins involved and sets up SSL in a more general context\\n\\n#### Self-signing a Certificate\\n\\n##### Establishing a Certificate Authority\\n\\n:::info What is a Self-Signed Certificate?\\n\\nA **self-signed certificate** is an SSL/TSL certificate not signed by a public or private certificate authority.\\nInstead, it is signed by the creator\u2019s own personal or root CA certificate.\\n\\nHere is what we do to request paid SSL/TLS certificate from a **well-known Certificate Authority** like Verisign or\\ncomodo.\\n\\n![Error loading well-known-certificate-authority.png](well-known-certificate-authority.png)\\n\\n1. Create a **certificate signing request (CSR)** with a private key. A CSR contains details about location,\\n   organization, and FQDN (Fully Qualified Domain Name).\\n2. Send the CSR to the trusted CA authority.\\n3. The CA authority will send us the SSL certificate signed by their root certificate authority and private key.\\n4. We can then validate and use the SSL certificate with our applications.\\n\\nBut for a **self-signed certificate**, here is what we do.\\n\\n![Error loading self-signed-cert.png](self-signed-cert.png)\\n\\n1. Create our own root CA certificate & CA private key (We act as a CA on our own)\\n2. Create a server private key to generate CSR\\n3. Create an SSL certificate with CSR using our root CA and CA private key.\\n4. Install the CA certificate in the browser or Operating system to avoid security warnings.\\n\\nMost browsers & operating systems hold a copy of root CA certificates of all the trusted certified Certificated\\nAuthorities. That\'s the reason the browsers won\'t show any security messages when we visit standard websites that use\\nSSL from a trusted and well-known commercial Certificate authority. For example, the following image shows the root CA\\npresent in the Firefox browser by default.\\n\\n![Error loading trusted-ca.png](trusted-ca.png)\\n\\nAt the same time, if we use a self-signed certificate, our browser will throw a security warning. The reason is browsers\\nonly trust SSL from a trusted Certificate authority. For example,\\n\\n```bash\\nYour connection is not private\\nAttackers might be trying to steal your information from demo.apps.mlopshub.com (for example, passwords, messages or\\ncredit cards)\\n```\\n\\nBut we can force browsers & operating systems to accept our own certificate authority. So we won\'t see the security\\nwarning once we install the CA certificate and add it to the trusted list. We can also share the CA certificate with our\\ndevelopment team to install in their browsers as well.\\n\\nAlso, we can use this CA to create **more than one SSL certificate**.\\n\\n:::\\n\\nSelf-signed certificates have a major advantage in that they are completely free to use, and they may be generated,\\nsigned, and used on an as-needed basis. Self-signed certificates are great for use in closed-lab environments or for\\ntesting purposes. One of the drawbacks of using self-signed certificates, however, is that warnings will typically be\\nissued by a user\'s Web browser, and other applications, upon accessing an SSL-secured server that uses a self-signed\\ncertificate. By default, client applications (e.g., Firefox) will suppress such warnings for certificates that are\\nsigned using only a globally-recognized and trusted Certificate Authority, but warnings may also be squelched by\\nimporting a server\'s root certificate into client applications; a relevant demonstration is shown later in this guide.\\nUsing self-signed certificates in a publicly-accessible, production environment is not recommended due to the implicit\\ntrust issues arising from these warnings, in addition to the potential confusion caused to users.\\n\\n:::caution\\n\\nWe must obtain a certificate signed by a recognized Certificate Authority in order to establish a commercial site, e.g.,\\nfor conducting \\"e-commerce\\", which is however not the case in the context of Jenkins we deiscuss here\\n\\n:::\\n\\nProvided we\'ve have the OpenSSL toolkit properly installed on our EC2 instance, the generation of X.509 SSL certificates\\nis quite simple. For self-signed certificates, we must **first establish a Certificate Authority (CA)** by following the\\nsteps below:\\n\\nFirst, create an initial working environment, for example within our home directory by issuing the following command\\nfrom a terminal prompt:\\n\\n```bash\\ncd && mkdir -p myCA/signedcerts && mkdir myCA/private && cd myCA\\n```\\n\\nThe above command will place us in a newly-created subdirectory of our home directory named \\"myCA\\", and within this\\nsubdirectory, we should have two additional subdirectories named \\"signedcerts\\" and \\"private\\".\\n\\nWithin this initial working environment, the significance of the subdirectories, and their contents is as follows:\\n\\n1. `~/myCA`: contains CA certificate, certificates database, generated certificates, keys, and requests\\n2. `~/myCA/signedcerts`: contains copies of each signed certificate\\n3. `~/myCA/private`: contains the private key\\n\\nNext, create an initial certificate database in the `~/myCA` subdirectory with the following command at a terminal\\nprompt:\\n\\n```bash\\necho \'01\' > serial  && touch index.txt\\n```\\n\\nNow create an initial **caconfig.cnf** file suitable for the creation of CA certificates. Using our favorite editor,\\nedit the file `~/myCA/caconfig.cnf`, and insert the following content into the file:\\n\\n```bash\\nsudo nano ~/myCA/caconfig.cnf\\n```\\n\\n```conf\\n# My sample caconfig.cnf file.\\n#\\n# Default configuration to use when one is not provided on the command line.\\n#\\n[ ca ]\\ndefault_ca      = local_ca\\n#\\n#\\n# Default location of directories and files needed to generate certificates.\\n#\\n[ local_ca ]\\ndir             = /home/<username>/myCA\\ncertificate     = $dir/cacert.pem\\ndatabase        = $dir/index.txt\\nnew_certs_dir   = $dir/signedcerts\\nprivate_key     = $dir/private/cakey.pem\\nserial          = $dir/serial\\n#\\n#\\n# Default expiration and encryption policies for certificates.\\n#\\ndefault_crl_days        = 365\\ndefault_days            = 1825\\ndefault_md              = sha1\\n#\\npolicy          = local_ca_policy\\nx509_extensions = local_ca_extensions\\n#\\n#\\n# Copy extensions specified in the certificate request\\n#\\ncopy_extensions = copy\\n#\\n#\\n# Default policy to use when generating server certificates.  The following\\n# fields must be defined in the server certificate.\\n#\\n[ local_ca_policy ]\\ncommonName              = supplied\\nstateOrProvinceName     = supplied\\ncountryName             = supplied\\nemailAddress            = supplied\\norganizationName        = supplied\\norganizationalUnitName  = supplied\\n#\\n#\\n# x509 extensions to use when generating server certificates.\\n#\\n[ local_ca_extensions ]\\nbasicConstraints        = CA:false\\n#\\n#\\n# The default root certificate generation policy.\\n#\\n[ req ]\\ndefault_bits    = 2048\\ndefault_keyfile = /home/<username>/myCA/private/cakey.pem\\ndefault_md      = sha1\\n#\\nprompt                  = no\\ndistinguished_name      = root_ca_distinguished_name\\nx509_extensions         = root_ca_extensions\\n#\\n#\\n# Root Certificate Authority distinguished name.  Change these fields to match\\n# your local environment!\\n#\\n[ root_ca_distinguished_name ]\\ncommonName              = MyOwn Root Certificate Authority\\nstateOrProvinceName     = NC\\ncountryName             = US\\nemailAddress            = root@tradeshowhell.com\\norganizationName        = Trade Show Hell\\norganizationalUnitName  = IT Department\\n#\\n[ root_ca_extensions ]\\nbasicConstraints        = CA:true\\n```\\n\\n:::caution\\n\\nMake sure to adjust the site-specific details in the file, such as the two instances of `/home/<username>/` under\\n`[ local_ca ]` and `[ req ]`. Also change `commonName`, `stateOrProvinceName` `countryName` etc under\\n`[ root_ca_distinguished_name ]` accordingly. For more information on the directives contained within this configuration\\nfile, use the `man config` command.\\n\\n:::\\n\\nWhen we\'ve edited the file to match our environment, save the file as `~/myCA/caconfig.cnf`.\\n\\nNext, we need to generate the **Certificate Authority Root Certificate** and **Key**, by issuing a few commands. First,\\ndo this:\\n\\n```bash\\nexport OPENSSL_CONF=~/myCA/caconfig.cnf\\n```\\n\\nThe previous command sets an environment variable, `OPENSSL_CONF`, which forces the openssl tool to look for a\\nconfiguration file in an alternative location (in this case, `~/myCA/caconfig.cnf`).\\n\\nNow, generate the CA certificate and key with the following command:\\n\\n```bash\\nopenssl req -x509 -newkey rsa:2048 -out cacert.pem -outform PEM -days 1825\\n```\\n\\nWe should be prompted for a passphrase, and see output similar to this:\\n\\n```bash\\nGenerating a 2048 bit RSA private key\\n.................................+++\\n.................................................................................................+++\\nwriting new private key to \'/home/bshumate/myCA/private/cakey.pem\'\\nEnter PEM pass phrase:\\nVerifying - Enter PEM pass phrase:\\n-----\\n```\\n\\n**Do not forget the passphrase used with the command above!** We\'ll need it every time we want to generate and sign a\\nnew server or client certificate!\\n\\nThe process above will create a self-signed certificate using PEM format and RSA public/private key encryption. The\\ncertificate will be valid for _1825_ days. The location, and purpose of the resultant files is as follows:\\n\\n- `~/myCA/cacert.pem`: CA public **certificate**\\n- `~/myCA/private/cakey.pem`: CA private key\\n\\n##### Creating a Self-Signed Server Certificate\\n\\nNow that we have a Certificate Authority configured, we may use it to sign self-signed certificates. Prior to beginning\\nthe steps below, we may wish to encrypt the certificate\'s private key with a passphrase. The advantages of encrypting\\nthe key with a passphrase include protection of the certificate in the event it is stolen.\\n\\nThe certificate cannot be used with SSL-enabled applications without entering the passphrase every time the SSL-enabled\\napplication is started. This condition, while being most secure, can present a problem: If the server must be started in\\nan unattended manner as in the case of a computer restart, then no one will be available to enter the passphrase, and\\nsubsequently the server will not start. One way to eliminate this condition involves a trade-off in security: The key\\nmay be decrypted, to remove the passphrase necessity; thus SSL-enabled applications will start automatically, without a\\nneed for us to enter a passphrase.\\n\\nTo actually generate a self-signed certificate for use with an SSL application, follow this process:\\n\\nCreate the server configuration file, by editing `~/myCA/exampleserver.cnf` with your favorite text editor. Add this\\nexample content:\\n\\n```conf\\n#\\n# exampleserver.cnf\\n#\\n\\n[ req ]\\nprompt                  = no\\ndistinguished_name      = server_distinguished_name\\nreq_extensions          = v3_req\\n\\n[ server_distinguished_name ]\\ncommonName              = tradeshowhell.com\\nstateOrProvinceName     = NC\\ncountryName             = US\\nemailAddress            = root@tradeshowhell.com\\norganizationName        = My Organization Name\\norganizationalUnitName  = Subunit of My Large Organization\\n\\n[ v3_req ]\\nbasicConstraints        = CA:FALSE\\nkeyUsage                = nonRepudiation, digitalSignature, keyEncipherment\\nsubjectAltName          = @alt_names\\n\\n[ alt_names ]\\nDNS.0                   = tradeshowhell.com\\nDNS.1                   = alt.tradeshowhell.com\\n```\\n\\nBe sure to change the values under `server_distinguished_name` especially the **commonName** value. The commonName value\\nmust match the host name (i.e. AWS EC2 **Public IPv4 DNS**), or CNAME for the host we wish to use the key for. If the\\ncommonName does not match the intended hostname, then host/certificate mismatch errors will appear in the client\\napplications of clients attempting to access the server.\\n\\nOnce we\'ve edited the file appropriately, save it as `~/myCA/exampleserver.cnf`. Generate the server certificate, and\\nkey with the following commands:\\n\\n```bash\\nexport OPENSSL_CONF=~/myCA/exampleserver.cnf\\n```\\n\\nThe previous command sets an environment variable `OPENSSL_CONF` which forces the `openssl` tool to look for a\\nconfiguration file in an alternative location (in this case, `~/myCA/exampleserver.cnf`).\\n\\nNow generate the certificate, and key:\\n\\n```bash\\nopenssl req -newkey rsa:1024 -keyout tempkey.pem -keyform PEM -out tempreq.pem -outform PEM\\n```\\n\\nWe should be prompted for a passphrase, and see output similar to this:\\n\\n```bash\\nGenerating a 1024 bit RSA private key\\n...++++++\\n...............++++++\\nwriting new private key to \'tempkey.pem\'\\nEnter PEM pass phrase:\\nVerifying - Enter PEM pass phrase:\\n-----\\n```\\n\\n**Don\'t forget the passphrase!**\\n\\nNext, we may translate the temporary private key into an unencrypted key by using the following command:\\n\\n```bash\\nopenssl rsa < tempkey.pem > server_key.pem\\n```\\n\\nWe should be prompted for the passphrase used above, and see the following output:\\n\\n```bash\\nEnter pass phrase:\\nwriting RSA key\\n```\\n\\nIf we wish to leave the key encrypted with a passphrase, we will simply rename the temporary key using the following\\ncommand, instead of following the step above:\\n\\n:::info\\n\\nIf we use a server key encrypted with a passphrase, the passphrase will have to be entered each time the server\\napplication using the encrypted key is started. This means the server application will not start unless someone, or\\nsomething enters the key.\\n\\n:::\\n\\n```bash\\nmv tempkey.pem server_key.pem\\n```\\n\\n##### Signing the Self-Signed Server Certificate\\n\\nNow we need to sign the server certificate with the Certificate Authority (CA) key using these commands:\\n\\n```bash\\nexport OPENSSL_CONF=~/myCA/caconfig.cnf\\n```\\n\\nThe previous command modifies the environment variable `OPENSSL_CONF` which forces the `openssl` tool to look for a\\nconfiguration file in an alternative location (in this case, `~/myCA/caconfig.cnf` to switch back to the CA\\nconfiguration).\\n\\nThen sign the certificate as follows:\\n\\n```bash\\nopenssl ca -in tempreq.pem -out server_crt.pem\\n```\\n\\nWe will be prompted for the passphrase of the CA key as created in the\\n[Certificate Authority setup](#establishing-a-certificate-authority) section above. Enter this passphrase at the prompt,\\nand we will then be prompted to confirm the information in the `exampleserver.cnf`, and finally asked to confirm signing\\nthe certificate. Output should be similar to this:\\n\\n```bash\\nUsing configuration from /home/bshumate/myCA/caconfig.cnf\\nEnter pass phrase for /home/bshumate/myCA/private/cakey.pem:\\nCheck that the request matches the signature\\nSignature ok\\nThe Subject\'s Distinguished Name is as follows\\ncommonName            :PRINTABLE:\'tradeshowhell.com\'\\nstateOrProvinceName   :PRINTABLE:\'NC\'\\ncountryName           :PRINTABLE:\'US\'\\nemailAddress          :IA5STRING:\'root@tradeshowhell.com\'\\norganizationName      :PRINTABLE:\'Trade Show Hell\'\\norganizationalUnitName:PRINTABLE:\'Black Ops\'\\nCertificate is to be certified until Jan  4 21:50:08 2011 GMT (1825 days)\\nSign the certificate? [y/n]:y\\n\\n1 out of 1 certificate requests certified, commit? [y/n]y\\nWrite out database with 1 new entries\\nData Base Updated\\n```\\n\\nRemove the temporary certificate, and key files with the following command:\\n\\n```bash\\nrm -f tempkey.pem && rm -f tempreq.pem\\n```\\n\\nHooray! We now have a self-signed server application certificate, and key pair:\\n\\n1. **server_crt.pem**: Server application certificate file\\n2. **server_key.pem**: Server application key file\\n\\nNext, we shall use the certificate for our Jenkins instance.\\n\\n##### Chrome: Bypass \\"Your connection is not private\\" Message\\n\\n- Option 1 - **Simply Proceed**: If Chrome says the security certificate is from the same domain we are attempting to\\n  login to, it is likely there is nothing to worry about when this warning appears. To proceed, simply choose the\\n  \\"**Advanced**\\" link, then choose \\"`Proceed to <link> (unsafe)`\\".\\n\\n  ![Error loading Chrome-Advanced.png](Chrome-Advanced.png)\\n  ![Error loading Chrome-proceed-unsafe.png](Chrome-proceed-unsafe.png)\\n\\n- Option 2 - **Prevent Warning**: Click a blank section of the denial page and use our keyboard, type `thisisunsafe`.\\n  This will add the website to a safe list, where we should not be prompted again. _Strange steps, but it surely\\n  works!_\\n\\n#### Applying for a Certificate using Certbot\\n\\n:::note Requirements\\n\\n![Error loading certbot-requirements.png](certbot-requirements.png)\\n\\n:::\\n\\n##### Install and Setup Certbot\\n\\n1. **SSH into the server** SSH into the server (i.e. EC2 instance) running our Jenkins instance as a user with sudo\\n   privileges.\\n\\n2. **Install snapd** from command line\\n\\n    ```bash\\n    $ sudo apt update\\n    $ sudo apt install snapd\\n    ```\\n\\n   Either log out and back in again, or restart the EC2 instance, to ensure snap\'s paths are updated correctly. To test\\n   our system, install the [hello-world](https://snapcraft.io/hello-world) snap and make sure it runs correctly:\\n\\n    ```bash\\n    $ sudo snap install hello-world\\n    hello-world 6.4 from Canonical\u2713 installed\\n    $ hello-world\\n    Hello World!\\n    ```\\n\\n3. **Ensure that our version of snapd is up to date** Execute the following instructions on the command line on the\\n   machine to ensure that we have the latest version of `snapd`.\\n\\n    ```bash\\n    $ sudo snap install core; sudo snap refresh core\\n    ```\\n\\n4. **Install Certbot** Run this command on the command line on the machine to install Certbot.\\n\\n    ```bash\\n    $ sudo snap install --classic certbot\\n    ```\\n\\n5. **Prepare the Certbot command** Execute the following instruction on the command line on the machine to ensure that\\n   the `certbot` command can be run.\\n\\n    ```bash\\n    $ sudo ln -s /snap/bin/certbot /usr/bin/certbot\\n    ```\\n\\n6. **Install Nginx**\\n\\n    We would choose Nginx instead of Apache server in this case by installing Nginx using\\n\\n    ```bash\\n    $ sudo apt install python3-certbot-nginx\\n    ```\\n\\n   Note that Certbot does not install nginx for us automatically and will through us an error, in the next step, of\\n\\n    ```\\n    Could not find a usable \'nginx\' binary. Ensure nginx exists, the binary is executable, and your PATH is set\\n    correctly\\n    ```\\n\\n7. **Get and Install Certificates** Run this command to get a certificate and have Certbot edit our nginx configuration\\n   automatically to serve it, turning on HTTPS access in a single step.\\n\\n   :::note Prerequisite\\n\\n   1. A DNS domain record, e.g. \\"jenkins.my-domain.com\\", has been setup to use\\n   2. The server running Jenkins can be reached at port 80 publicly\\n\\n   :::\\n\\n   ```bash\\n   sudo certbot --nginx\\n   ```\\n\\n   Enter an email address of ours\\n\\n   ```bash\\n   $ sudo certbot --nginx\\n   Saving debug log to ...\\n   Enter email address (used for urgent renewal and security notices)\\n    (Enter \'c\' to cancel):\\n   ```\\n\\n   Next give content to the Terms of Service and decide whether or not to receive promotion emails from Let\'s Encrypt\\n   and Certbot\\n\\n   ```bash\\n   - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n   Please read the Terms of Service at\\n   https://letsencrypt.org/documents/LE-SA-v1.3-September-21-2022.pdf. You must\\n   agree in order to register with the ACME server. Do you agree?\\n   - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n   (Y)es/(N)o: Y\\n   - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n   Would you be willing, once your first certificate is successfully issued, to\\n   share your email address with the Electronic Frontier Foundation, a founding\\n   partner of the Let\'s Encrypt project and the non-profit organization that\\n   develops Certbot? We\'d like to send you email about our work encrypting the web,\\n   EFF news, campaigns, and ways to support digital freedom.\\n   - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n   (Y)es/(N)o: Y\\n    ```\\n\\n   As the last step, provide the domain name sitting in front of our Jenkins instance. Let\'s give it as\\n   \\"jenkins.my-domain.com\\"\\n\\n   ```bash\\n   Account registered.\\n   Please enter the domain name(s) you would like on your certificate (comma and/or\\n   space separated) (Enter \'c\' to cancel): jenkins.my-domain.com\\n   Requesting a certificate for jenkins.my-domain.com\\n   ```\\n\\n   If everything works successfully, we shall see the output similar to the one below\\n\\n   ```bash\\n   Successfully received certificate.\\n   Certificate is saved at: /path/to/server_crt.pem\\n   Key is saved at:         /path/to/server_key.pem\\n   This certificate expires on ****-**-**.\\n   These files will be updated when the certificate renews.\\n   Certbot has set up a scheduled task to automatically renew this certificate in the background.\\n\\n   Deploying certificate\\n   Successfully deployed certificate for jenkins.my-domain.com to /some/path/...\\n   Congratulations! You have successfully enabled HTTPS on <https://jenkins.my-domain.com>\\n\\n   ...\\n   ```\\n\\n   :::tip Troubleshooting\\n\\n   ```bash\\n   Please enter the domain name(s) you would like on your certificate (comma and/or\\n   space separated) (Enter \'c\' to cancel): jenkins.my-domain.com\\n   Requesting a certificate for jenkins.my-domain.com\\n\\n   Certbot failed to authenticate some domains (authenticator: nginx). The Certificate Authority reported these\\n   problems:\\n   Domain: jenkins.my-domain.com\\n   Type:   connection\\n   Detail: ...: Timeout during connect (likely firewall problem)\\n\\n   Hint: The Certificate Authority failed to verify the temporary nginx configuration changes made by Certbot. Ensure\\n   the listed domains point to this nginx server and that it is accessible from the internet.\\n\\n   Some challenges have failed.\\n   Ask for help or search for solutions at https://community.letsencrypt.org. See the logfile\\n   /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\\n   ```\\n\\n   This problem usually happens when we are re-deploying certificate. There are cases when we would like to deploy\\n   Jenkins to a new server or VM and we are setting everything up anew. We then would want to request the same\\n   certificate. _But before we do this our DNS record (i.e. \\"jenkins.my-domain.com\\") is still pointing to the old\\n   Jenkins instance which in most cases has already been unreachable_. The solution then is to delete that DNS record\\n   from our domain config\\n\\n   ```bash\\n   Please enter the domain name(s) you would like on your certificate (comma and/or\\n   space separated) (Enter \'c\' to cancel): jenkins.my-domain.com\\n   Requesting a certificate for jenkins.my-domain.com\\n\\n   Certbot failed to authenticate some domains (authenticator: nginx). The Certificate Authority reported these\\n   problems:\\n   Domain: jenkins.my-domain.com\\n   Type:   dns\\n   Detail: DNS problem: NXDOMAIN looking up A for jenkins.my-domain.com - check that a DNS record exists for this\\n   domain; DNS problem: NXDOMAIN looking up AAAA for jenkins.my-domain.com - check that a DNS record exists for this\\n   domain\\n\\n   Hint: The Certificate Authority failed to verify the temporary nginx configuration changes made by Certbot. Ensure\\n   the listed domains point to this nginx server and that it is accessible from the internet.\\n\\n   Some challenges have failed.\\n   Ask for help or search for solutions at https://community.letsencrypt.org. See the logfile\\n   /var/log/letsencrypt/letsencrypt.log or re-run Certbot with -v for more details.\\n   ```\\n\\n   This error is rather straightforward. After we delete the DNS record, we simply forgot to link the DNS record to\\n   a new server/VM IP\\n   :::\\n\\n   Remember the two paths from the command above: \\"**/path/to/server_crt.pem**\\" and \\"**/path/to/server_key.pem**\\". We\\n   will need them in order to [load certificate onto Jenkins instance](#convert-ssl-keys-to-pkcs12-format) later. In\\n   addition, under \\"/path/to/\\" there is also the CA certificate \\"**/path/to/cert.pem**\\", which we will be using\\n\\n   :::tip **Or**, just get a certificate\\n\\n   If you\'re feeling more conservative and would like to make the changes to the nginx configuration by hand, run this\\n   command instead of the one above:\\n\\n   ```bash\\n   sudo certbot certonly --nginx\\n   ```\\n\\n   :::\\n\\n8. **Test automatic renewal** The Certbot packages on our system come with a cron job or systemd timer that will renew\\n   our certificates automatically before they expire. We will not need to run Certbot again, unless we change our\\n   configuration. We can test automatic renewal for our certificates by running this command:\\n\\n   ```bash\\n   sudo certbot renew --dry-run\\n   ```\\n\\n   :::info\\n\\n   The command to renew certbot is installed in one of the following locations:\\n\\n   - `/etc/crontab/`\\n   - `/etc/cron.*/*`\\n   - `systemctl list-timers`\\n\\n   :::\\n\\n#### Convert SSL keys to PKCS12 format\\n\\nWhen [SSL is ready](#generating-ssl-certificates), let\'s enable secure connection to our Jenkins instance. We would need\\nthe following files:\\n\\n1. `server_crt.pem` - The signed certificate\\n2. `server_key.pem` - The certificate key\\n3. `cacert.pem` - The CA certificate\\n\\n**_In the case of [self-signed certificate](#self-signing-a-certificate)_**, under `~/myCA` directory, execute\\n\\n```bash\\nopenssl pkcs12 -export -out jenkins.p12 \\\\\\n-passout \'pass:your-strong-password\' -inkey server_key.pem \\\\\\n-in server_crt.pem -certfile cacert.pem -name jenkins.some-domain.com\\n```\\n\\n_**In the case of [publicly authorized certificate](#applying-for-a-certificate-using-certbot)**_, run\\n\\n```bash\\nsudo openssl pkcs12 -export \\\\\\n    -out jenkins.p12 \\\\\\n    -passout \'pass:your-strong-password\' \\\\\\n    -inkey /path/to/server_key.pem \\\\\\n    -in /path/to/server_crt.pem \\\\\\n    -certfile /path/to/cert.pem \\\\\\n    -name jenkins.some-domain.com\\n```\\n\\nwhere `your-strong-password` would be a password of our choice; for example, if you choose \\"sdfef3qxA\\" as the password,\\nput `-passout \'pass:sdfef3qxA\'` there. Replace all the occurrences of \\"your-secrete-password\\" seen below with\\n\\"sdfef3qxA\\". In adiition, _Replace \\"jenkins.some-domain.com\\" with our own CNAME_\\n\\nThe command given above converts SSL certs to intermediate PKCS12 format named **jenkins.p12**.\\n\\n#### Convert PKCS12 to JKS format\\n\\nUse the following keytool command to convert the `jenkins.p12` file to JKS format.\\n\\n```bash\\nkeytool -importkeystore -srckeystore jenkins.p12 \\\\\\n-srcstorepass \'your-secret-password\' -srcstoretype PKCS12 \\\\\\n-srcalias jenkins.some-domain.com -deststoretype JKS \\\\\\n-destkeystore jenkins.jks -deststorepass \'your-secret-password\' \\\\\\n-destalias jenkins.some-domain.com\\n```\\n\\nReplace the following with our own values:\\n\\n- `-srcstorepass` - [The password we created above](#convert-ssl-keys-to-pkcs12-format)\\n- `-deststorepass` - A new password we created in this step\\n- `-srcalias` - The \\"jenkins.some-domain.com\\" or our own CNAME we used\\n  [from the previous step](#convert-ssl-keys-to-pkcs12-format)\\n- `-destalias` - A destination CNAME, usually the same as `srcalias`\\n\\nWe should, after the command is executed, see a file named **jenkins.jks** in the current location.\\n\\n#### Add JKS to Jenkins Path\\n\\nThe jenkins.jks file should be saved in a specific location where Jenkins can access it. Let\'s create a folder for it\\nand move the jenkins.jks key to that location.\\n\\n```bash\\nsudo mkdir -p /etc/jenkins\\nsudo cp jenkins.jks /etc/jenkins/\\n```\\n\\nChange the permissions of the keys and folder.\\n\\n```bash\\nsudo chown -R jenkins: /etc/jenkins\\nsudo chmod 700 /etc/jenkins\\nsudo chmod 600 /etc/jenkins/jenkins.jks\\n```\\n\\n> The `jenkins:` used in the first command is a standard Jenkins user that get\'s created when we install\\n> [Jenkins package](#long-term-support-release). We are essentially granting permission for that user here so do not\\n> throw some other \\"smart\\" names there or change, please.\\n\\n#### Modify Jenkins Configuration for SSL\\n\\n```bash\\nsudo systemctl edit jenkins\\n```\\n\\n```conf\\n### Anything between here and the comment below will become the new contents of the file\\n\\n[Service]\\nEnvironment=\\"JENKINS_PORT=-1\\"\\nEnvironment=\\"JENKINS_HTTPS_PORT=443\\"\\nEnvironment=\\"JENKINS_HTTPS_KEYSTORE=/etc/jenkins/jenkins.jks\\"\\nEnvironment=\\"JENKINS_HTTPS_KEYSTORE_PASSWORD=<some strong password>\\"\\nEnvironment=\\"JENKINS_HTTPS_LISTEN_ADDRESS=0.0.0.0\\"\\n\\n### Lines below this comment will be discarded\\n```\\n\\n:::note\\n\\nThe config content above actually gets appended to the end of `systemctl cat jenkins` output, which we can verify to\\nmake sure that the changes here actually propagates to Jenkins runtime.\\n\\nIgnore the comments lines which starts with `### ...`\\n\\n:::\\n\\n#### Restart Jenkins\\n\\nThe restart command on Ubuntu C2 instance is\\n\\n```bash\\nsudo systemctl restart jenkins.service\\n```\\n\\nwhich would probably fail, however, with the error\\n\\n```bash\\n$ sudo systemctl restart jenkins.service\\nJob for jenkins.service failed because the control process exited with error code.\\nSee \\"systemctl status jenkins.service\\" and \\"journalctl -xeu jenkins.service\\" for details.\\n```\\n\\nWhenever Jenkins control failes with `systemctl`, we can use its diagnosing command `journalctl` to further investigate:\\n\\n```bash\\njournalctl -u jenkins.service -r\\n```\\n\\nwhich probably would show us the follwoing error\\n\\n```bash\\nCaused: java.io.IOException: Failed to bind to /0.0.0.0:443\\n        at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:339)\\n        at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\\n        at java.base/sun.nio.ch.Net.bind(Net.java:448)\\n        at java.base/sun.nio.ch.Net.bind(Net.java:459)\\n        at java.base/sun.nio.ch.Net.bind0(Native Method)\\njava.net.SocketException: Permission denied\\n```\\n\\nThis is because Jetty, which is the Jenkins server, does not have sudo permission to listen on SSL port 443 on startup.\\nWe could verify this by having Jenkins SSL port listen on a non-sudo port such as 8443. To do so, we will run\\n\\n```bash\\nsudo systemctl edit jenkins\\n```\\n\\nagain and change the line from\\n\\n```conf\\nEnvironment=\\"JENKINS_HTTPS_PORT=443\\"\\n```\\n\\nto\\n\\n```conf\\nEnvironment=\\"JENKINS_HTTPS_PORT=8443\\"\\n```\\n\\nWe will see that Jenkins process started:\\n\\n```bash\\n$ ps -aux | grep jenkins\\njenkins    46742 45.1 99.1 3245534 89876543 ?      Ssl  06:12   0:29 /usr/bin/java -Djava.awt.headless=true -jar\\n/usr/share/java/jenkins.war --webroot=/var/cache/jenkins/war --httpPort=-1 --httpsPort=8443 --httpsListenAddress=0.0.0.0\\n--httpsKeyStore=/etc/jenkins/jenkins.jks --httpsKeyStorePassword=**************\\n```\\n\\nWe have 2 options.\\n\\n1. Instruct `sudo systemctl restart jenkins.service` to execute the command above with `sudo`\\n2. Simply add an inbound rule to AWS EC2 security group to have Jenkins HTTPS listen on a port greater than 1024\\n\\nWe would go with the 2nd option in this post. If we want to open the port for HTTPS and it\'s not 443, then in addition\\nto adding HTTPS rule, we could also just add a \\"Custom TCP\\" rule for the desired port (i.e. 8443) and it will work.\\n\\nNow we should be able to access Jenkins over HTTPS with port 8443 at `https://jenkins.some-domain.com:8443`\\n\\n#### HTTPS via Nginx Reverse Proxy\\n\\nProxying is typically used to distribute the load among several servers, seamlessly show content from different\\nwebsites, or pass requests for processing to application servers over protocols other than HTTP.\\n\\n:::info What Is a Reverse Proxy Server?\\n\\nA proxy server is a go\u2011between or intermediary server that forwards requests for content from multiple clients to\\ndifferent servers across the Internet. A **reverse proxy server** is a type of proxy server that typically sits behind\\nthe firewall in a private network and directs client requests to the appropriate backend server. A reverse proxy\\nprovides an additional level of abstraction and control to ensure the smooth flow of network traffic between clients and\\nservers.\\n\\nCommon uses for a reverse proxy server include:\\n\\n- **Load balancing** A reverse proxy server can act as a \\"traffic cop\\" sitting in front of backend servers and\\n  distributing client requests across a group of servers in a manner that maximizes speed and capacity utilization\\n  while ensuring no one server is overloaded, which can degrade performance. If a server goes down, the load balancer\\n  redirects traffic to the remaining online servers.\\n- **Web acceleration** Reverse proxies can compress inbound and outbound data, as well as cache commonly requested\\n  content, both of which speed up the flow of traffic between clients and servers. They can also perform additional\\n  tasks such as SSL encryption to take load off of your web servers, thereby boosting their performance.\\n- **Security & Anonymity** By intercepting requests headed for your backend servers, a reverse proxy server protects\\n  their identities and acts as an additional defense against security attacks. It also ensures that multiple servers can\\n  be accessed from a single record locator or URL regardless of the structure of your local area network.\\n\\n:::\\n\\n![Error loading nginx-directives.png](nginx-directives.png)\\n\\n##### Understanding the Nginx Configuration File Structure and Configuration Contexts\\n\\nWhile we know Certbot utilizes Nginx\'s capabilities, people are often confused by some of the conventions they find in\\nNginx configuration files. In this section, we will focus first on the basic structure of an Nginx configuration file.\\n\\nThe main Nginx configuration file is located at \\"**/etc/nginx/nginx.conf**\\" in our EC2 Ubuntu instance.\\n\\nOne of the first things that we should notice when looking at the main configuration file is that it appears to be\\norganized in a tree-like structure, defined by sets of brackets (that look like `{` and `}`). In Nginx parlance, the\\nareas that these brackets define are called \u201ccontexts\u201d because they contain configuration details that are separated\\naccording to their area of concern. Basically, these divisions provide an organizational structure along with some\\nconditional logic to decide whether to apply the configurations within.\\n\\nBecause contexts can be layered within one another, Nginx provides a level of directive inheritance. As a general rule,\\nif a directive is valid in multiple nested scopes, a declaration in a broader context will be passed on to any child\\ncontexts as default values. The children contexts can override these values at will. It is worth noting that an override\\nto any array-type directives will replace the previous value, not append to it.\\n\\nDirectives can only be used in the contexts that they were designed for. Nginx will error out on reading a configuration\\nfile with directives that are declared in the wrong context. The\\n[Nginx documentation](http://nginx.org/en/docs/dirindex.html) contains information about which contexts each directive\\nis valid in, so it is a great reference if you are unsure.\\n\\nNext, we\'ll discuss the most common contexts in Nginx.\\n\\n###### The Main Context\\n\\nThe most general context is the \\"main\\" or \\"global\\" context. It is the only context that is not contained within the\\ntypical context blocks that look like this:\\n\\n```config\\n# The main context is here, outside any other contexts\\n\\n. . .\\n\\ncontext {\\n\\n    . . .\\n\\n}\\n```\\n\\nAny directive that exist entirely outside of these blocks is said to inhabit the \\"main\\" context. Keep in mind that if\\nour Nginx configuration is set up in a modular fashion, some files will contain instructions that appear to exist\\noutside of a bracketed context, but which will be included within such a context when the configuration is stitched\\ntogether.\\n\\nThe main context represents the broadest environment for Nginx configuration. It is used to configure details that\\naffect the entire application on a basic level. While the directives in this section affect the lower contexts, many of\\nthese aren\'t inherited because they cannot be overridden in lower levels.\\n\\nSome common details that are configured in the main context are the user and group to run the worker processes as, the\\nnumber of workers, and the file to save the main process\'s PID. We can even define things like worker CPU affinity and\\nthe \\"niceness\\" of worker processes. The default error file for the entire application can be set at this level (this can\\nbe overridden in more specific contexts).\\n\\n###### The Events Context\\n\\nThe \\"events\\" context is contained within the \\"main\\" context. It is used to set global options that affect how Nginx\\n**handles connections** at a general level. There can only be a single events context defined within the Nginx\\nconfiguration.\\n\\nThis context will look like this in the configuration file, outside of any other bracketed contexts:\\n\\n```conf\\n# main context\\n\\nevents {\\n\\n    # events context\\n    . . .\\n\\n}\\n```\\n\\nNginx uses an event-based connection processing model, so the directives defined within this context determine how\\nworker processes should handle connections. Mainly, directives found here are used to either select the connection\\nprocessing technique to use, or to modify the way these methods are implemented.\\n\\nUsually, the connection processing method is automatically selected based on the most efficient choice that the platform\\nhas available. For Linux systems, the _epoll_ method is usually the best choice.\\n\\nOther items that can be configured are the number of connections each worker can handle, whether a worker will only take\\na single connection at a time or take all pending connections after being notified about a pending connection, and\\nwhether workers will take turns responding to events.\\n\\n###### The HTTP Context\\n\\nWhen configuring Nginx as a web server or **reverse proxy**, the \\"http\\" context will hold the majority of the\\nconfiguration. This context will contain all of the directives and other contexts necessary to define how the program\\nwill handle HTTP or **HTTPS** connections.\\n\\nThe http context is a sibling of the events context, so they should be listed side-by-side, rather than nested. They\\nboth are children of the main context:\\n\\n```conf\\n# main context\\n\\nevents {\\n    # events context\\n\\n    . . .\\n\\n}\\n\\nhttp {\\n    # http context\\n\\n    . . .\\n\\n}\\n```\\n\\nWhile lower contexts get more specific about how to handle requests, directives at this level control the defaults for\\nevery virtual server defined within. A large number of directives are configurable at this context and below, depending\\non how we would like the inheritance to function.\\n\\nSome of the directives that we are likely to encounter control the default locations for access and error logs\\n(**access_log** and **error_log**), configure asynchronous I/O for file operations (aio, sendfile, and directio), and\\nconfigure the server\'s statuses when errors occur (error_page). Other directives configure compression (gzip and\\ngzip_disable), fine-tune the TCP keep alive settings (**keepalive_disable**, **keepalive_requests**, and\\n**keepalive_timeout**), and the rules that Nginx will follow to try to optimize packets and system calls (sendfile,\\ntcp_nodelay, and tcp_nopush). Additional directives configure an application-level document root and index files (root\\nand index) and set up the various hash tables that are used to store different types of data (*_hash_bucket_size and\\n*_hash_max_size for server_names, types, and variables).\\n\\n###### The Server Context\\n\\nThe \\"server\\" context is declared within the \\"http\\" context. This is our first example of nested, bracketed contexts. It\\nis also the first context that allows for multiple declarations.\\n\\nThe general format for server context may look something like this. Remember that these reside within the http context:\\n\\n```conf\\n# main context\\n\\nhttp {\\n\\n    # http context\\n\\n    server {\\n\\n        # first server context\\n\\n    }\\n\\n    server {\\n\\n        # second server context\\n\\n    }\\n\\n}\\n```\\n\\nThe reason for allowing multiple declarations of the server context is that **each instance defines a specific virtual\\nserver to handle client requests**. You can have as many server blocks as you need, each of which can handle a specific\\nsubset of connections.\\n\\nDue to the possibility and likelihood of multiple server blocks, this context type is also the first that Nginx must use\\na selection algorithm to make decisions. Each client request will be handled according to the configuration defined in a\\nsingle server context, so Nginx must decide which server context is most appropriate based on details of the request.\\nThe directives which decide if a server block will be used to answer a request are:\\n\\n- **listen**: The _IP:port_ combination that this server block is designed to respond to. If a request is made by a\\n  client that matches these values, this block will potentially be selected to handle the connection.\\n- **server_name**: This directive is the other component used to select a server block for processing. If there are\\n  multiple server blocks with listen directives of the same specificity that can handle the request, Nginx will parse\\n  the \\"Host\\" header of the request and match it against this directive.\\n\\nThe directives in this context can override many of the directives that may be defined in the http context, including\\nlogging, the document root, compression, etc. In addition to the directives that are taken from the http context, we\\nalso can configure files to try to respond to requests (try_files), issue redirects and rewrites (return and rewrite),\\nand set arbitrary variables (set).\\n\\n###### The Location Context\\n\\nThe next context that we will deal with regularly is the location context. Location contexts share many relational\\nqualities with server contexts. For example, multiple location contexts can be defined, each location is used to handle\\na certain type of client request, and each location is selected by virtue of matching the location definition against\\nthe client request through a selection algorithm.\\n\\nWhile the directives that determine whether to select a server block are defined within the server context, the\\ncomponent that decides on a location\'s ability to handle a request is located in the location definition (the line that\\nopens the location block).\\n\\nThe general syntax looks like this:\\n\\n```config\\nlocation match_modifier location_match {\\n\\n    . . .\\n\\n}\\n```\\n\\nLocation blocks live within server contexts and, unlike server blocks, can be nested inside one another. This can be\\nuseful for creating a more general location context to catch a certain subset of traffic, and then further processing it\\nbased on more specific criteria with additional contexts inside:\\n\\n```conf\\n# main context\\n\\nserver {\\n\\n    # server context\\n\\n    location /match/criteria {\\n\\n        # first location context\\n\\n    }\\n\\n    location /other/criteria {\\n\\n        # second location context\\n\\n        location nested_match {\\n\\n            # first nested location\\n\\n        }\\n\\n        location other_nested {\\n\\n            # second nested location\\n\\n        }\\n\\n    }\\n\\n}\\n```\\n\\nWhile server contexts are selected based on the requested IP address/port combination and the host name in the \\"Host\\"\\nheader, location blocks further divide up the request handling within a server block by looking at the request URI. The\\nrequest URI is the portion of the request that comes after the domain name or IP address/port combination.\\n\\nFor example, if a client requests `http://www.example.com/blog` on port 80, the `http`, `www.example.com`, and port 80\\nwould all be used to determine which server block to select. After a server is selected, the \\"/blog\\" portion (the\\nrequest URI), would be evaluated against the defined locations to determine which further context should be used to\\nrespond to the request.\\n\\nMany of the directives you are likely to see in a location context are also available at the parent levels. New\\ndirectives at this level allow you to reach locations outside of the document root (alias), mark the location as only\\ninternally accessible (internal), and proxy to other servers or locations (using http, fastcgi, scgi, and uwsgi\\nproxying).\\n\\n##### How Certbot + Nginx Enable SSL\\n\\nRecall that we have made our Jenkins instance available at `https://jenkins.some-domain.com:8443` with a port number of\\n\\"8443\\". If we would like to get rid of it and simply visit Jenkins at `https://jenkins.some-domain.com`, we will add an\\nredirect to Certbot-managed Nginx config at **/etc/nginx/sites-enabled/default**:\\n\\n```config\\nserver {\\n\\n    location / {\\n        proxy_pass https://localhost:8443;\\n    }\\n\\n    listen [::]:443 ssl ipv6only=on;\\n    listen 443 ssl;\\n    ssl_certificate ...;\\n    ssl_certificate_key ...;\\n}\\n```\\n\\nNote the proxying rule of `proxy_pass https://localhost:8443;`. If, instead, we have some other app that runs on top of\\nHTTP only (with port 8080 for example) while our EC2 host has SSL certificate, we would simply have our EC2 instance\\ntaking care of all the SSL stuff and proxy all HTTPS request to the HTTP app via `proxy_pass http://localhost:8080;`\\n\\n:::note\\n\\nWe may need to restart EC2 instance for the change above to take effect\\n\\n:::\\n\\n### Post-Installation Setup Wizard\\n\\nAfter downloading, installing and running Jenkins on SSL, the post-installation setup wizard begins.\\n\\nThe setup wizard takes us through a few quick \\"one-off\\" steps to unlock Jenkins, customize it with plugins and create\\nthe first administrator user through which we can continue accessing Jenkins.\\n\\n#### Unlocking Jenkins\\n\\nWhen we first access a new Jenkins instance, we are asked to unlock it using an automatically-generated password. Browse\\nto `https://jenkins.my-domain.com:8443` and wait until the Unlock Jenkins page appears.\\n\\n![Error loading unlock-jenkins.png](unlock-jenkins.png)\\n\\nAs prompted, enter the password found in **/var/lib/jenkins/secrets/initialAdminPassword** on our EC2 instance path. Use\\nthe following command to display this password:\\n\\n```bash\\n[ec2-user ~]$ sudo cat /var/lib/jenkins/secrets/initialAdminPassword\\n```\\n\\nOn the Unlock Jenkins page, paste this password into the **Administrator password** field and click **Continue**.\\n\\n:::note\\n\\nThis password also serves as the default administrator account\'s password (with username \\"admin\\") if we happen to skip\\nthe subsequent user-creation step in the setup wizard (So, DO NOT skip this subsequent step).\\n\\n:::\\n\\n#### Customizing Jenkins with Plugins\\n\\nAfter [unlocking Jenkins](#unlocking-jenkins), the Customize Jenkins page appears. Here we can install any number of\\nuseful plugins as part of our initial setup.\\n\\nClick one of the two options shown:\\n\\n- **Install suggested plugins** - to install the recommended set of plugins, which are based on most common use cases.\\n- **Select plugins to install** - to choose which set of plugins to initially install. When we first access the plugin\\n  selection page, the suggested plugins are selected by default.\\n\\n:::tip\\n\\nIf we are not sure what plugins we need, choose **Install suggested plugins**. We can install (or remove) additional\\nJenkins plugins at a later point in time via the\\n[**Manage Jenkins**](https://www.jenkins.io/doc/book/managing/) >\\n[**Manage Plugins page in Jenkins**](https://www.jenkins.io/doc/book/managing/plugins/).\\n\\n:::\\n\\nThe setup wizard shows the progression of Jenkins being configured and our chosen set of Jenkins plugins being\\ninstalled. This process may take a few minutes.\\n\\n#### Creating the First Administrator User\\n\\n![Error loading create-admin-user.png](create-admin-user.png)\\n\\nFinally, after [customizing Jenkins with plugins](#customizing-jenkins-with-plugins), Jenkins asks us to create our\\nfirst administrator user.\\n\\n1. When the **Create First Admin User** page appears, specify the details for our administrator user in the respective\\n   fields and click **Save and Finish**.\\n2. When the **Jenkins is ready** page appears, click **Start using Jenkins**.\\n\\n   - This page may indicate **Jenkins is almost ready!** instead and if so, click **Restart**.\\n   - If the page does not automatically refresh after a minute, use our web browser to refresh the page manually.\\n\\n3. If required, log in to Jenkins with the credentials of the user we just created and we are ready to start using\\n   Jenkins!\\n\\nTroubleshooting\\n---------------\\n\\n### Docker Permission Error When Trigger by Jenkins\\n\\nWhen a job **Build Steps** contains Shell Execution which includes some Docker command, it might fail with\\n\\n```bash\\n+ docker build ...\\nGot permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \\"...\\":\\ndial unix /var/run/docker.sock: connect: permission denied\\n```\\n\\nThe user \\"jenkins\\" needs to be added to the group docker:\\n\\n```bash\\nsudo usermod -a -G docker jenkins\\n```\\n\\nThen restart Jenkins. (and reboot server if not working)\\n\\n### Echo Off in Jenkins Console Output\\n\\nBy default, Jenkins launches Execute Shell script with `set -x`. This causes all commands to be echoed. We can type\\n`set +x` before any command to temporary override that behavior. Of course you will need `set -x` to start showing them\\nagain.\\n\\nYou can override this behaviour for the whole script by putting the following at the top of the build step:\\n\\n```bash\\n#!/bin/bash +x\\n```"},{"id":"aws-troubleshooting","metadata":{"permalink":"/blog/aws-troubleshooting","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-10-12-aws-troubleshooting/index.md","source":"@site/blog/2022-10-12-aws-troubleshooting/index.md","title":"AWS Troubleshooting","description":"New Volume in EC2 Instance Not Reflecting","date":"2022-10-12T00:00:00.000Z","formattedDate":"October 12, 2022","tags":[{"label":"AWS","permalink":"/blog/tags/aws"}],"readingTime":0.705,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"aws-troubleshooting","title":"AWS Troubleshooting","authors":["jiaqi"],"tags":["AWS"]},"unlisted":false,"prevItem":{"title":"Deploying Jenkins to AWS","permalink":"/blog/jenkins-on-aws"},"nextItem":{"title":"AWS Identity and Access Management (IAM)","permalink":"/blog/aws-iam"}},"content":"\x3c!--truncate--\x3e\\n\\nNew Volume in EC2 Instance Not Reflecting\\n-----------------------------------------\\n\\nWhen we have increased the size of the volume attached a running EC2 instance. We are able to see the new volume using\\n`lsblk`:\\n\\n![Error loading ec2-volume-1.png](ec2-volume-1.png)\\n\\nBut when `df -h` command still displays the old volume size:\\n\\n![Error loading ec2-volume-2.png](ec2-volume-2.png)\\n\\nThis is because new volumes should be formatted to be accessible. Resized existing volumes should also be modified\\n(resized) from the inside of the operating system. The general information on how to do this safely (e.g. with snapshots)\\nis given in the following AWS documentation:\\n\\n* [Making an Amazon EBS volume available for use on Linux](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html)\\n* [Extending a Linux file system after resizing a volume](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/recognize-expanded-volume-linux.html)\\n\\n* Based on the discussion in comments, two commands were used to successfully solve the problem:\\n\\n```bash\\nsudo growpart /dev/xvda 1\\nsudo resize2fs /dev/xvda1\\n```"},{"id":"aws-iam","metadata":{"permalink":"/blog/aws-iam","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-10-09-aws-iam/index.md","source":"@site/blog/2022-10-09-aws-iam/index.md","title":"AWS Identity and Access Management (IAM)","description":"IAM provides the infrastructure necessary to control authentication and authorization for a user\'s account.","date":"2022-10-09T00:00:00.000Z","formattedDate":"October 9, 2022","tags":[{"label":"Security","permalink":"/blog/tags/security"}],"readingTime":9.335,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"aws-iam","title":"AWS Identity and Access Management (IAM)","authors":["jiaqi"],"tags":["Security"]},"unlisted":false,"prevItem":{"title":"AWS Troubleshooting","permalink":"/blog/aws-troubleshooting"},"nextItem":{"title":"Nexus 3 Repository Manager OSS","permalink":"/blog/nexus"}},"content":"IAM provides the infrastructure necessary to control authentication and authorization for a user\'s account.\\n\\n\x3c!--truncate--\x3e\\n\\nThe IAM infrastructure includes the following elements:\\n\\n![Error loading intro-diagram-policies-800.png](intro-diagram-policies-800.png)\\n\\n* **IAM Resources** The user, group, role, policy, and identity provider objects that are stored in IAM. As with other\\n  AWS services, we can add, edit, and remove resources from IAM. A resource is an object that exists within a service.\\n  Examples include an Amazon EC2 instance, an IAM user, and an Amazon S3 bucket. **The service defines a set of actions\\n  that can be performed on each resource**. If you create a request to perform an unrelated action on a resource, that\\n  request is denied. For example, if you request to delete an IAM role but provide an IAM group resource, the request\\n  fails.\\n* **IAM Identities** The IAM resource objects that are used to identify and group. We can attach a policy to an IAM\\n  identity. These include users, groups, and roles.\\n* **IAM Entities** The IAM resource objects that AWS uses for authentication. These include IAM users and roles.\\n* **Principals** A person or application that can make a request for an action or operation on an AWS resource. The\\n  principal is authenticated as the AWS account root user or an IAM entity to make requests to AWS. As a best practice,\\n  do not use root user credentials for daily work. Instead, create IAM entities (users and roles). We can also support\\n  federated users or programmatic access to allow an application to access our AWS account.\\n\\n  When a principal tries to use the AWS Management Console, the AWS API, or the AWS CLI, that principal sends a request\\n  to AWS. The request includes the following information\\n\\n    - **Actions or operations** The actions or operations that the principal wants to perform. This can be an action in\\n      the AWS Management Console, or an operation in the AWS CLI or AWS API.\\n    - **Resources** The AWS resource object upon which the actions or operations are performed.\\n    - **Principal** The person or application that used an entity (user or role) to send the request. Information about\\n      the principal includes the policies that are associated with the entity that the principal used to sign in.\\n    - **Environment data** Information about the IP address, user agent, SSL enabled status, or the time of day.\\n    - **Resource data** Data related to the resource that is being requested. This can include information such as a\\n      DynamoDB table name or a tag on an Amazon EC2 instance.\\n\\n  AWS gathers the request information into a request context, which is used to evaluate and authorize the request.\\n* **Authentication**  A principal must be authenticated (signed in to AWS) using their credentials to send a request to\\n  AWS. Some services, such as Amazon S3 and AWS STS, allow a few requests from anonymous users. However, they are the\\n  exception to the rule.\\n\\n  To authenticate from the console as a root user, we must sign in with our email address and password. As an IAM user,\\n  provide our account ID or alias, and then our user name and password. To authenticate from the API or AWS CLI, we must\\n  provide our access key and secret key. We might also be required to provide additional security information. For\\n  example, AWS recommends that we use multi-factor authentication (MFA) to increase the security of our account.\\n* **Authorization** We must also be authorized (allowed) to complete our request. During authorization, AWS uses values\\n  from the request context to check for policies that apply to the request. It then uses the policies to determine whether\\n  to allow or deny the request. **Most policies are stored in AWS as JSON documents** and specify the permissions for\\n  principal entities. There are several types of policies that can affect whether a request is authorized. _To provide\\n  our users with permissions to access the AWS resources in their own account, we need only identity-based policies_.\\n  Resource-based policies are popular for granting cross-account access. The other policy types are advanced features\\n  and should be used carefully.\\n\\n  AWS checks each policy that applies to the context of a request. If a single permissions policy includes a denied\\n  action, AWS denies the entire request and stops evaluating. This is called an **explicit deny**. Because requests are\\n  denied by default, AWS authorizes a request only if every part of the request is allowed by the applicable permissions\\n  policies.\\n\\n### Create IAM Admin User and User Group\\n\\nAs a best practice, do not use the AWS account root user for any task where it\'s not required. Instead,\\n[create a new IAM user for each person that requires administrator access][create IAM admin]. Then make those users\\nadministrators by placing the users into an \\"Administrators\\" user group to which you attach the AdministratorAccess\\nmanaged policy.\\n\\n> \u26a0\ufe0f **Safeguard our root user credentials and don\'t use them for everyday tasks** \u26a0\ufe0f\\n>\\n> When we create an AWS account you establish a root username and password to sign in to the AWS Management Console.\\n> Safeguard our root user credentials the same way we would protect other sensitive personal information. We can do\\n> this by configuring MFA for our root user credentials. It is not recommended to generate access keys for our root\\n> user, because they allow full access to all our resources for all AWS services, including our billing information.\\n> Don\'t use our root user for everyday tasks. Use the root user to complete the tasks that only the root user can\\n> perform. For the complete list of these tasks, see [Tasks that require root user credentials][root user tasks] in the\\n> _AWS General Reference_.\\n\\n### Identities\\n\\n#### User Groups\\n\\nAn IAM user group is a collection of IAM users. User groups let you specify permissions for multiple users, which can\\nmake it easier to manage the permissions for those users. For example, you could have a user group called Admins and\\ngive that user group typical administrator permissions. Any user in that user group automatically has Admins group\\npermissions. If a new user joins your organization and needs administrator privileges you can assign the appropriate\\npermissions by adding the user to the Admins user group. If a person changes jobs in your organization, instead of\\nediting that user\'s permissions you can remove him or her from the old user groups and add him or her to the appropriate\\nnew user groups.\\n\\nHere are some important characteristics of user groups:\\n\\n* A user group can contain many users, and a user can belong to multiple user groups.\\n* User groups can\'t be nested; they can contain only users, not other user groups.\\n* There is no default user group that automatically includes all users in the AWS account. If you want to have a user\\n  group like that, you must create it and assign each new user to it.\\n* The number and size of IAM resources in an AWS account, such as the number of groups, and the number of groups that a user can be a member of, are limited. For more information, see\\n  [IAM and AWS STS quotas, name requirements, and character limits](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html).\\n\\nThe following diagram shows a simple example of a small company. The company owner creates an **Admins** user group for\\nusers to create and manage other users as the company grows. The Admins user group creates a Developers user group and a\\nTest user group. Each of these user groups consists of users (humans and applications) that interact with AWS (Jim,\\nBrad, DevApp1, and so on). Each user has an individual set of security credentials. In this example, each user belongs\\nto a single user group. However, users can belong to multiple user groups.\\n\\n![Error loading relationship-between-entities-example-diagram.png](relationship-between-entities-example-diagram.png)\\n\\nReferences\\n\\n* [Creating IAM user groups](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_create.html)\\n* [Managing IAM user groups](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_manage.html)\\n\\n### Access Management\\n\\nWe manage access in AWS by creating policies and attaching them to IAM [identities](#identities) (users, groups of\\nusers, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource,\\ndefines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request.\\nPermissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON\\ndocuments. AWS supports six types of policies:\\n\\n1. [identity-based policies](#identity-based-policies)\\n2. [resource-based policies](#resource-based-policies)\\n3. permissions boundaries\\n4. Organizations SCPs\\n5. ACLs,\\n6. and session policies.\\n\\n#### Identity-Based Policies\\n\\nIdentity-based policies are JSON permissions policy documents that control what actions an identity (users, groups of\\nusers, and roles) can perform, on which resources, and under what conditions. Identity-based policies can be further\\ncategorized:\\n\\n* **Managed policies** - Standalone identity-based policies that you can attach to multiple users, groups, and roles in\\n  your AWS account. There are two types of managed policies:\\n    - **AWS managed policies** - Managed policies that are created and managed by AWS.\\n    - **Customer managed policies** - Managed policies that you create and manage in your AWS account. Customer managed\\n      policies provide more precise control over your policies than AWS managed policies.\\n* **Inline policies** - Policies that you add directly to a single user, group, or role. Inline policies maintain a\\n  strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity.\\n\\nTo learn how to choose between managed and inline policies, see\\n[Choosing between managed policies and inline policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#choosing-managed-or-inline).\\n\\n#### Resource-Based Policies\\n\\nResource-based policies are JSON policy documents that we attach to a resource such as an Amazon S3 bucket. The policies\\ngrant the specified principal permission to perform specific actions on that resource and defines under what conditions\\nthis applies. _Resource-based policies are inline policies_; there are no managed resource-based policies.\\n\\nTo enable cross-account access, we can specify an entire account or IAM entities in another account as the principal in\\na resource-based policy. Adding a cross-account principal to a resource-based policy, however, is only half of\\nestablishing the trust relationship. When the principal and the resource are in separate AWS account, we must also use\\nan identity-based policy to grant the principal access to the resource. However, if a resource-based policy grants\\naccess to a principal in the same account, no additional identity-based policy is required.  For step-by step\\ninstructions for granting cross-account access, see\\n[IAM tutorial: Delegate access across AWS accounts using IAM roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html).\\n\\nThe IAM service supports only one type of resource-based policy called **role trust policy**, which is attached to an\\nIAM role. _An IAM role is both an identity and a resource that supports resource-based policies_. For that reason, we\\nmust attach both a trust policy and an identity-based policy to an IAM role. Trust policies define which principal\\nentities (accounts, users, roles, and federated users) can assume the role. To learn how IAM roles are different from\\nother resource-based policies, see\\n[How IAM roles differ from resource-based policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html).\\n\\nTo see which other services support resource-based policies, see\\n[AWS services that work with IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html). To learn more about resource-based policies, see [Identity-based policies and resource-based policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html). To learn whether principals in accounts outside of your zone of trust (trusted organization or account) have access to assume your roles, see\\n[What is IAM Access Analyzer?](https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html)."},{"id":"nexus","metadata":{"permalink":"/blog/nexus","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-09-30-nexus/index.md","source":"@site/blog/2022-09-30-nexus/index.md","title":"Nexus 3 Repository Manager OSS","description":"The proliferation of different repository formats and tools accessing them as well as the emergence of more publicly","date":"2022-09-30T00:00:00.000Z","formattedDate":"September 30, 2022","tags":[{"label":"Nexus","permalink":"/blog/tags/nexus"}],"readingTime":44.345,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"nexus","title":"Nexus 3 Repository Manager OSS","authors":["jiaqi"],"tags":["Nexus"]},"unlisted":false,"prevItem":{"title":"AWS Identity and Access Management (IAM)","permalink":"/blog/aws-iam"},"nextItem":{"title":"Continuous Delivery","permalink":"/blog/continuous-delivery"}},"content":"The proliferation of different repository formats and tools accessing them as well as the emergence of more publicly\\navailable repositories has triggered the need to manage access and usage of these repositories and the components they\\ncontain.\\n\\n\x3c!--truncate--\x3e\\n\\nHosting our private repositories for internal components has proven to be a very efficient methodology to exchange\\ncomponents during all phases of the software development lifecycle. It is considered a best practice at this stage.\\n\\nThe task of managing all the repositories a development teams interact with can be supported by the use of a dedicated\\nserver application - a **repository manager**. To put it simply, a repository manager provides two core features:\\n\\n1. **the ability of proxying a remote repository and cache components saving both bandwidth and time required to\\n   retrieve a software component from a remote repository repeatedly**\\n2. **the ability of hosting a repository providing an organization with a deployment target for internal software\\n   components**\\n\\nJust as Source Code Management (SCM) tools are designed to manage source code, repository managers have been designed to\\nmanage and trace external dependencies and components generated by internal build.\\n\\nRepository managers are an essential part of any enterprise or open-source software development effort and they enable\\ngreater collaboration between developers and wider distribution of software by facilitating the exchange and usage of\\nbinary components.\\n\\nWhen we install a repository manager, we are bringing the power of a repository like the Central Repository into our\\norganization. We can use it to proxy the Central Repositories and other repositories, and host our own repositories for\\ninternal and external use.\\n\\nIn addition to the two aforementioned core features, a repository manager can support the following use cases\\n\\n- allows us to manage binary software components through the software development lifecycle\\n- search and catalogue software components\\n- control component releases with rules and add automated notifications\\n- integrate with external security systems, such as LDAP\\n- manage component metadata\\n- control access to components and repositories\\n- display component dependencies\\n- brose component archive contents\\n\\nUsing a repository manager provides a number of benefits, including\\n\\n- improved software build performance due to faster component download off the local repository manager\\n- reduced bandwidth usage due to component caching\\n- higher predictability and scalability due to limited dependency on external repositories\\n- increased understanding of component usage due to centralized storage of all used components\\n- simplified developer configuration due to central access configuration to remote repositories and components on the\\n  repository manager\\n- unified method to provide components to consumer reducing complexity overheads\\n- improved collaboration due to the simplified exchange of binary components\\n\\n\x3c!--truncate--\x3e\\n\\nSystem Requirements\\n-------------------\\n\\n### Memory Requirements\\n\\nThe requirements assume there are no other significant memory hungry processes running on the same host.\\n\\n| ****                     | **JVM Heap** | **JVM Direct**                           | **Host Physical/RAM** |\\n|:------------------------:|:------------:|:----------------------------------------:|:---------------------:|\\n| **Minimum ( default ) ** | 2703MB       | 2703MB                                   | 8GB                   |\\n| **Maximum**              | 4GB          | (host physical/RAM * 2/3) - JVM max heap | no limit              |\\n\\n### Disk Space\\n\\n**Application Directory** - The size of this directory varies slightly each release. It currently around 330 MB. It is\\nnormal to have multiple application directories installed on the same host over time as repository manager is upgraded.\\n\\n**Data Directory** - On first start, repository manager creates the base files needed to operate. The bulk of disk\\nspace will be held by our deployed and proxied artifacts, as well as any search indexes. This is highly installation\\nspecific, and will be dependent on the repository formats used, the number of artifacts stored, the size of our teams\\nand projects, etc.  It\'s best to plan for a lot though, formats like Docker and Maven can use very large amounts of\\nstorage (500Gb easily).  **When available disk space drops below 4GB the database will switch to read-only mode**.\\n\\nConcepts\\n--------\\n\\nThe Nexus Repository Manager OSS is all about working with **components** and **repositories**.\\n\\n### Components\\n\\nA component is a resource like a library or a framework that is used as part of a software application at run-time,\\nintegration or unit test execution time or required as part of build process. It could be an entire application or a\\nstatic resource like an image.\\n\\nTypically these components are archives of a large variety of files, such as Java bytecode in class files, text files,\\nor binary files such as images, PDFs, and music files. The archives have numerous formats such as JAR, WAR, ZIP, NPM\\npackages, or .sh\\n\\nComponents can be composed of multiple, nested components themselves. For example, consider a Java web application\\npackaged as a WAR component. It contains a number of JAR components and a number of JavaScript libraries. All of these\\nare standalone components in other contexts and happend to be included as part of the WAR component.\\n\\nComponents provide all the building blocks and features that allow a development team to create powerful applications\\nby assembling them and adding their own business related components to create a full-fledged, powerful application.\\n\\nComponents, in other tool-chains, are called artifacts, packages, bundles, archives, and other terms. The concept and\\nidea, however, remain the same and component is used as the independent, generic term.\\n\\n### Repository\\n\\nA wide variety of components exists and more are continuously created by the open source community as well as\\nproprietary vendors. These are libraries and frameworks written in various languages on different platforms that are\\nused for application development every day. It has become a default pattern to build applications by combining the\\nfeatures of multiple components with our own custom components containing our application code to create an application\\nfor a specific domain\\n\\nIn order to ease the consumption and usage of components, they are aggregated into collection of components. These are\\ncalled **repositories** and are typically available on the internet as a service. On different platforms terms such as\\nregistry and others are used for the same concept.\\n\\nExamples for such repositories are\\n\\n- the Central Repository, also known as Maven Central\\n- the NuGet Gallery\\n- RubyGems.org\\n- npmjs.org\\n\\nComponents in these repositories are accessed by numerous tools including\\n\\n- package managers like npm, nuget or gem\\n- build tools such as Maven, Gradle, rake or grunt\\n- IDE\'s such as Eclipse and IntelliJ\\n\\n### Repository Format\\n\\nThe different repositories use different technologies to store and expose the components in them to client tools. This\\ndefines a repository format and as such is closely related to the tools interacting with the repository.\\n\\nFor example, the Maven repository format relies on a specific directory structure defined by the identifiers of the\\ncomponents and a number of XML formatted files for metadata. Component interaction is performed via plain HTTP commands\\nand some additional custom interaction with the XML files.\\n\\nOther repositories formats use database for storage and REST API interactions, or different directory structures wit\\nformat specific files for the metadata\\n\\nRepository Management\\n---------------------\\n\\nWe\'ve seen that repositories are the containers for the components provided to our users. Creating and managing\\nrepositories is an essential part of our Nexus Repository configuration, since it allows us to expose more components\\nto our users. It supports proxy repositories, hosted repositories and repository groups in a number of different\\nrepository formats.\\n\\n:::info\\n\\nTo manage repositories select the **Repositories** item in the Repository sub menu of the **Administration** menu.\\n\\n:::\\n\\nThe binary parts of a repository are stored in **blob stores**, which can be configured by selecting Blob Stores from\\nthe Repository sub menu of the Administration menu.\\n\\n### Repository Types\\n\\n#### Proxy Repository\\n\\n**A repository with the type proxy, also known as a proxy repository, is a repository that is linked to a remote\\nrepository**. Any request for a component is verified against the local content of the proxy repository. If no local\\ncomponent is found, the request is forwarded to the remote repository. The component is then retrieved and stored\\nlocally in the repository manager, which acts as a cache. Subsequent requests for the same component are then fulfilled\\nfrom the local storage, therefore eliminating the network bandwidth and time overhead of retrieving the component from\\nthe remote repository again.\\n\\nBy default, the repository manager ships with the following configured proxy repositories:\\n\\n- **maven-central** This proxy repository accesses the Central Repository, formerly known as Maven Central. It is the\\n  default component repository built into Apache Maven\\n- **nuget.org-proxy** This proxy repository accesses the [NuGet Gallery](https://www.nuget.org/). It is the default\\n  component repository used by the `nuget` package management tool used for .Net development.\\n\\n#### Hosted Repository\\n\\nA repository with the type hosted, also known as a **hosted repository**, is a repository that stores components in the\\nrepository manager as the authoritative location for these components.\\n\\nBy default, the repository manager ships with the following configured hosted repositories:\\n\\n- **maven-releases** This hosted repository uses the maven2 repository format with a release version policy. It is\\n  intended to be the repository where an organization publishes internal releases. We can also use this repository for\\n  third-party components that are not available in external repositories and can therefore not be retrieved via a\\n  configured proxy repository. Examples of these components could be commercial, proprietary libraries such as an\\n  Oracle\\n  JDBC driver that may be referenced by the organization.\\n- **maven-snapshots** This hosted repository uses the maven2 repository format with a snapshot version policy. It is\\n  intended to be the repository where the organization publishes internal development versions, also known as\\n  snapshots.\\n- **nuget-hosted**  This hosted repository is where the organization can publish internal releases in repository using\\n  the nuget repository format. We can also use this repository for third-party components that are not available in\\n  external repositories, that could potentially be proxied to gain access to the components.\\n\\n#### Repository Group\\n\\nA repository with the type group, also known as **repository group**, represents a powerful feature of Nexus Repository\\nManager. They allow us to combine multiple repositories and other repository groups in a single repository. This in\\nturn means that our users can rely on a single URL for their configuration needs, while the administrators can add more\\nrepositories and therefore components to the repository group.\\n\\n:::caution\\n\\nWhen a user is given a privilege to a group repository, then that user will also have that privilege to all transitive\\nmembers of that group repository **only when their request is directed to the group repository**. Direct requests to\\nindividual member repositories will only work if the user is given explicit permission to the individual repository.\\n\\n:::\\n\\nThe repository manager ships with the following groups:\\n\\n- **maven-public** The maven-public group is a repository group of maven2 formatted repositories and combines the\\n  important **external proxy repository for the Central Repository** with the hosted repositories **maven-releases**\\n  and maven-snapshots. This allows us to _expose the components of the Central Repository as well as our internal\\n  components in one single, simple-to-use repository_ and therefore URL.\\n- **nuget-group** This group combines the nuget formatted repositories nuget-hosted and nuget.org-proxy into a single\\n  repository for .Net development with NuGet.\\n\\n### Managing Repositories and Repository Groups\\n\\nTBA\\n\\n#### Setting Up Maven Repositories\\n\\nHistorically Nexus Repository Manager started as a repository manager supporting the Maven repository format and it\\ncontinues to include excellent support for users of Apache Maven.\\n\\n### Maven Repository Format Overview\\n\\n> Looking at the Maven repository format and associated concepts and ideas allows us to grasp some of the details and\\n> intricacies involved with different tools and repository formats, that will help us appreciate the need for\\n> repository management.\\n\\nMaven developers are familiar with the concept of a repository, since repositories are used by default. The primary\\ntype of a binary component in a Maven format repository is a JAR file containing Java byte-code. This is due to the\\nJava background of Maven and the fact that the default component type is a JAR. Practically however, there is no limit\\nto what type of component can be stored in a Maven repository. For example, we can easily deploy WAR or EAR files,\\nsource archives, Flash libraries and applications, Android archives or applications or Ruby libraries to a Maven\\nrepository.\\n\\nEvery software component is described by an XML document called a Project Object Model (POM). This POM contains\\ninformation that describes a project and lists a project\u2019s dependencies - the binary software components, which a given\\ncomponent depends upon for successful compilation or execution.\\n\\nWhen Maven downloads a component like a dependency or a plugin from a repository, it also downloads that component\'s\\nPOM. _Given a component\'s POM, Maven can then download any other components that are required by that component_.\\n\\nMaven, which interacts with a Maven repository to search for binary software components, model the projects they manage\\nand retrieve software components on-demand from a repository.\\n\\n#### The Central Repository\\n\\nWhen we download and install Maven without any customization, it retrieves components from the Central Repository. It\\nserves millions of Maven users every single day. It is the default, built-in repository using the Maven repository\\nformat and is _managed by Sonatype_. We can also view statistics about the size of the Central Repository\\n[online](https://search.maven.org/stats).\\n\\nThe Central Repository is the largest repository for Java-based components. It can be easily used from other build\\ntools as well. One can look at the Central Repository as an example of how Maven repositories operate and how they are\\nassembled. Here are some of the properties of release repositories such as the Central Repository:\\n\\n- **Component Metadata** All software components added to the Central Repository require proper metadata, including a\\n  Project Object Model (POM) for each component that describes the component itself and any dependencies that software\\n  component might have.\\n- **Release Stability** Once published to the Central Repository, a component and the metadata describing that\\n  component never change. This property of a release repository, like the Central Repository,  guarantees that\\n  projects that depend on releases will be repeatable and stable over time. While new software components are being\\n  published every day, once a component is assigned a release number on the Central Repository, there is a strict\\n  policy against modifying the contents of a software component after a release.\\n- **Component Security** The Central Repository contains cryptographic hashes and PGP signatures that can be used to\\n  verify the authenticity and integrity of software components served and supports connections in a secure manner via\\n  **HTTPS**.\\n- **Performance** The Central Repository is exposed to the users globally via a high performance content delivery\\n  network of servers.\\n\\nIn addition to the Central Repository, there are a number of major organizations, such as Red Hat, Oracle or the Apache\\nSoftware foundation, which maintain separate additional repositories. Best practice to facilitate these available\\nrepositories with Nexus Repository Manager OSS is to proxy and cache the contents on our own network.\\n\\n#### Component Coordinates and the Repository Format\\n\\nComponent coordinates create a unique identifier for a component. Maven coordinates use the following values: groupId,\\nartifactId, version, and packaging. This set of coordinates is often referred to as a **GAV coordinate**, which is\\nshort for Group, Artifact, Version coordinate. The GAV coordinate standard is the foundation for Maven\'s ability to\\nmanage dependencies. Four elements of this coordinate system are described below:\\n\\n- **groupId** A group identifier groups a set of components into a logical group. Groups are often designed to reflect\\n  the **organization** under which a particular software component is being produced. For example, software components\\n  produced by the Maven project at the Apache Software Foundation are available under the groupId \\"org.apache.maven\\"\\n- **artifactId** An artifactId is an identifier for a software component and should be a descriptive name. The\\n  combination of groupId and artifactId must be unique for a specific project.\\n- **version** The version of a project ideally follows the established convention of\\n  [**semantic versioning**](http://semver.org/). For example, if our simple-library component has a major release\\n  version of 1, a minor release version of 2 and point release version of 3, the version would be 1.2.3. Versions can\\n  also have **alphanumeric qualifiers** which are often used to denote release status. An example of such a qualifier\\n  would be a version like \\"1.2.3-BETA\\" where BETA signals a stage of testing meaningful to consumers of a software\\n  component.\\n- **packaging** Maven was initially created to handle JAR files, but a Maven repository is completely agnostic about\\n  the type of component it is managing. Packaging can be anything that describes any binary software format including:\\n  zip, nar, war, ear, sar and aar.\\n\\nTools designed to interact Maven repositories translate component coordinates into a URL which corresponds to a\\nlocation in a Maven repository. If a tool such as Maven is looking for version \\"1.2.0\\" of the \\"commons-lang\\" JAR in\\nthe group \\"org.apache.commons\\", this request is translated into:\\n\\n```bash\\n<repoURL>/org/apache/commons/commons-lang/1.2.0/commons-lang-1.2.0.jar\\n```\\n\\nMaven also downloads the corresponding POM for \\"commons-lang 1.2.0\\" from:\\n\\n```bash\\n<repoURL>/org/apache/commons/commons-lang/1.2.0/commons-lang-1.2.0.pom\\n```\\n\\nThis POM may contain references to other components, which are then retrieved from the same repository using the same\\nURL patterns.\\n\\n#### Release and Snapshot Repositories\\n\\nA Maven repository stores two types of components:\\n\\n1. **Release repositories** are for stable, static release components. A release component is a component which was\\n   created by a specific, versioned release. For example, consider the \\"1.2.0\\" release of the \\"commons-lang\\" library\\n   stored in the Central Repository. This release component, \\"commons-lang-1.2.0.jar\\", and the associated POM,\\n   \\"commons-lang-1.2.0.pom\\", are static objects which will never change in the Central Repository. Released components\\n   are considered to be solid, stable and perpetual in order to guarantee that builds which depend upon them are\\n   repeatable over time. The released JAR component is associated with a PGP signature, an MD5, and a SHA check-sum\\n   which can be used to verify both the authenticity and integrity of the binary software component.\\n2. **Snapshot repositories** are frequently updated repositories that store binary software components from projects\\n   under constant development. Snapshot components are components generated during the development of a software\\n   project. A Snapshot component has both a version number such as \\"1.3.0\\" or \\"1.3\\" and a timestamp in its name. For\\n   example, a snapshot component for \\"commons-lang 1.3.0\\" might have the name\\n   \\"commons-lang-1.3.0.-20090314.182342-1.jar\\". The associated POM, MD5 and SHA hashes would also have a similar name.\\n   To facilitate collaboration during the development of software components, Maven and other clients that know how to\\n   consume snapshot components from a repository also know how to interrogate the metadata associated with a Snapshot\\n   component to retrieve the latest version of a Snapshot dependency from a repository.\\n\\n**While it is possible to create a repository which serves both release and snapshot components, repositories are\\nusually segmented into release or snapshot repositories serving different consumers and maintaining different standards\\nand procedures for deploying components**. Much like the difference between networks, a release repository is considered\\nlike a production network and a snapshot repository is more like a development or a testing network. While there is a\\nhigher level of procedure and ceremony associated with deploying to a release repository, snapshot components can be\\ndeployed and changed frequently without regard for stability and repeatability concerns.\\n\\n:::info\\n\\nA project under active development produces snapshot components that change over time. A release is comprised of\\ncomponents which will remain unchanged over time.\\n\\n:::\\n\\n### Version policy\\n\\nEvery repository has one of the 3 **version policies** configured:\\n\\n- **Release** A Maven repository can be configured to be suitable for release components with the Release version\\n  policy. The Central Repository uses a release version policy.\\n- **Snapshot** Continuous development is typically performed with snapshot versions supported by the Snapshot version\\n  policy. These version values have to **end with -SNAPSHOT in the POM file**. This allows repeated uploads where the\\n  actual number used is composed of a date/timestamp and an enumerator and the retrieval can still use the -SNAPSHOT\\n  version string. The repository manager and client tools manage the metadata files that manage this translation from\\n  the snapshot version to the timestamp value.\\n- **Mixed** The Mixed version policy allows us to support both approaches within one repository.\\n\\n### Hosting Maven Repositories\\n\\nA hosted Maven repository can be used to deploy our own as well as third-party components. A default installation of\\nNexus Repository Manager includes two hosted Maven repositories. The **maven-releases** repository uses a release\\nversion policy and the **maven-snapshots** repository uses a snapshot version policy.\\n\\n#### Deploying to Hosted Maven repository\\n\\nDeployment to a repository is configured in 2 steps\\n\\n1. [Have Maven project point to the repository](#configuring-repository-in-maven-project)\\n2. [Authenticate push access to the repository](#obtaining-push-access-to-repository)\\n\\n##### Configuring Repository in Maven Project\\n\\nDeployment to a repository is configured in the  `pom.xml` for the respective project in the \\"distributionManagement\\"\\nsection:\\n\\n```xml\\n<project>\\n   ...\\n\\n    <distributionManagement>\\n        <repository>\\n            <id>nexus</id>\\n            <name>Releases</name>\\n            <url>https://nexus-host/repository/maven-releases</url>\\n        </repository>\\n        <snapshotRepository> \x3c!-- This repository can be omitted --\x3e\\n            <id>nexus</id>\\n            <name>Snapshot</name>\\n            <url>https://nexus-host/repository/maven-snapshots</url>\\n        </snapshotRepository>\\n    </distributionManagement>\\n\\n   ...\\n```\\n\\n:::tip\\n\\n- Replace the `nexus-host` with the NDS address pointing to the actual Nexus instance and\\n  `maven-releases`/`maven-snapshots` to the actual repository name. For example,\\n\\n  ```xml\\n  <distributionManagement>\\n      <repository>\\n          <id>nexus</id>\\n          <name>Releases</name>\\n          <url>https://nexus.paion-data.dev/repository/maven-oss</url>\\n      </repository>\\n  </distributionManagement>\\n  ```\\n\\n- The \\"Snapshot\\" repository (tagged `snapshotRepository` above) can be omitted in the most CI/CD-enabled project team\\n\\n:::\\n\\n##### Obtaining Push Access to Repository\\n\\nThe credentials used for the deployment are configured in the \\"server\\" section of the `settings.xml` file. In the\\nexample below server contains \\"nexus\\" as the id, along with a \\"headless user\'s\\" username and password. They are the\\ncredentials needed to authenticate against the repository:\\n\\n```xml\\n<settings>\\n...\\n\\n    <servers>\\n        <server>\\n            <id>nexus</id>\\n            <username>some-headless-username</username>\\n            <password>some-headless-password</password>\\n        </server>\\n    </servers>\\n\\n...\\n```\\n\\nNext, we need to grant the user above (i.e. \\"some-headless-username\\") the push access on the repository side in the\\nfollowing steps:\\n\\n1. Create a [content selector](#content-selectors)\\n2. Create a [privilege](#privileges)\\n3. Create a [role](#roles)\\n4. Assign role to [the headless user](#users) defined above\\n\\nA full build of project, including downloading the declared dependencies and uploading the build output to the\\nrepository manager, can now be invoked with `mvn clean deploy`.\\n\\n##### Deploying to Hosted Maven Repository via CI/CD (e.g. GitHub Actions)\\n\\nNote that we deployed in the last minute our artifact to repository through a manual `mvn clean deploy` command. To\\nenable automatic deployment to the repository, we will simply go through\\n[the exact same procedure](#deploying-to-hosted-maven-repository-manually) except that instead of running\\n`mvn clean deploy` locally and manually, we let CI/CD execute that for us.\\n\\n###### Up-version Automatically\\n\\nIt should be noted that a standard release repository does not allow an artifacts with a defined version to be pushed\\ninto it twice. For example, if our artifacts is my-artifact-1.1.2.jar, this JAR, with version 1.1.2, will fail to be\\npublished onto the repository the 2nd time unless we delete the version 1.1.2 up there.\\n\\nTherefore, we need to bump the artifact version each time we release. We achieve this using GitHub\'s tagging system. The\\ntwo scripts below, taken from [Yahoo/fili\'s CI/CD](https://github.com/yahoo/fili/tree/master/screwdriver), creates a new\\ntag based on previous release tag:\\n\\n1. **tag-for-release.bash**\\n\\n   ```bash\\n   #!/bin/bash\\n\\n   # Pick up the tags from the adjusted remote\\n   git fetch --unshallow\\n   git fetch --tags\\n\\n   echo $(git branch -v)\\n\\n   # Get the last tag on this branch\\n   LAST_TAG=$(git describe)\\n   echo \\"INFO Last tag: $LAST_TAG\\"\\n\\n   # Build the new tag to push\\n   NEW_TAG=$(LAST_TAG=${LAST_TAG} python .github/upversion.py)\\n   echo \\"INFO Creating tag: $NEW_TAG\\"\\n   git tag $NEW_TAG -a -m \\"Autogenerated version bump tag\\"\\n\\n   # Push the new tag\\n   echo \\"INFO Pushing tag: $NEW_TAG\\"\\n   git push origin $NEW_TAG\\n   ```\\n\\n2. **upversion.py**\\n\\n   ```python\\n   #!/usr/bin/python\\n\\n   import os\\n\\n   split_tag = (os.environ[\'LAST_TAG\'].split(\'-\')[0]).split(\\".\\")\\n   split_tag[-1] = str(int(split_tag[-1]) + 1)\\n   print(\\".\\".join(split_tag))\\n   ```\\n\\n_We put these two files under **.github** directory_.\\n\\n###### Deploying to [GitHub Packages](https://github.com/features/packages)\\n\\nCreate a **release.yml** file under _.github/workflows_ directory with the following content:\\n\\n```yaml\\n---\\nname: Release\\n\\non:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  test:\\n    uses: ./.github/workflows/test.yml\\n    secrets: inherit\\n\\n  publish:\\n    needs: [test]\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: write # allow for pushing tag\\n      packages: write\\n    steps:\\n      - uses: actions/checkout@v3\\n      - uses: actions/setup-java@v3\\n        with:\\n          java-version: \'11\'\\n          distribution: \'adopt\'\\n      - name: Tag for release\\n        run: |\\n          git config --global user.name \'QubitPi\'\\n          git config --global user.email \'jack20191124@proton.me\'\\n          .github/tag-for-release.bash\\n      - name: Set release version\\n        run: |\\n          VERSION=$(git describe)\\n          mvn versions:set -DnewVersion=$VERSION -DgenerateBackupPoms=false\\n          mvn versions:update-property -Dproperty=version.owner -DnewVersion=$VERSION -DgenerateBackupPoms=false\\n      - name: Publish Package to Paion Nexus Repository\\n        run: mvn --batch-mode deploy\\n        env:\\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\\n      - name: Deploy documentation to GitHub Pages\\n        uses: peaceiris/actions-gh-pages@v3\\n        with:\\n          github_token: ${{ secrets.GITHUB_TOKEN }}\\n          publish_dir: ./docs\\n          enable_jekyll: true\\n          user_name: QubitPi\\n          user_email: jack20191124@proton.me\\n```\\n\\n:::info\\n\\nThe `test` job defined above is a [workflow call][reusing GitHub workflow] that delegates all testing to a separate\\nGitHub Action. An example of such workflow could be:\\n\\n```yaml\\n---\\nname: Test\\n\\non:\\n  workflow_call:\\n  pull_request:\\n    types: [opened, synchronize, reopened]\\n\\njobs:\\n  test:\\n    name: Test\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v2\\n      - name: Set up JDK 11 for Build\\n        uses: actions/setup-java@v1\\n        with:\\n          java-version: 11\\n      - name: Build\\n        run: mvn -B clean verify\\n```\\n\\n:::\\n\\n:::info\\n\\nThe \\"Set release version\\" step above was inspired by\\n[yahoo/fili\'s approach](https://github.com/yahoo/fili/blob/master/screwdriver/scripts/publish.sh)\\n\\n:::\\n\\n###### Deploying to Nexus 3\\n\\nOr we can deploy to any hosted repository such as Nexus 3 with a slightly modified release definition:\\n\\n:::tip\\n\\nNote that this file differs from the [GitHub version](#deploying-to-github-packageshttpsgithubcomfeaturespackages) by\\nreplacing the `GITHUB_TOKEN` with the dedicated credentials put in a dynamically generated settints.xml file using\\nthe [whelk-io/maven-settings-xml-action](https://github.com/whelk-io/maven-settings-xml-action) action. These\\ncredentials are sealed securely in [GitHub Secrets][GitHub Secrets]. We need to define the following 3 secrets:\\n\\n1. **MAVEN_DEPLOY_SERVER_ID**\\n2. **MAVEN_DEPLOY_SERVER_USERNAME**\\n3. **MAVEN_DEPLOY_SERVER_PASSWD**\\n\\n:::\\n\\n```yaml\\n---\\nname: Release\\n\\non:\\n  push:\\n    branches:\\n      - master\\n\\njobs:\\n  test:\\n    uses: ./.github/workflows/test.yml\\n    secrets: inherit\\n\\n  publish:\\n    needs: [test]\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: write # allow for pushing tag\\n      packages: write\\n    steps:\\n      - uses: actions/checkout@v3\\n      - uses: actions/setup-java@v3\\n        with:\\n          java-version: \'11\'\\n          distribution: \'adopt\'\\n      - name: Tag for release\\n        run: |\\n          git config --global user.name \'QubitPi\'\\n          git config --global user.email \'jack20191124@proton.me\'\\n          .github/tag-for-release.bash\\n      - name: Set release version\\n        run: |\\n          VERSION=$(git describe)\\n          mvn versions:set -DnewVersion=$VERSION -DgenerateBackupPoms=false\\n          mvn versions:update-property -Dproperty=version.owner -DnewVersion=$VERSION -DgenerateBackupPoms=false\\n      - name: Generate settings.xml in order to grant push access to Paion Nexus Repository\\n        uses: whelk-io/maven-settings-xml-action@v20\\n        with:\\n          servers: >\\n            [\\n              {\\n                \\"id\\": \\"${{ secrets.MAVEN_DEPLOY_SERVER_ID }}\\",\\n                \\"username\\": \\"${{ secrets.MAVEN_DEPLOY_SERVER_USERNAME }}\\",\\n                \\"password\\": \\"${{ secrets.MAVEN_DEPLOY_SERVER_PASSWD }}\\"\\n              }\\n            ]\\n      - name: Publish Package to Paion Nexus Repository\\n        run: mvn --batch-mode deploy\\n      - name: Deploy documentation to GitHub Pages\\n        uses: peaceiris/actions-gh-pages@v3\\n        with:\\n          github_token: ${{ secrets.GITHUB_TOKEN }}\\n          publish_dir: ./docs\\n          enable_jekyll: true\\n          user_name: QubitPi\\n          user_email: jack20191124@proton.me\\n```\\n\\n:::tip\\n\\nWe can later pull our released artifact from the hosted repository by placing the **repository** section. For example:\\n\\n```xml\\n<project>\\n\\n    ...\\n\\n    <repositories>\\n        <repository>\\n            <id>paion</id>\\n            <name>Paion Data Official Release Repository</name>\\n            <url>https://nexus.paion-data.dev/repository/maven-oss</url>\\n        </repository>\\n    </repositories>\\n</project>\\n```\\n\\n:::\\n\\n#### Using Repository Manager with Apache Maven\\n\\nTo use repository manager with [Apache Maven](http://maven.apache.org/), configure Maven to check the repository manager\\ninstead of the default, built-in connection to the Central Repository.\\n\\nTo do this, we add a mirror configuration and override the default configuration for the central repository in our\\n\\"~/.m2/settings.xml\\", shown below:\\n\\n:::tip\\n\\nReplace the \\"nexus-host\\" with the NDS address pointing to the actual Nexus instance.\\n\\n:::\\n\\n```xml\\n<settings>\\n  <mirrors>\\n    <mirror>\\n      \x3c!--This sends everything else to /public --\x3e\\n      <id>nexus</id>\\n      <mirrorOf>*</mirrorOf>\\n      <url>https://nexus-host/repository/maven-public/</url>\\n    </mirror>\\n  </mirrors>\\n\\n  <profiles>\\n    <profile>\\n      <id>nexus</id>\\n      \x3c!--Enable snapshots for the built in central repo to direct --\x3e\\n      \x3c!--all requests to nexus via the mirror --\x3e\\n      <repositories>\\n        <repository>\\n          <id>central</id>\\n          <url>http://central</url>\\n          <releases><enabled>true</enabled></releases>\\n          <snapshots><enabled>true</enabled></snapshots>\\n        </repository>\\n      </repositories>\\n     <pluginRepositories>\\n        <pluginRepository>\\n          <id>central</id>\\n          <url>http://central</url>\\n          <releases><enabled>true</enabled></releases>\\n          <snapshots><enabled>true</enabled></snapshots>\\n        </pluginRepository>\\n      </pluginRepositories>\\n    </profile>\\n  </profiles>\\n\\n  <activeProfiles>\\n    \x3c!--make the profile active all the time --\x3e\\n    <activeProfile>nexus</activeProfile>\\n  </activeProfiles>\\n</settings>\\n```\\n\\nIn the configuration above, a single profile called **nexus** is defined. It configures a repository and a\\npluginRepository with the id **central** that overrides the same repositories in the Super POM. The _Super POM_ is\\ninternal to every Apache Maven install and establishes default values. These overrides are important since they change\\nthe repositories by enabling snapshots and replacing the URL with a bogus URL. This URL is overridden by the **mirror**\\nsetting in the same settings.xml file to point to the URL of our single repository group. This repository group can,\\ntherefore, contain release as well as snapshot components and Maven will pick them up.\\n\\nIn addition, The **mirrorOf** pattern of `*` causes any repository request to be redirected to this mirror and to our\\nsingle repository group, which in the example is the public group.\\n\\n:::tip\\n\\nIt is possible to use other patterns in the \\"mirrorOf\\" field. A possible valuable setting is to use **external:***.\\nThis matches all repositories except those using \\"localhost\\" or file based repositories. This is used in conjunction\\nwith a repository manager when you want to exclude redirecting repositories that are defined for integration testing.\\nThe integration test runs for Apache Maven itself require this setting.\\n\\nMore documentation about mirror settings can be found in the\\n[mini guide on the Maven web site](http://maven.apache.org/guides/mini/guide-mirror-settings.html).\\n\\n:::\\n\\n:::note\\n\\nFor startup who doesn\'t really need to proxy the entire Maven Central at the moment, another approach is to set up\\nmultiple repositories. In each project they are working on, they will still fetch everything from Maven Central except\\nfor their hosted component from Nexus\\n\\nThere are two different ways that we can specify the use of multiple repositories. The first way is to specify in a POM\\nwhich repositories we want to use. That is supported both inside and outside of build profiles:\\n\\n```xml\\n<project>\\n    ...\\n\\n    <repositories>\\n        <repository>\\n            <id>my-repo1</id>\\n            <name>your custom repo</name>\\n            <url>http://jarsm2.dyndns.dk</url>\\n        </repository>\\n        <repository>\\n            <id>my-repo2</id>\\n            <name>your custom repo</name>\\n            <url>http://jarsm2.dyndns.dk</url>\\n        </repository>\\n    </repositories>\\n\\n    ...\\n</project>\\n```\\n\\nThe other way we can specify multiple repositories is by creating a profile in the `${user.home}/.m2/settings.xml` or\\n`${maven.home}/conf/settings.xml` file like the following:\\n\\n```xml\\n<settings>\\n    ...\\n    <profiles>\\n        ...\\n        <profile>\\n            <id>myprofile</id>\\n            <repositories>\\n                <repository>\\n                    <id>my-repo2</id>\\n                    <name>your custom repo</name>\\n                    <url>http://jarsm2.dyndns.dk</url>\\n                </repository>\\n            </repositories>\\n        </profile>\\n        ...\\n    </profiles>\\n\\n     <activeProfiles>\\n         <activeProfile>myprofile</activeProfile>\\n     </activeProfiles>\\n     ...\\n</settings>\\n```\\n\\nIf we specify repositories in profiles we must remember to activate that particular profile. As we can see above we do\\nthis by registering a profile to be active in the `activeProfiles` element.\\n\\nWe could also activate this profile on the command like by executing the following command:\\n\\n```bash\\nmvn -Pmyprofile ...\\n```\\n\\nIn fact the `-P` option will take a CSV list of profiles to activate if we wish to activate multiple profiles\\nsimultaneously.\\n\\nThe settings descriptor documentation can be found on the\\n[Maven Local Settings Model Website](https://maven.apache.org/maven-settings/settings.html).\\n\\n:::\\n\\nRemote repository URLs are queried in the following order for artifacts until one returns a valid result:\\n\\n1. Global settings.xml\\n2. User settings.xml\\n3. Local pom.xml\\n4. Parent POMs, recursively\\n5. Super POM\\n6. effective POMs from dependency path to the artifact.\\n\\nFor each of these locations, the repositories within the profiles are queried first in the order outlined at\\n[Introduction to build profiles](https://maven.apache.org/guides/introduction/introduction-to-profiles.html).\\n\\nBefore downloading from a repository,\\n[mirrors configuration](https://maven.apache.org/guides/mini/guide-mirror-settings.html) is applied.\\n\\nEffective settings and local build POM, with profile taken into account, can easily be reviewed to see their\\nrepositories order with `mvn help:effective-settings` and mvn `help:effective-pom -Dverbose`.\\n\\n### Setting Up Docker Registry\\n\\nDocker containers and their usage have revolutionized the way applications and the underlying operating system are\\npackaged and deployed to development, testing and production systems. The creation of the\\n[Open Container Initiative](https://opencontainers.org/), and the involvement of a large number of stakeholders,\\nguarantees that the ecosystem of tools around the lightweight containers and their usage will continue to flourish.\\n[Docker Hub](https://hub.docker.com/) is the original registry for Docker container images and it is being joined by\\nmore and more other publicly available registries such as the\\n[Google Container Registry](https://cloud.google.com/container-registry/) and others.\\n\\nNexus Repository Manager OSS support Docker registries as the Docker repository format for **hosted** and **proxy**\\nrepositories. We can expose these repositories to the client-side tools directly or as a\\n[repository group](#repository-group), which is a repository that merges and exposes the contents of multiple\\nrepositories in one convenient URL. This allows us to reduce time and bandwidth usage for accessing Docker images in a\\nregistry as well as share our images within our organization in a hosted repository. Users can then launch containers\\nbased on those images, resulting in a completely private Docker registry with all the features available in the\\nrepository manager.\\n\\n#### Docker Port Scalability\\n\\nThe Docker client has strict requirements about how it can retrieve content from a repository (i.e., a registry). These\\nrequirements mainly center around the path at which it expects everything to be hosted.\\n\\nWhile it is possible to tell the Docker client to use a chosen host from which to retrieve (or to which to upload)\\nimages, it is not possible to tell it to use an arbitrary base path where images are stored in a registry.\\n\\nTo further explain, the Docker client is given a registry to contact by specifying only the hostname + port. It\'s also\\ngiven a specific path to an image in that registry. So, for example, it would be given\\n`example:443/some/custom/image` to specify an image. We are not able to specify a registry application path.\\n\\nNexus Repository exposes its Docker registries with a repository path of `/repository/<repo_name>/` and, by default,\\nand application context path of `/`.\\n\\nSo, a full Docker image in the repository \\"docker-hosted\\" might be accessible at full URL\\n\\"example:443/nexus3/repository/docker-hosted/some/custom/image\\", which can be broken down as follows:\\n\\n- **example.com** = host name\\n- **443** = port\\n- /nexus3 = application context path\\n- /repository/docker-hosted = base registry path\\n- **/some/custom/image** = specific image path in the registry\\n\\nThere is no way to give the Docker client the application context path or base registry path. Docker needs the registry\\nexposed at the root of the host + port that it is accessing.\\n\\nThis is important because Nexus Repository uses request paths to separate content between different repositories. There\\nare a few potential ways to overcome this Docker limitation:\\n\\n### Access Control\\n\\nNexus Repository uses **role-based access contro**l (**RBAC**) to give administrators fine-grained control over user\\nrights to the following:\\n\\n- Access to the Nexus Repository web application\\n- Read access to a component path in a repository\\n- Administator access to configuration\\n- Publish or upload files to a repository\\n\\nThe default configuration ships with the administrator role and optional anonymous access to browse and read all\\nrepositories. **We should not use the default anonymous role if we need to create protected repositories**.\\n\\n### Realms\\n\\n:::info What Is a Realm?\\n\\nA realm is a security policy domain defined for a web or application server. The protected resources on a server can be\\npartitioned into a set of protection spaces, **each with its own authentication scheme** and/or authorization database\\ncontaining a collection of users and groups.\\n:::\\n\\nThe realms can be accessed via the **Realms** menu item located under **Security**, in the Administration main menu.\\n\\n![Error loading nexus3-realms.png](nexus3-realms.png)\\n\\nEffectively, the configuration shown above determines what authentication realm is used to grant a user access and the\\norder the realms are used.\\n\\n- **Local Authenticating Realm and Local Authorizing Realm** These are the built-in realms used by default. They allow\\n  the repository manager to manage security setup without additional external systems. Sonatype recommends keeping the\\n  Local realms at the top of the active list.  In the event of system recovery, if we have them lower in the order (or\\n  removed), restoration may be more difficult.\\n\\n### Privileges\\n\\nPrivileges define actions which can be performed against specific functionality. Privileges can only be assigned to\\nroles.\\n\\nTo access Privileges go to **Security** in the Administration menu, where it\'s listed as a sub-section. An extensive\\nlist of privileges is already built in the repository manager and is partially shown in the figure below\\n\\n![Error loading nexus3-privileges-partial-list.png](nexus3-privileges-partial-list.png)\\n\\nThis feature allows us to inspect existing privileges and create custom privileges as required. Users will need\\n_nx-privilege_ or _nx-all_ privileges to access this page.\\n\\n#### Privilege Names\\n\\nNames are unique identifiers. Privileges included by default are prefixed with **nx-** . Privileges that are migrated\\nfrom Nexus Repository 2 will be named as they were in Repository 2. Privileges that we create ourselves can only\\nconsist of letters, digits, underscores(`_`), hyphens(`-`), and dots(`.`). A privilege name cannot start with an\\nunderscore or dot.\\n\\nFor custom privileges, it is encouraged that we use a **simple convention** to namespace our privileges. For example\\nusing a simple acronym representing our organization name. \\"Example Organization Inc.\\" could prefix its privilege names\\nwith **eoi-** for example.\\n\\n#### Privilege Types\\n\\nThe privilege list displays an icon for the privilege Type as the first column in the list:\\n\\n:::info Privilege Permissions\\n\\nPrivilege permissions are represented by a colon separated list of text segments, where each segment can be one of:\\n\\n- a single text value\\n- a comma separated list of text values\\n- an asterisk character * to represent all values in that segment\\n\\nThe internal segment matching algorithm uses\\n[Apache Shiro wildcard permissions](https://shiro.apache.org/permissions.html).\\n\\n:::\\n\\n- application (`nexus:{name}:{actions}`)\\n\\n  * Applicable Actions: create, read, update, delete\\n  * Description: Application type privileges are most commonly the built-in privileges that control access to specific\\n    product feature areas in the Administration **UI**. For example, \\"nexus:blobstores:create,read\\" means allowing for\\n    creating and reading blobstores\\n\\n- repository-admin (`nexus:repository-admin:{format}:{repository}:{actions}`)\\n\\n  * Applicable Actions: browse,read,edit,add,delete\\n  * Description: Repository Admin privileges control administration of **configuration** for specific repositories or\\n    repository formats. For example, \\"nexus:repository-admin:nuget:nuget.org-proxy:browse,read\\" means allowing viewing\\n    of the repository configuration for the nuget format repository named \\"nuget.org-proxy\\". **These privileges do not\\n    control access to repository content.**\\n\\n- repository-content-selector (`nexus:repository-content-selector:{selector}:{format}:{repository}:{actions}`)\\n\\n  * Applicable Actions: browse,read,edit,add,delete\\n  * Description: Repository Content Selector privileges provide fine-grained control over access to content within a\\n    repository by way of a content selector. For example, \\"nexus:repository-content-selector:*:maven2:*:read\\" means\\n    allowing a user for read access to any content matching a content selector defined for the maven2 format.\\n\\n- repository-view (`nexus:repository-view:{format}:{repository}:{actions}`)\\n\\n  * Applicable Actions: browse,read,edit,add,delete\\n  * Description: Repository View privileges control general access to all content contained within specific\\n    repositories or repository formats. For example, \\"nexus:repository-view:maven2:central:browse,read\\" means allow\\n    browsing and viewing content within the maven2 format repository named central. **These privileges do not allow\\n    changing configuration of a repository.**\\n\\n- script (`nexus:script:{script name}:{actions}`)\\n\\n  * Applicable Actions: browse,read,edit,add,delete,run\\n  * Description: Script privileges control access to using the Groovy Script related REST APIs as documented in\\n    [REST and Integration API](https://help.sonatype.com/repomanager3/integrations/rest-and-integration-api) . These\\n    privileges do not control general REST API access. For example, \\"nexus:script:*:read\\" means allowing for read\\n    access to all scripts of any name. \\"nexus:script:my-uploaded-script:run\\" means allowing the calling user for\\n    running (executing) the script named my-uploaded-script\\n\\n- wildcard (`*`)\\n\\n  * Applicable Actions: *\\n  * Description: Wildcard privileges allow one to build a privilege string using a free-form series of segments. All\\n    other privilege types are more specific segment forms of a wildcard privilege. There is only one wildcard privilege\\n    included by default named **nx-all** with permission **nexus:***  that gives access to all functionality.\\n\\n:::info Privilege Actions\\n\\nActions are functions allowing an explicit behavior the privilege can perform with the associated function.\\n\\nThe Actions to choose from are\\n\\n- **add** allows privileges to add repositories or scripts.\\n- **read** allows privileges to view various configuration lists and scripts. Without **read**, any associated action\\n  will permit a privilege to see these lists but not its contents. The **read** action also allows privileges to\\n  utilize tools that can look at content from the command line.\\n- **browse**  allows privileges to view the contents of associated repositories. Unlike **read**, privilege types with\\n  browse can only view and administrate repository _contents_ from _UI_.\\n- **create** allows privileges to create applicable configurations within the repository manager. Since a **read**\\n  permission is required to view a configuration, this action is associated with most existing create privileges.\\n- **delete** allows privileges to delete repository manager configurations, repository contents, and scripts. A\\n  **read** action is generally associated with delete actions, so the actor can view these configurations to remove\\n  them.\\n- **edit** allows privileges to modify associated scripts, repository content, and repository administration.\\n- **update** allows privileges to update repository manager configurations. Most existing privileges with update\\n  include **read** actions. Therefore, if creating custom privileges with update, the actor should consider adding read\\n  to the privilege in order to view repository manager configuration updates.\\n- **`*`**, the wildcard, gives us the ability to group all actions together.\\n\\nWe must assign a single or combination of comma-delimited actions when creating new privileges. The privilege type to\\nwhich we apply any of these Actions will perform the action\'s implied behavior. Consider how each action behaves when\\napplied to a privilege type\\n\\n:::\\n\\n#### Creating a Privilege\\n\\nClick the **Create privilege** button to view a list of privilege types\\n\\n![Error loading nexnus3-privileges-types.png](nexnus3-privileges-types.png)\\n\\nAfter selecting a type, fill in the required fields and save the privilege. The privilege can be found listed among the\\ndefault privileges on the main **Privileges** screen. We can use the **Filter** input box to find a specific\\nprivilege. In the following example, an _Application_ privilege type is created:\\n\\n![Error loading nexus3-privileges-application.png](nexus3-privileges-application.png)\\n\\nThe form provides Name, Description, Domain, and Actions in figure above. The form is completed for a privilege that\\nallows read access to the LDAP administration. If assigned this privilege, a user is able to view LDAP administration\\nconfiguration but not edit it, create a new LDAP configuration, nor delete any existing LDAP configurations.\\n\\nIn another example, a _Repository View_ privilege type is created:\\n\\n![Error loading nexus3-privileges-repository-view.png](nexus3-privileges-repository-view.png)\\n\\nThe form provides Name, Description, Format, Repository, and Actions in figure above. The form is completed for a\\nprivilege granting sufficient access to publish images to a specific hosted repository. A user with this privilege can\\nview and read the contents of the repository as well as publish new images to it, but not delete images.\\n\\n### Content Selectors\\n\\nContent selectors provide a means for us to select specific content from all of our content. The content we select is\\nevaluated against expressions written in **CSEL** (**Content Selector Expression Language**). CSEL is a light version\\nof **JEXL** used to script queries along specific paths and coordinates available to our repository manager formats.\\n\\nContent selectors allow us to define what content users are allowed to access. We can define, in a simplified example,\\na selector named \\"Apache Maven\\" with a search expression of `path =~ \\"^/org/apache/maven/\\"`. This would match all\\ncomponents that start with the designated component path. Another, yet more complete, example would be to \\"select all\\nmaven2 content along a path that starts with `org.apache.commons`\\":\\n`format == \\"maven2\\" and path =~ \\"^/org/apache/commons/.*\\"`\\n\\n#### Creating a Query\\n\\nBefore we identify user permissions for our selector, create the query first. Click **Content Selectors** located in\\n**Repository**, from the **Administration** menu. Click **Create Selector** to open a new form.\\n\\n![Error loading nexus3-content-selector-example.png](nexus3-content-selector-example.png)\\n\\n:::tip\\n\\nWe can preview our selector and what results it will return by clicking the **Preview** results button located\\nsomewhere in the middle section of the page. Select a repository or grouping of repositories from the **Preview\\nRepository** dropdown and click the **Preview** button. Assets that match will be returned in the space below the\\nfilter and can be filtered upon if we wish to check on a specific result.\\n\\n:::\\n\\nOnce we are satisfied with our fields, click **Save** to create the content selector. All saved selector queries we\\ncreate will be listed in the **Content Selectors** screen.\\n\\n#### Finer Access Control with \\"Content Selector privilege\\"\\n\\nAs part of our security setup, we can create user permissions to manage the filters we built in the _Create Selector_\\nform. We can add a new privilege that controls operations of read, edit, delete or * (all) for components matching that\\nselector. The privilege can even span multiple repositories.\\n\\nTo create a new **content selector privilege**, click **Privileges** in the **Security** section of the\\n**Administration** panel. Then click the **Create privilege** button. Locate and click **Repository Content Selector**\\nfrom the list of options in **Select Privilege Type**. We will see a form that displays the following:\\n\\n- Name: Create a name for the content selector privilege.\\n- Description: Add a brief description for the privilege.\\n- Content Selector: Use this dropdown to select from a list of selectors we created.\\n- Repository: Use this dropdown to select from either a range of all repository contents, all repository contents of an\\n  individual format, or repositories created by us.\\n- Actions: Grant read, edit, delete, or * (all) privileges for user access control.\\n\\nTo complete the form, save the new privilege by clicking **Create privilege**. We can use our new privilege to regulate\\nwhat permissible data we want the user to access. We could group all related privileges into a [role](#roles).\\nUltimately, we could assign our roles to a [user](#users).\\n\\n#### Content Selector Query Reference\\n\\nBelow are the allowable attributes for content selectors that define path and format as values supported by Nexus\\nRepository Manager.\\n\\n| **Attribute** | **Allowed Values**                           |\\n|---------------|----------------------------------------------|\\n| format        | The format of the content for which we query |\\n| path          | The path of our repository content           |\\n\\nValid Operators are\\n\\n| Operator   | Definition                                |                          Example                           |\\n|:----------:|:-----------------------------------------:|:----------------------------------------------------------:|\\n| `==`       | Matches text _exactly_                    |                    `format == \\"maven2\\"`                    |\\n| `=~`       | Matches a Java regular expression pattern |            `path =~ \\"^/org/apache/commons/.*\\"`             |\\n| `=^`       | Starts with text.                         |                 `path =^ \\"/com/example/\\"`                  |\\n| `and`      | Match all expressions                     | `format == \\"maven2\\" and path =~ \\"^/org/apache/commons/.*\\"` |\\n| `or`       | Match any expression                      |          `format == \\"maven2\\" or format == \\"npm\\"`           |\\n| `( expr )` | Group multiple expressions.               |                         [^example]                         |\\n\\n[^example]: `format == \\"npm\\" or (format == \\"maven2\\" and path =~ \\"^/org/apache/commons/.*\\")`\\n\\n:::info Version Range Regular Expressions\\n\\nTo avoid encountering database errors, we should escape dashes in version range regular expressions. For example,\\n`path =~ \\"[0-9a-zA-Z\\\\-_]\\"`\\n\\n:::\\n\\n:::caution\\n\\nWhen writing a content selector, remember that the asset\'s path will always begin with a leading slash when the\\nselector is evaluated. This is true even though the leading slash is not displayed when searching or browsing assets.\\n\\n:::\\n\\n### Roles\\n\\nRoles aggregate [privileges](#privileges) into a related context and can, in turn, be grouped to create more complex\\nroles.\\n\\nThe repository manager ships with a predefined _admin_ as well as an _anonymous role_. These can be inspected in the\\n**Roles** feature view accessible via the **Roles** item in the **Security** section of the **Administration** main\\nmenu. A simple example is shown in figure below. The list displays the _Name_ and _Description_ of the role as well as\\nthe **Source**, which displays whether the role is internal (Nexus) or a mapping to an external source like LDAP. In\\norder to access these functions, a user must have _nx-roles_ or _nx-all_ [privileges](#privileges).\\n\\n![Error loading nexus3-roles-list.png](nexus3-roles-list.png)\\n\\nTo create a new role, click on the **Create Role** button, select **Nexus role** and fill out the Role creation feature\\nview:\\n\\n:::info\\n\\nTo use functions of creating, editing and deleting roles, a user without the _nx-all_ privilege also will need\\n_nx-privilege-read_. This is because the roles page lists privileges on it.\\n\\n:::\\n\\n![Error loading nexus3-roles-create.png](nexus3-roles-create.png)\\n\\nWhen creating a new role, we will need to supply a **Role ID** and a **Role Name** and optionally a **Description**.\\nRoles are comprised of other roles and individual privileges. To assign a role or privilege to a role, drag and drop\\nthe desired privileges from the _Available_ list to the _Given_ list under the _Privileges_ header. We can use the\\n**Filter** input to narrow down the list of displayed privileges and the arrow buttons to add or remove privileges.\\n\\nThe same functionality is available under the _Roles_ header to select among the _Available_ roles and add them to the\\nlist of _Contained_ roles.\\n\\nFinally press the **Save** button to get the role created.\\n\\nAn existing role can be inspected and edited by clicking on the row in the list. This role-specific view allows us to\\ndelete the role with the **Delete** button. The built-in roles are managed by the repository manager and _cannot_ be\\nedited or deleted.\\n\\n#### Mapping External Groups to Nexus Roles\\n\\nIn addition to creating an internal role, the **Create Role** button allows us to create an **External Role Mapping**\\nto an external authorization system configured in the repository manager such as LDAP. This is something we would do,\\nif we want to grant every member of an externally managed group (such as an LDAP group) a number of privileges and\\nroles in the repository manager\\n\\nFor example, assume that we have a group in LDAP named \\"scm\\" and we want to make sure that everyone in that group has\\nadministrative privileges.\\n\\nSelect **External Role Mapping** and **LDAP** to see a list of roles managed by that external realm in a dialog. Pick\\nthe desired scm group and confirm by pressing **Create** mapping.\\n\\n:::tip\\n\\nFor faster access or if we cannot see our group name, we can also type in a portion or the whole name of the group and\\nit will limit the dropdown to the selected text.\\n\\n:::\\n\\nOnce the external role has been selected, creates a linked role. We can then assign other roles and privileges to this\\nnew externally mapped role like we would do for any other role.\\n\\nAny user that is part of the scm group in LDAP, receives all the privileges defined in the created role allowing us to\\nadapt our generic role in LDAP to the repository manager-specific use cases we want these users to be allowed to\\nperform.\\n\\n### Users\\n\\nThe repository manager ships with two users by default: _admin_ and _anonymous_. The _admin_ user has all privileges\\nand the _anonymous_ user has read-only privileges. The initial password for the admin user can be found in an\\n\\"admin.password\\" file found in the `$data-dir` directory after starting the server.\\n\\nThe Users feature view displayed in figure below can be accessed via the **Users** item in the **Security** section of\\nthe **Administration** menu. Users must have _nx-users_ or _nx-all_ [privileges](#privileges) to see this page. On page\\nload, the security Source of \\"Local\\" is selected and represents the local NXRM realm. The filtered list shows the\\nusers\' User ID, First Name, Last Name, Email and Status from the security Source selected in the dropdown.\\n\\n![Error loading nexus3-users-list.png](nexus3-users-list.png)\\n\\nClicking on a user in the list or clicking on the **Create local user** button displays the details view to edit or\\ncreate the account shown in figure below. For external users, such as LDAP or Crowd, once we have our external realm\\nsetup we can edit their permissions here as well. Simply select the realm the user is on from the **Source** dropdown.\\nThen type the user ID into the field to the right of that dropdown and search for it. Then click on the result desired\\nto edit, same as a local user.\\n\\n:::info\\n\\nTo use functions of creating, editing and deleting users, a user without the _nx-all_ privilege also will need\\n_nx-roles-read_. This is because the users page lists roles on it.\\n\\n:::\\n\\n![Error loading nexus3-users-create.png](nexus3-users-create.png)\\n\\nThe **ID** can be defined upon initial creation and remains fixed thereafter. In addition we can specify the users\\n**First Name**, **Last Name** and **Email address**.  We also must enter and confirm a **Password**.\\n\\nThe Status allows us to set an account to be _Disabled_ or _Active_. The [_Roles_](#roles) control allows us to add and\\nremove defined roles to the user and therefore control the privileges assigned to the user. A user can be assigned one\\nor more roles that in turn can include references to other roles or to individual privileges.\\n\\nOn edit, we can select **Change Password** item in the drop down. The password can be changed in a dialog, provided the\\nuser is managed by the built-in security realm.\\n\\n### Default Role\\n\\nThe Default Role is a role that is automatically granted to all authenticated users.\\n\\nTo enable appending a default role to all authenticated users, navigate to the **Capabilities** item in the **System**\\nsection of the **Administration** menu; then hit **Create capability** and choose capability type **Default Role** as\\npictured below; we will then be able to select the role that we want applied to users.\\n\\n![Error loading nexus3-default-role.png](nexus3-default-role.png)\\n\\nOnce this is saved, the _Default Role Realm_ will be added to the active list of security realms and start applying the\\nnew role to all authenticated users.\\n\\n:::caution\\n\\nThis default role is appended to authenticated users dynamically, and will **NOT** show up as assigned to any user via\\nthe User administration page.\\n\\n:::\\n\\nTroubleshooting\\n---------------\\n\\n### \\"413 Request Entity Too Large\\"\\n\\nIf deploying, for example, a Maven JAR or Docker image to some Nexus repository results in \\"413 Request Entity Too\\nLarge\\" error, that\'s due to, in the case of Nginx as reverse proxy in front of the Nexus, our server block having a\\ndefault value for **client_max_body_size** of around 1MB in size when unset.\\n\\nTo resolve this, we will need to add the following line to our server block (`/etc/nginx/nginx.conf`):\\n\\n![Error loading nexus-413-solution.png](nexus-413-solution.png)\\n\\nFor more information, such as where \\"client_max_body_size\\" directive should be placed, please refer to\\n[Nginx documentation](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size)\\n\\n### \\"400 Repository does not allow updating assets\\"\\n\\nThe version already exists on a release repository. We will need to either bump component version or let Nexus admin\\nto delete the root folder of that component.\\n\\n### \\"A database error occurred\\"\\n\\nWhen we perform some disk I/O related task on UI and we see the following on the top right corner of the page:\\n\\n![Error loading nexus3-strange-error.png](nexus3-strange-error.png)\\n\\nThe error itself does not indicate any information about the nature of the error. We will then need to know more\\ndetails about the error by going down to the Nexus server log.\\n\\n:::note\\n\\nIn case Nexus is running as a Docker container, we need to jump into the container first by executing\\n\\n```bash\\ndocker exec -it <nexus-container-name> bash\\n```\\n\\nwhere `nexus-container-name` can be seen using `docker ps -a`\\n\\n:::\\n\\nThe log file we will be looking at, in the case of running inside Docker container, is located at\\n`/nexus-data/log/nexus.log`\\n\\n[GitHub Secrets]: https://docs.github.com/en/actions/security-guides/encrypted-secrets\\n\\n[reusing GitHub workflow]: https://docs.github.com/en/actions/using-workflows/reusing-workflows#creating-a-reusable-workflow"},{"id":"continuous-delivery","metadata":{"permalink":"/blog/continuous-delivery","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-08-31-continuous-delivery/index.md","source":"@site/blog/2022-08-31-continuous-delivery/index.md","title":"Continuous Delivery","description":"Continuous delivery is an approach where teams release quality products frequently and predictably from source code","date":"2022-08-31T00:00:00.000Z","formattedDate":"August 31, 2022","tags":[{"label":"Continuous Delivery","permalink":"/blog/tags/continuous-delivery"}],"readingTime":28.855,"hasTruncateMarker":true,"authors":[{"name":"continuousdelivery.com","title":"Origin","url":"https://continuousdelivery.com/","imageURL":"https://continuousdelivery.com/images/cd-book.png","key":"continuousdelivery.com"},{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"},{"name":"Sten Pittet","url":"https://www.atlassian.com/continuous-delivery/software-testing/types-of-software-testing","imageURL":"https://wac-cdn.atlassian.com/dam/jcr:57a8bbb8-4f5c-46fc-9ceb-224cf79af3d8/Screen%20Shot%202017-04-14%20at%2010.43.26%20AM.png?cdnVersion=998","key":"sten-pittet"}],"frontMatter":{"slug":"continuous-delivery","title":"Continuous Delivery","authors":["continuousdelivery.com","jiaqi","sten-pittet"],"tags":["Continuous Delivery"]},"unlisted":false,"prevItem":{"title":"Nexus 3 Repository Manager OSS","permalink":"/blog/nexus"},"nextItem":{"title":"Deploying v.s. Releasing","permalink":"/blog/deploying-vs-releasing"}},"content":"Continuous delivery is an approach where teams release quality products frequently and predictably from source code\\nrepository to production in an automated fashion.\\n\\n\x3c!--truncate--\x3e\\n\\nWhat is Continuous Delivery\\n---------------------------\\n\\nContinuous Delivery is the ability to get changes of all types - including new features, configuration changes, bug\\nfixes and experiments - into production, or into the hands of users, _safely_ and _quickly_ in a _sustainable_ way.\\n\\nThe goal of continuous delivery is to make deployments - whether of a large-scale distributed system, a complex\\nproduction environment, an embedded system, or an app - predictable, routine affairs that can be performed on demand.\\n\\nWe achieve all this by ensuring our code is always in a deployable state, even in the face of teams of thousands of\\ndevelopers making changes on a daily basis. We thus completely eliminate the integration, testing and hardening phases\\nthat traditionally followed \\"dev complete\\", as well as code freezes.\\n\\n### Why Continuous Delivery\\n\\nIt is often assumed that if we want to deploy software more frequently, we must accept lower levels of stability and\\nreliability in our systems. In fact, peer-reviewed research shows that this is not the case. High performance teams\\nconsistently deliver services faster and more reliably than their low performing competition. This is true even in\\nhighly regulated domains such as [financial services](https://www.youtube.com/watch?v=eMS97X5ZTGc) and\\n[government](https://www.youtube.com/watch?v=QwHVlJtqhaI). This capability provides an incredible competitive advantage\\nfor organizations that are willing to invest the effort to pursue it.\\n\\n:::info\\n\\n- Firms with high-performing IT organizations were twice as likely to exceed their profitability, market share and\\n  productivity goals.\\n- High performers achieved higher levels of both throughput and stability.\\n- The use of continuous delivery practices including version control, continuous integration, and test automation\\n  predicts higher IT performance.\\n- Culture is measurable and predicts job satisfaction and organizational performance.\\n- Continuous Delivery measurably reduces both deployment pain and team burnout.\\n\\n:::\\n\\nThe practices at the heart of continuous delivery help us achieve several important benefits:\\n\\n- **Low risk releases**. The primary goal of continuous delivery is to make software deployments painless, low-risk\\n  events that can be performed at any time, on demand. By applying [patterns](#patterns) such as **blue-green\\n  deployments** it is relatively straightforward to achieve zero-downtime deployments that are undetectable to users.\\n\\n  :::info blue-green deployment\\n\\n  ![Error loading blue-green-deployments.png ](blue-green-deployments.png)\\n\\n  One of the challenges with automating deployment is the cut-over itself, taking software from the final stage of\\n  testing to live production. We usually need to do this quickly in order to minimize downtime. The blue-green\\n  deployment approach does this by ensuring we have **two production environments**, as identical as possible. At any\\n  time one of them, let\'s say blue for the example, is live. As we prepare a new release of our software we do our\\n  final stage of testing in the green environment. Once the software is working in the green environment, we switch the\\n  router so that all incoming requests go to the green environment - the blue one is now idle.\\n\\n  Blue-green deployment also gives us a rapid way to rollback - if anything goes wrong we switch the router back to\\n  our blue environment. There\'s still the issue of dealing with missed transactions while the green environment was\\n  live, but depending on our design we may be able to feed transactions to both environments in such a way as to keep\\n  the blue environment as a backup when the green is live. Or we may be able to put the application in read-only mode\\n  before cut-over, run it for a while in read-only mode, and then switch it to read-write mode. That may be enough to\\n  flush out many outstanding issues.\\n\\n  The two environments need to be different but as identical as possible. In some situations they can be different\\n  pieces of hardware, or they can be different virtual machines running on the same (or different) hardware. They can\\n  also be a single operating environment partitioned into separate zones with separate IP addresses for the two slices.\\n\\n  Once we\'ve put our green environment live and we\'re happy with its stability, we then use the blue environment as\\n  our **staging environment** for the final testing step for our next deployment. When we are ready for our next\\n  release, we switch from green to blue in the same way that we did from blue to green earlier. That way both green and\\n  blue environments are regularly cycling between live, previous version (for rollback) and staging the next version.\\n\\n  An advantage of this approach is that it\'s the same basic mechanism as we need to get a hot-standby working. Hence\\n  this allows us to test our disaster-recovery procedure on every release.\\n\\n  The fundamental idea is to have two easily switchable environments to switch between, there are plenty of ways to vary\\n  the details. One project did the switch by bouncing the web server rather than working on the router. Another\\n  variation would be to use the same database, making the blue-green switches for web and domain layers.\\n\\n  Databases can often be a challenge with this technique, particularly when we need to change the schema to support a\\n  new version of the software. The trick is to **separate the deployment of schema changes from application upgrades**.\\n  So first apply a database refactoring to change the schema to support both the new and old version of the application,\\n  deploy that, check everything is working fine so we have a rollback point, then deploy the new version of the\\n  application. (And when the upgrade has bedded down remove the database support for the old version.)\\n  :::\\n\\n- **Faster time to market**. It\'s common for the integration and test/fix phase of the traditional phased software\\n  delivery lifecycle to consume weeks to even months. When teams work together to automate the build and deployment,\\n  environment provisioning, and regression testing process, developers can incorporate integration and regression\\n  testing into their daily work and completely remove these phases. We also avoid the large amount of re-work that\\n  plague the phased approach.\\n- **Higher quality and Better products**. When developers have automated tools that discover regressions within minutes,\\n  teams are freed to **focus their effort on user research and higher level testing activities** such as exploratory\\n  testing, usability testing, and performance and security testing. By building a deployment pipeline, these activities\\n  can be performed continuously throughout the delivery process, ensuring quality is built into products and services\\n  from the beginning. Continuous delivery makes it economic to work in small batches. This means we can get feedback\\n  from users throughout the delivery lifecycle based on working software.\\n- **Lower costs**. Any successful software product or service will evolve significantly over the course of its lifetime.\\n  By investing in build, test, deployment and environment automation, we substantially reduce the cost of making and\\n  delivering incremental changes to software by **eliminating many of the fixed costs** associated with the release\\n  process.\\n- **Happier teams**. Continuous Delivery makes releases less painful and reduces team burnout. Furthermore, when we\\n  release more frequently, software delivery teams can engage more actively with users, learn which ideas work and which\\n  don\'t, and see first-hand then outcomes of the work they have done. By removing low-value painful activities\\n  accociated with software delivery, we can fodus on what we care about most - continuous delighting our users.\\n\\n**Continuous delivery is about continuous, daily improvement - the constant discipline of pursuing higher performance by\\nfollowing the heuristic \\"if it hurts, do it more often, and bring the pain forward.\\"**\\n\\nPrinciples\\n----------\\n\\nThere are five principles at the heart of continuous delivery:\\n\\n1. Build quality in\\n2. Work in small batches\\n3. Computers perform repetitive tasks, people solve problems\\n4. Relentlessly pursue continuous improvement\\n5. Everyone is responsible\\n\\nIt\'s easy to get bogged down in the details of implementing continuous delivery - tools, architecture, practices,\\npolitics - if you find yourself lost, try revisiting these principles and you may find it helps you refocus on what\'s\\nimportant.\\n\\n### Build Quality In\\n\\nW. Edwards Deming, a key figure in the history of the Lean movement, offered\\n[14 key principles](https://deming.org/explore/fourteen-points) for management. Principle three states, \\"Cease\\ndependence on inspection to achieve quality. Eliminate the need for inspection on a mass basis by building quality into\\nthe product in the first place\\".\\n\\nIt\'s much cheaper to fix problems and defects if we find them immediately - ideally before they are ever checked into\\nversion control, by running automated tests locally. Finding defects downstream through inspection (such as manual\\ntesting) is time-consuming, requiring significant triage. Then we must fix the defect, trying to recall what we were\\nthinking when we introduced the problem days or perhaps even weeks ago.\\n\\nCreating and evolving feedback loops to detect problems as early as possible is essential and never-ending work in\\ncontinuous delivery. If we find a problem in our exploratory testing, we must not only fix it, but then ask: How could\\nwe have caught the problem with an automated acceptance test? When an acceptance test fails, we should ask: Could we\\nhave written a unit test to catch this problem?\\n\\n### Work in Small Batches\\n\\nIn traditional phased approaches to software development, handoffs from dev to test or test to IT operations consist of\\nwhole releases: months worth of work by teams consisting of tens or hundreds of people.\\n\\nIn continuous delivery, we take the opposite approach, and try and get every change in version control as far towards\\nrelease as we can, getting comprehensive feedback as rapidly as possible.\\n\\nWorking in small batches has many benefits. It reduces the time it takes to get feedback on our work, makes it easier to\\ntriage and remediate problems, increases efficiency and motivation, and prevents us from succumbing to the sunk cost\\nfallacy.\\n\\nThe reason we work in large batches is because of the large fixed cost of handing off changes. **A key goal of\\ncontinuous delivery is to change the economics of the software delivery process to make it economically viable to work\\nin small batches so we can obtain the many benefits of this approach**.\\n\\n:::info\\n\\nA key goal of continuous delivery is to change the economics of the software delivery process to make it economically\\nviable to work in small batches so we can obtain the many benefits of this approach\\n\\n:::\\n\\n### Relentlessly Pursue Continuous Improvement\\n\\nContinuous improvement, or _kaizen_ in Japanese, is another key idea from the Lean movement.\\n[Taiichi Ohno](http://www.amazon.com/dp/0071808019?tag=contindelive-20), a key figure in the history of the Toyota\\ncompany, once said,\\n\\n> \\"Kaizen opportunitites are infinite. Don\'t think you have made things better than before and be at ease\u2026 This would be\\n> like the student who becomes proud because they bested their master two times out of three in fencing. Once you pick\\n> up the sprouts of kaizen ideas, it is important to have the attitude in our daily work that just underneath one kaizen\\n> idea is yet another one\\".\\n\\nDon\'t treat transformation as a project to be embarked on and then completed so we can return to business as usual. The\\nbest organizations are those where everybody treats improvement work as an essential part of their daily work, and where\\nnobody is satisfied with the status quo.\\n\\n### Everyone is Responsible\\n\\nIn high performing organizations, nothing is \\"somebody else\'s problem.\\" Developers are responsible for the quality and\\nstability of the software they build. Operations teams are responsible for helping developers build quality in. Everyone\\nworks together to achieve the organizational level goals, rather than optimizing for what\u2019s best for their team or\\ndepartment.\\n\\nWhen people make local optimizations that reduce the overall performance of the organization, it\'s often due to systemic\\nproblems such as poor management systems such as annual budgeting cycles, or incentives that reward the wrong behaviors.\\nA classic example is rewarding developers for increasing their velocity or writing more code, and rewarding testers\\nbased on the number of bugs they find.\\n\\nMost people want to do the right thing, but they will adapt their behaviour based on how they are rewarded. Therefore,\\nit is very important to create fast feedback loops from the things that really matter: how customers react to what we\\nbuild for them, and the impact on our organization.\\n\\nFoundations - Prerequisites for Continuous Delivery\\n---------------------------------------------------\\n\\n### Configuration Management\\n\\nAutomation plays a vital role in ensuring we can release software repeatably and reliably. One key goal is to take\\nrepetitive manual processes like build, deployment, regression testing and infrastructure provisioning, and automate\\nthem. In order to achieve this, we need to version control everything required to perform these processes, including\\nsource code, test and deployment scripts, infrastructure and application configuration information, and the many\\nlibraries and packages we depend upon. We also want to make it straightforward to query the current -and historical -\\nstate of our environments.\\n\\nWe have two overriding goals:\\n\\n1. **Reproducibility**: We should be able to provision any environment in a fully automated fashion, and know that any\\n   new environment reproduced from the same configuration is identical.\\n2. **Traceability**: We should be able to pick any environment and be able to determine quickly and precisely the\\n   versions of every dependency used to create that environment. We also want to be able to compare previous versions of\\n   an environment and see what has changed between them.\\n\\nThese capabilities give us several very important benefits:\\n\\n1. **Disaster recovery**: When something goes wrong with one of our environments, for example a hardware failure or a\\n   security breach, we need to be able to reproduce that environment in a deterministic amount of time in order to be\\n   able to restore service.\\n2. **Auditability**: In order to demonstrate the integrity of the delivery process, we need to be able to show the path\\n   backwards from every deployment to the elements it came from, including their version. Comprehensive configuration\\n   management, combined with deployment pipelines, enable this.\\n3. **Higher quality**: The software delivery process is often subject to long delays waiting for development, testing\\n   and production environments to be prepared. When this can be done automatically from version control, we can get\\n   feedback on the impact of our changes much more rapidly, enabling us to build quality in to our software.\\n4. **Capacity management**: When we want to add more capacity to our environments, the ability to create new\\n   reproductions of existing servers is essential. This capability, using [OpenStack](https://www.openstack.org/) for\\n   example, enables the horizontal scaling of modern cloud-based distributed systems.\\n5. **Response to defects**: When we discover a critical defect, or a vulnerability in some component of our system, we\\n   want to get a new version of our software released as quickly as possible. Many organizations have an emergency\\n   process for this type of change which goes faster by bypassing some of the testing and auditing. This presents an\\n   especially serious dilemma in safety-critical systems. Our goal should be to be able to use our normal release\\n   process for emergency fixes - which is precisely what continuous delivery enables, on the basis of comprehensive\\n   configuration management.\\n\\nAs environments become more complex and heterogeneous, it becomes progressively harder to achieve these goals. Achieving\\nperfect reproducibility and traceability to the last byte for a complex enterprise system is impossible (apart from\\nanything else, every real system has state). Thus a key part of configuration management is working to **simplify our\\narchitecture, environments and processes** to reduce the investment required to achieve the desired benefits.\\n\\n#### Configuration Management Learning Resources\\n\\n- [Infrastructure as Code](https://www.oreilly.com/library/view/infrastructure-as-code/9781491924334/)\\n- [Pedro Canahuati on scaling operations at Facebook](http://www.infoq.com/presentations/scaling-operations-facebook)\\n\\n### Continuous Integration\\n\\nCombining the work of multiple developers is hard. Software systems are complex, and an apparently simple,\\nself-contained change to a single file can easily have unintended consequences which compromise the correctness of the\\nsystem. As a result, some teams have developers work isolated from each other on their own branches, both to keep\\ntrunk/master stable, and to prevent them treading on each other\u2019s toes.\\n\\nHowever, over time these branches diverge from each other. While merging a single one of these branches into mainline is\\nnot usually troublesome, the work required to integrate multiple long-lived branches into mainline is usually painful,\\nrequiring significant amounts of re-work as conflicting assumptions of developers are revealed and must be resolved.\\n\\nTeams using long-lived branches often require code freezes, or even integration and stabilization phases, as they work\\nto integrate these branches prior to a release. Despite modern tooling, this process is still expensive and\\nunpredictable. On teams larger than a few developers, the integration of multiple branches requires multiple rounds of\\nregression testing and bug fixing to validate that the system will work as expected following these merges. This problem\\nbecomes exponentially more severe as team sizes grow, and as branches become more long-lived.\\n\\nThe practice of continuous integration was invented to address these problems. CI (continuous integration) follows the\\nXP (extreme programming) principle that if something is painful, we should do it more often, and bring the pain forward.\\nThus in CI developers integrate all their work into trunk (also known as mainline or master) on a regular basis (at\\nleast daily). A set of automated tests is run both **before and after** the merge to validate that no regressions are\\nintroduced. If these automated tests fail, the team stops what they are doing and someone fixes the problem immediately.\\n\\nThus we ensure that the software is always in a working state, and that developer branches do not diverge significantly\\nfrom trunk. The benefits of continuous integration are very significant - higher levels of throughput, more stable\\nsystems, and higher quality software. However the practice is still controversial, for two main reasons.\\n\\nFirst, it requires developers to break up large features and other changes into smaller, more incremental steps that can\\nbe integrated into trunk/master. This is a paradigm shift for developers who are not used to working in this way. It also\\ntakes longer to get large features completed. However in general we don\'t want to optimize for the speed at which\\ndevelopers can declare their work \\"dev complete\\" on a branch. Rather, we want to be able to get changes reviewed,\\nintegrated, tested and deployed as fast as possible - and this process is an order of magnitude faster and cheaper when\\nthe changes are small and self-contained, and the branches they live on are short-lived. Working in small batches also\\nensures developers get regular feedback on the impact of their work on the system as a whole - from other developers,\\ntesters, customers, and automated performance and security tests\u2014which in turn makes any problems easier to detect,\\ntriage, and fix.\\n\\nSecond, continuous integration requires a fast-running set of comprehensive automated unit tests. These tests should be\\ncomprehensive enough to give a good level of confidence that the software will work as expected, while also running in a\\nfew minutes or less. If the automated unit tests take longer to run, developers will not want to run them frequently,\\nand they will become harder to maintain. Creating maintainable suites of automated unit tests is complex and is best done\\nthrough test-driven development (TDD), in which developers write failing automated tests before they implement the code\\nthat makes the tests pass. TDD has several benefits, the most important of which is that it ensures developers write code\\nthat is modular and easy to test, reducing the maintenance cost of the resulting automated test suites. But TDD is still\\nnot sufficiently widely practiced.\\n\\nDespite these barriers, **helping software development teams implement continuous integration should be the number one\\npriority for any organization** wanting to start the journey to continuous delivery. By creating rapid feedback loops\\nand ensuring developers work in small batches, CI enables teams to build quality into their software, thus reducing the\\ncost of ongoing software development, and increasing both the productivity of teams and the quality of the work they\\nproduce.\\n\\n#### Continuous Integration Learning Resources\\n\\n- [Paul Duvall\'s book on Continuous Integration](http://www.amazon.com/dp/0321336380?tag=contindelive-20)\\n\\n### Continuous Testing\\n\\nThe key to building quality into our software is making sure we can get fast feedback on the impact of changes.\\nTraditionally, extensive use was made of manual inspection of code changes and manual testing (testers following\\ndocumentation describing the steps required to test the various functions of the system) in order to demonstrate the\\ncorrectness of the system. This type of testing was normally done in a phase following \u201cdev complete\u201d. However this\\nstrategy have several drawbacks:\\n\\n- Manual regression testing takes a long time and is relatively expensive to perform, creating a bottleneck that\\n  prevents us releasing software more frequently, and getting feedback to developers weeks (and sometimes months) after\\n  they wrote the code being tested.\\n- Manual tests and inspections are not very reliable, since people are notoriously poor at performing repetitive tasks\\n  such as regression testing manually, and it is extremely hard to predict the impact of a set of changes on a complex\\n  software system through inspection.\\n- When systems are evolving over time, as is the case in modern software products and services, we have to spend\\n  considerable effort updating test documentation to keep it up-to-date.\\n\\nIn order to build quality in to software, we need to adopt a\\n[different approach](#different-types-of-software-testing).\\n\\nThe more features and improvements go into our code, the more we\'ll need to test to make sure that all our system works\\nproperly. And then for each bug we fix, it would be wise to check that they don\'t get back in newer releases.\\nAutomation is key to make this possible and writing tests sooner rather than later will become part of our development\\nworkflow.\\n\\nOnce we have continuous integration and test automation in place, we create a\\n[deployment pipeline](#the-deployment-pipeline). In the deployment pipeline pattern, every change runs a build that\\n\\n- creates packages that can be deployed to any environment and\\n- runs unit tests (and possibly other tasks such as static analysis), giving feedback to developers in the space of a\\n  few minutes.\\n\\nPackages that pass this set of tests have more comprehensive automated acceptance tests run against them. Once we have\\npackages that pass all the automated tests, they are available for deplyment to other environments.\\n\\nIn the deployment pipeline, every change is effectively a release candidate. The job of the deployment pipeline is to\\ncatch known issues. If we can\'t detect any known problems, we should feel totally comfortable releasing any packages\\nthat have gone through it. If we aren\'t, or if we discover defects later, it means we need to improve our pipeline,\\nperhaps adding or updating some tests.\\n\\nOur goal should be to find problems as soon as possible, and make the lead time from check-in to release as short as\\npossible. Thus we want to parallelize the activities in the deployment pipeline, not have many stages executing in\\nseries. If we discover a defect in the acceptance tests, we should be looking to improve our unit tests (most of our\\ndefects should be discovered through unit testing).\\n\\n#### Different Types of Software Testing\\n\\n##### Unit Tests\\n\\nUnit tests are very low level and close to the source of an application. They consist in testing individual methods and\\nfunctions of the classes, components, or modules used by our software. Unit tests are generally quite cheap to automate\\nand can run very quickly by a continuous integration server.\\n\\n##### Integration Tests\\n\\nIntegration tests verify that different modules or services used by our application work well together. For example, it\\ncan be testing the interaction with the database or making sure that microservices work together as expected. These\\ntypes of tests are more expensive to run as they require multiple parts of the application to be up and running.\\n\\n##### Functional Tests\\n\\nFunctional tests focus on the business requirements of an application. They only verify the output of an action and do\\nnot check the intermediate states of the system when performing that action.\\n\\nThere is sometimes a confusion between integration tests and functional tests as they both require multiple components\\nto interact with each other. The difference is that an integration test may simply verify that we can query the\\ndatabase while a functional test would expect to get a specific value from the database as defined by the product\\nrequirements.\\n\\n##### End-to-End Tests\\n\\nEnd-to-end testing replicates a user behavior with the software in a complete application environment. It verifies that\\nvarious user flows work as expected and can be as simple as loading a web page or logging in or much more complex\\nscenarios verifying email notifications, online payments, etc...\\n\\nEnd-to-end tests are very useful, but they\'re expensive to perform and can be hard to maintain when they\'re automated.\\nIt is recommended to have a few key end-to-end tests and rely more on lower level types of testing (unit and\\nintegration tests) to be able to quickly identify breaking changes.\\n\\n##### Acceptance Tests\\n\\nAcceptance tests are formal tests that verify if a system satisfies business requirements. They require the entire\\napplication to be running while testing and focus on replicating user behaviors. But they can also go further and\\nmeasure the performance of the system and reject changes if certain goals are not met.\\n\\n##### Performance Tests\\n\\nPerformance tests evaluate how a system performs under a particular workload. These tests help to measure the\\nreliability, speed, scalability, and responsiveness of an application. For instance, a performance test can observe\\nresponse times when executing a high number of requests, or determine how a system behaves with a significant amount of\\ndata. It can determine if an application meets performance requirements, locate bottlenecks, measure stability during\\npeak traffic, and more.\\n\\n##### Smoke Tests\\n\\nSmoke tests are basic tests that check the basic functionality of an application. They are meant to be quick to\\nexecute, and their goal is to give us the assurance that the major features of our system are working as expected.\\n\\nSmoke tests can be useful right after a new build is made to decide whether or not we can run more expensive tests, or\\nright after a deployment to make sure that they application is running properly in the newly deployed environment.\\n\\nImplementing Continuous Delivery\\n--------------------------------\\n\\nOrganizations attempting to deploy continuous delivery tend to make two common mistakes. The first is to treat\\ncontinuous delivery as an end-state, a goal in itself. The second is to spend a lot of time and energy worrying about\\nwhat products to use.\\n\\n### Evolutionary Architecture\\n\\nIn the context of enterprise architecture there are typically multiple attributes we are concerned about, for example\\navailability, security, performance, usability and so forth. In continuous delivery, we introduce two new architectural\\nattributes:\\n\\n1. **testability**\\n2. **deployability**\\n\\nIn a _testable_ architecture, we design our software such that most defects can (in principle, at least) be discovered\\nby developers by running automated tests on their workstations. We shouldn\u2019t need to depend on complex, integrated\\nenvironments in order to do the majority of our acceptance and regression testing.\\n\\nIn a _deployable_ architecture, deployments of a particular product or service can be performed independently and in a\\nfully automated fashion, without the need for significant levels of orchestration. Deployable systems can typically be\\nupgraded or reconfigured with zero or minimal downtime.\\n\\nWhere testability and deployability are not prioritized, we find that much testing requires the use of complex,\\nintegrated environments, and deployments are \\"big bang\\" events that require that many services are released at the same\\ntime due to complex interdependencies. These \\"big bang\\" deployments require many teams to work together in a carefully\\norchestrated fashion with many hand-offs, and dependencies between hundreds or thousands of tasks. Such deployments\\ntypically take many hours or even days, and require scheduling significant downtime.\\n\\nDesigning for testability and deployability starts with ensuring our products and services are composed of\\nloosely-coupled, well-encapsulated components or modules\\n\\nWe can define a well-designed modular architecture as one in which it is possible to test or deploy a single component\\nor service on its own, with any dependencies replaced by a suitable test double, which could be in the form of a virtual\\nmachine, a stub, or a mock. Each component or service should be deployable in a fully automated fashion on developer\\nworkstations, test environments, or in production. In a well-designed architecture, it is possible to get a high level of\\nconfidence the component is operating properly when deployed in this fashion.\\n\\n:::info Test Double\\n\\nTest Double is a generic term for any case where you replace a production object for testing purposes. There are various\\nkinds of double:\\n\\n- **Dummy** objects are passed around but never actually used. Usually they are just used to fill parameter lists.\\n- **Fake** objects actually have working implementations, but usually take some shortcut which makes them not suitable\\n  for production (an InMemoryTestDatabase is a good example).\\n- **Stubs** provide canned answers to calls made during the test, usually not responding at all to anything outside\\n  what\'s programmed in for the test.\\n- **Spies** are stubs that also record some information based on how they were called. One form of this might be an email\\n  service that records how many messages it was sent.\\n- **Mocks** are pre-programmed with expectations which form a specification of the calls they are expected to receive.\\n  They can throw an exception if they receive a call they don\'t expect and are checked during verification to ensure\\n  they got all the calls they were expecting.\\n\\n:::\\n\\nAny true service-oriented architecture should have these properties\u2014but unfortunately many do not. However, the\\nmicroservices movement has explicitly prioritized these architectural properties.\\n\\nOf course, many organizations are living in a world where services are distinctly hard to test and deploy. Rather than\\nre-architecting everything, we recommend an iterative approach to improving the design of enterprise system, sometimes\\nknown as evolutionary architecture. In the evolutionary architecture paradigm, we accept that successful products and\\nservices will require re-architecting during their lifecycle due to the changing requirements placed on them.\\n\\nOne pattern that is particularly valuable in this context is the strangler application. In this pattern, we iteratively\\nreplace a monolithic architecture with a more componentized one by ensuring that new work is done following the\\nprinciples of a service-oriented architecture, while accepting that the new architecture may well delegate to the system\\nit is replacing. Over time, more and more functionality will be performed in the new architecture, and the old system\\nbeing replaced is \\"strangled\\".\\n\\n![](strangler.png)\\n\\n### Patterns\\n\\n#### The Deployment Pipeline\\n\\nThe key pattern introduced in continuous delivery is the **deployment pipeline**. Our goal was to make deployment to any\\nenvironment a fully automated, scripted process that could be performed on demand in minutes. We wanted to be able to\\nconfigure testing and production environments purely from configuration files stored in version control. The apparatus\\nwe used to perform these tasks became known as _deployment pipelines_\\n\\nIn the deployment pipeline pattern, every change in version control triggers a process (usually in a CI server) which\\ncreates deployable packages and runs automated unit tests and other validations such as static code analysis. This first\\nstep is optimized so that it takes only a few minutes to run. If this initial commit stage fails, the problem must be\\nfixed immediately; nobody should check in more work on a broken commit stage. Every passing commit stage triggers the\\nnext step in the pipeline, which might consist of a more comprehensive set of automated tests. Versions of the software\\nthat pass all the automated tests can then be deployed to production.\\n\\nDeployment pipelines tie together [configuration management](#configuration-management),\\n[continuous integration](#continuous-integration) and [test](#continuous-testing) and deployment automation in a\\nholistic, powerful way that works to improve software quality, increase stability, and reduce the time and cost required\\nto make incremental changes to software, whatever domain we\'re operating in. When building a deployment pipeline, the\\nfollowing practices become valuable:\\n\\n- **Only build packages once**. We want to be sure the thing we\'re deploying is the same thing we\'ve tested throughout\\n  the deployment pipeline, so if a deployment fails we can eliminate the packages as the source of the failure.\\n- **Deploy the same way to every environment, including development**. This way, we test the deployment process many,\\n  many times before it gets to production, and again, we can eliminate it as the source of any problems.\\n- **Smoke test your deployments**. Have a script that validates all your application\'s dependencies are available, at\\n  the location you have configured your application. Make sure your application is running and available as part of the\\n  deployment process.\\n- **Keep your environments similar**. Although they may differ in hardware configuration, they should have the same\\n  version of the operating system and middleware packages, and they should be configured in the same way. This has\\n  become much easier to achieve with modern virtualization and container technology.\\n\\nWith the advent of infrastructure as code, it has became possible to use deployment pipelines to create a fully\\nautomated process for taking all kinds of changes\u2014including database and infrastructure changes\\n\\n### Patterns for Low-Risk Releases\\n\\nIn the context of web-based systems there are a number of patterns that can be applied to further reduce the risk of\\ndeployments. Michael Nygard also describes a number of important software design patterns which are instrumental in\\ncreating resilient large-scale systems in his book\\n[Release It!](http://www.amazon.com/dp/0978739213?tag=contindelive-20)\\n\\nThe 3 key principles that enable low-risk releases are\\n\\n1. **Optimize for Resilience**. Once we accept that failures are inevitable, we should start to move away from the idea\\n   of investing all our effort in preventing problems, and think instead about how to restore service as rapidly as\\n   possible when something goes wrong. Furthermore, when an accident occurs, we should treat it as a learning\\n   opportunity. Resilience isn\'t just a feature of our systems, it\'s a characteristic of a team\'s culture. High\\n   performance organizations are constantly working to improve the resilience of their systems by trying to break them\\n   and implementing the lessons learned in the course of doing so.\\n2. **Low-risk Releases are Incremental**. Our goal is to architect our systems such that we can release individual\\n   changes (including database changes) independently, rather than having to orchestrate big-bang releases due to tight\\n   coupling between multiple different systems.\\n3. **Focus on Reducing Batch Size**. Counterintuitively, deploying to production more frequently actually reduces the\\n   risk of release when done properly, simply because the amount of change in each deployment is smaller. When each\\n   deployment consists of tens of lines of code or a few configuration settings, it becomes much easier to perform root\\n   cause analysis and restore service in the case of an incident. Furthermore, because we practice the deployment\\n   process so frequently, we\u2019re forced to simplify and automate it which further reduces risk."},{"id":"deploying-vs-releasing","metadata":{"permalink":"/blog/deploying-vs-releasing","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-08-29-deply-vs-release/index.md","source":"@site/blog/2022-08-29-deply-vs-release/index.md","title":"Deploying v.s. Releasing","description":"The key distinction between deployment and release is the business rationale. Deployment doesn\'t necessarily mean users","date":"2022-08-29T00:00:00.000Z","formattedDate":"August 29, 2022","tags":[{"label":"Continuous Delivery","permalink":"/blog/tags/continuous-delivery"}],"readingTime":1.315,"hasTruncateMarker":true,"authors":[{"name":"Joseph Mathenge","url":"https://www.bmc.com/blogs/software-deployment-vs-release/","imageURL":"https://yt3.ggpht.com/ytc/AL5GRJX2ITW5UDZgTqodcXst-XYmK3-6UZfrAl0fq4tfYw=s68-c-k-c0x00ffffff-no-rj","key":"joseph-mathenge"},{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"deploying-vs-releasing","title":"Deploying v.s. Releasing","authors":["joseph-mathenge","jiaqi"],"tags":["Continuous Delivery"]},"unlisted":false,"prevItem":{"title":"Continuous Delivery","permalink":"/blog/continuous-delivery"},"nextItem":{"title":"Docker cAdvisor","permalink":"/blog/docker-c-advisor"}},"content":"The key distinction between deployment and release is the business rationale. Deployment doesn\'t necessarily mean users\\nhave access to features. Some companies will release at the same time as deployment to production is taking place.\\n\\nOthers will choose to wait, thereby having the new features in production but not availed to users until the business\\ndecides.\\n\\n\x3c!--truncate--\x3e\\n\\nWhat is Deployment\\n------------------\\n\\nDeployment involves moving software from one controlled environment to another. **An environment is a subset of IT\\ninfrastructure used for a particular purpose**. The most common environments are:\\n\\n- **Development**. Commonly referred to as _dev_, this is where developers build the code.\\n- **Integration**. Here, the new code is combined and validated that it works with existing code.\\n- **Test**. This is where both functional and non-functional tests are conducted on the merged code to confirm it meets\\n  organization and customer requirements.\\n- **Staging**. This environment is used to test the software using real data to validate it is ready for use.\\n- **Production**. Commonly referred to as prod, this is where the software is made available to users.\\n\\nWhat is Software Release\\n------------------------\\n\\nA release is **a collection of one or more new or changed services or service components deployed into the live\\nenvironment as a result of one or more changes**\\n\\nIn other words, a release makes services and features available to users. More often than not,\\n[release management](https://www.bmc.com/blogs/devops-release-management/) is more of a business responsibility than a\\ntechnical responsibility. This is because the decisions on scheduling releases can be tied to business strategy from a\\nrevenue or portfolio management perspective."},{"id":"docker-c-advisor","metadata":{"permalink":"/blog/docker-c-advisor","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-07-31-docker-c-advisor/index.md","source":"@site/blog/2022-07-31-docker-c-advisor/index.md","title":"Docker cAdvisor","description":"[cAdvisor] (Container Advisor) provides Docker container users an understanding of","date":"2022-07-31T00:00:00.000Z","formattedDate":"July 31, 2022","tags":[{"label":"Docker","permalink":"/blog/tags/docker"},{"label":"Virtualization","permalink":"/blog/tags/virtualization"}],"readingTime":0.775,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"docker-c-advisor","title":"Docker cAdvisor","authors":["jiaqi"],"tags":["Docker","Virtualization"]},"unlisted":false,"prevItem":{"title":"Deploying v.s. Releasing","permalink":"/blog/deploying-vs-releasing"},"nextItem":{"title":"Yahoo Cloud Object Store - Object Storage at Exabyte Scale","permalink":"/blog/yahoo-object-storage"}},"content":"[cAdvisor] (Container Advisor) provides Docker container users an understanding of\\nthe resource usage and performance characteristics of their running containers. It is a running daemon that collects,\\naggregates, processes, and exports information about running containers. Specifically, for each container it keeps\\nresource isolation parameters, historical resource usage, histograms of complete historical resource usage and network\\nstatistics. This data is exported by container and machine-wide.\\n\\n\x3c!--truncate--\x3e\\n\\nAlthough [cAdvisor] has some prelimilary (useful though) UI. It also offers\\n\\n1. [RESTful API to query container stats](https://github.com/google/cadvisor/blob/master/docs/api.md)\\n2. [Export capability to common data storage, such as Elasticsearch](https://github.com/google/cadvisor/blob/master/docs/storage/README.md)\\n\\nTo pull the image and run it:\\n\\n```bash\\nsudo docker run \\\\\\n    --volume=/:/rootfs:ro \\\\\\n    --volume=/var/run/docker.sock:/var/run/docker.sock:rw \\\\\\n    --volume=/sys:/sys:ro \\\\\\n    --volume=/var/lib/docker/:/var/lib/docker:ro \\\\\\n    --volume=/dev/disk/:/dev/disk:ro \\\\\\n    --publish=8080:8080 \\\\\\n    --detach=true \\\\\\n    --name=cadvisor \\\\\\n    --privileged \\\\\\n    --device=/dev/kmsg \\\\\\n    gcr.io/cadvisor/cadvisor:v0.36.0\\n```\\n\\n![cAdvisor Screenshot 1](cadvisor-1.png)\\n![cAdvisor Screenshot 2](cadvisor-2.png)\\n\\n### [docker-container-stats](https://github.com/virtualzone/docker-container-stats)\\n\\n[cAdvisor](https://github.com/google/cadvisor) is good for customizing container monitoring, but it\'s heavy. A\\nquick-and-lightweight option would be [docker-container-stats](https://github.com/virtualzone/docker-container-stats)\\n\\n![docker-container-stats Screenshot](docker-container-stats.png)\\n\\n[cAdvisor]: https://github.com/google/cadvisor"},{"id":"yahoo-object-storage","metadata":{"permalink":"/blog/yahoo-object-storage","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-05-12-yahoo-object-storage/index.md","source":"@site/blog/2022-05-12-yahoo-object-storage/index.md","title":"Yahoo Cloud Object Store - Object Storage at Exabyte Scale","description":"Yahoo stores more than 250 Billion objects and half an exabyte of perpetually durable user content such as photos,","date":"2022-05-12T00:00:00.000Z","formattedDate":"May 12, 2022","tags":[{"label":"Object Storage","permalink":"/blog/tags/object-storage"}],"readingTime":6.31,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"yahoo-object-storage","title":"Yahoo Cloud Object Store - Object Storage at Exabyte Scale","authors":["jiaqi"],"tags":["Object Storage"]},"unlisted":false,"prevItem":{"title":"Docker cAdvisor","permalink":"/blog/docker-c-advisor"},"nextItem":{"title":"Add Custom ASCII Banner Logo to SSH Login Screen","permalink":"/blog/ssh-ascii-banner"}},"content":"Yahoo stores more than 250 Billion objects and half an exabyte of perpetually durable user content such as photos,\\nvideos, email, and blog posts. Object storage at Yahoo is growing at 20-25% annually. The growth is primarily driven by\\nmobile, images, video, and user growth. Yahoo is betting on software defined storage to scale storage cost effectively\\nalong with the durability and latency guarantees.\\n\\n\x3c!--truncate--\x3e\\n\\nObject Storage Landscape at Yahoo\\n---------------------------------\\n\\nWhat is \\"object storage\\"? Images and photos in Flickr, Videos, and documents, spreadsheets, and presentations exchanged\\nas Mail attachments are classic examples of \\"objects\\". The typical quality of this class of data is\\n\\"**write-once-read-many**\\". Traditionally, Yahoo has used storage appliances for object storage. As Yahoo is\\nincreasingly becoming the guide for digital information to our users, object storage need in Yahoo is growing rapidly.\\nAdditionally, application characteristics differ in access patterns, durability and latency needs, and cost targets. To\\nsupport growth cost effectively and meet the varying application needs, object storage in Yahoo requires different\\ntradeoffs. **We need the flexibility offered by software defined storage to deliver these tradeoffs**.\\n\\nWhy Software Defined Storage\\n----------------------------\\n\\nKey benefits of software defined storage are:\\n\\n- **Cost-performance tradeoff**: Allows applications to choose performance and cost tradeoffs with different hardware\\n  and durability configurations using the same software stack.\\n- **Flexible interfaces**: Ability to choose industry standard API, embed client libraries in applications, or even use\\n  proprietary API where required. Industry standard APIs allow seamless migration of applications from public to Yahoo\\n  private cloud.\\n- **Different storage abstractions**: Leverage the same storage software stack across Object, Block, and File\\n  abstractions, thus reducing R&D and operational costs.\\n\\nCloud Object Store (COS) is Yahoo\'s commodity hardware based software defined storage solution. In partnership with\\nFlickr Yahoo has completed a multi-petabyte initial deployment of COS. Yahoo plans COS as a multi-tenant hosted service\\nand to grow COS by ten-fold to support Flickr, Yahoo Mail and Tumblr. That is 100s of petabytes of storage to be\\nsupported on COS.\\n\\nUnder the Hood\\n--------------\\n\\nCOS is deployed using **Ceph** storage technology. Yahoo evaluated open-source solutions such as Swift and Ceph, as well\\nas commercial solutions and chose Ceph because it enables consolidation of storage tiers for Object, Block, and File\\nwith inherent architectural support. Also, being an open-source product, Ceph provides the flexibility needed to\\ncustomize for Yahoo needs.\\n\\nCOS deployment consists of modular Ceph clusters with _each Ceph cluster treated as a pod_. Multiple such Ceph clusters\\ndeployed simultaneously form a COS **supercluster** as shown in figure below. Objects are uniformly distributed across\\nall the clusters in a supercluster. A _proprietary hashing mechanism_ is used to distribute objects. The hashing\\nalgorithm is implemented in a client library embedded in the applications.\\n\\n![Error loading cepu-cluster.png](cepu-cluster.png)\\n\\nSince each cluster consists of tens of commodity servers and hundreds of disks, it is highly likely that components will\\nfail frequently. High disk and network activity occurs during recovery due to rebalancing of objects, which in turn\\nincreases object read latency during this phase. Capping the size of each cluster allows Yahoo to limit the resource\\nusage during recovery phases in order to adhere to latency SLAs.\\n\\nYahoo users expect their images, videos and mail attachments to be perpetually stored, and made available\\ninstantaneously from anywhere around the world. This requires high data \u201cdurability\u201d guarantees. Durability is typically\\nachieved in storage systems either via redundancy or encoding. Redundancy can be provided through extra copies of data\\nor replicas. On the other hand, encoding can be provided via traditional mechanisms like simple parity, or more\\nsophisticated mechanisms like erasure coding. Erasure coding breaks down an object into fragments and stores them across\\nmultiple disks with a few redundant pieces to tolerate multiple failures.\\n\\n**The usable capacity of each cluster depends on the durability technique used. We currently employ erasure coding with\\neach object broken down into eight data and three coding fragments. This mechanism, called 8/3 erasure coding, can\\ntolerate up to three simultaneous server and/or disk failures with about 30% storage overhead for durability. This is\\nmuch lower than the 200% overhead in case of replication**.\\n\\nThe two durability techniques offer different price points and latency characteristics. Replication offers lower latency\\nbut a higher cost, whereas erasure coding reduces cost (sometimes by up to 50%)  at a slightly higher latency. We can\\nalso deploy different storage media such as SSD, HDD and Shingled Magnetic Recording (SMR) drives to enable different\\nservice levels depending on the application.\\n\\nTechnically, it is possible to scale a COS supercluster by adding storage needs to increase the capacity of the\\ncomponent clusters. However, this will lead to rebalancing of data within the component clusters, thereby creating\\nprolonged disk and network activity and impact latency SLA. To scale COS, our preferred approach is to add COS\\nsuperclusters as needed similar to adding storage farms. This approach is consistent with our current appliance-based\\nstorage solution that applications are already familiar with.\\n\\n## Latency Optimizations\\n\\nCOS is in the serving path for many Yahoo applications and has to guarantee latency SLAs to ensure consistent high\\nquality of user experience. We have implemented over 40 optimizations in Ceph to realize 50% improvement on average, and\\n70% improvement in 99.99% latency. The figure below depicts the latency chart before and after the optimizations under\\nnormal operations. The latencies in this chart are measured across objects of different sizes in the Flickr workload.\\n\\n![Error loading ceph-optimization-at-yahoo.png](ceph-optimization-at-yahoo.png)\\n\\nSome of the major optimizations are:\\n\\n- **Redundant parallel reads with erasure coding**: Currently, we have deployed 8/3 erasure coding scheme for\\n  durability. Increasing the parallel reads to 11 chunks, instead of the default 8 employed in Ceph, and reconstructing\\n  the object upon first 8 retrievals provided significant improvement in long tail read latency. This reduced average\\n  latency by approximately 40%.\\n- **Recovery Throttling**: Upon disk and node failures, Ceph automatically initiates recovery to maintain high\\n  durability of objects. During recovery, storage nodes are busy leading to high read/write latency. We implemented\\n  tunable recovery throttle rate to mitigate this impact. This reduce average latency during recovery by approximately\\n  60%.\\n- **Bucket Sharding**: Amazon S3 API specification requires objects to be bucketized. Ceph implements bucket as an\\n  object hosted on a single storage node. At our scale, the storage node that hosts the bucket becomes a hotspot, which\\n  we mitigated by implementing sharded buckets that are spread across multiple nodes.\\n\\nFuture Development\\n------------------\\n\\nSo far, Yahoo has tuned COS to a large Yahoo use-case, namely Flickr. However, other Yahoo use cases require object\\nstorage with different workload patterns and different tradeoffs. To make COS a widely used platform at Yahoo, several\\nenhancements in near to mid-term are\\n\\n- **Scale**: Yahoo has already deployed an initial multi-petabyte solution planned to grow this 10-fold or more to\\n  accommodate other use cases  such as Mail, Video, Tumblr etc. along with Flickr growth.\\n- **Geo Replication for Business Continuity**: Currently, geo replication is carried out at the application level. Ceph\\n  supports Geo-replication. However, Yahoo has not tested this capability for the scale and latency that Yahoo needs and\\n  planned to scale and deploy geo-replication in COS.\\n- **Optimize latency for small objects**: Many use-cases such as serving thumbnails and serving during image search have\\n  small objects of the order of a few kilobytes. COS needs to be tunned for these use-cases.\\n- **Lifecycle management**: One of the big advantages of Software Defined Storage is the hardware, software choices for\\n  cost and performance tradeoffs. Automatic classification of objects into hot, warm, and cold objects will allow us to\\n  take advantage of that flexibility and provide differentiated services."},{"id":"ssh-ascii-banner","metadata":{"permalink":"/blog/ssh-ascii-banner","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-04-30-ssh-ascii-banner/index.md","source":"@site/blog/2022-04-30-ssh-ascii-banner/index.md","title":"Add Custom ASCII Banner Logo to SSH Login Screen","description":"In the early 90\'s ASCII art became a thing. It was a way to make logos using regular ASCII characters to decorate readme","date":"2022-04-30T00:00:00.000Z","formattedDate":"April 30, 2022","tags":[{"label":"DevOps","permalink":"/blog/tags/dev-ops"}],"readingTime":1.17,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"},{"name":"Mike Andreasen","url":"https://guides.wp-bullet.com/add-custom-ascii-banner-logo-to-ssh-login/","imageURL":"https://secure.gravatar.com/avatar/0b12d8ba68100a5d983170f913d20d5e?s=100&d=mm&r=g","key":"mike-andreasen"}],"frontMatter":{"slug":"ssh-ascii-banner","title":"Add Custom ASCII Banner Logo to SSH Login Screen","authors":["jiaqi","mike-andreasen"],"tags":["DevOps"]},"unlisted":false,"prevItem":{"title":"Yahoo Cloud Object Store - Object Storage at Exabyte Scale","permalink":"/blog/yahoo-object-storage"},"nextItem":{"title":"Trunk-Based Development","permalink":"/blog/trunk-based-deployment"}},"content":"In the early 90\'s ASCII art became a thing. It was a way to make logos using regular ASCII characters to decorate readme\\nfiles and add some branding. Nowadays we can generate these banners using free tools like Text to ASCII Art Generator.\\nIt can be used to create a server login banner that is displayed each time people log in via SSH.\\n\\n\x3c!--truncate--\x3e\\n\\n![./example.png](example.png)\\n\\nFirst generate a ASCII logo with the [Text to ASCII Art Generator]. Then open up message of the day file\\n\\n```bash\\nsudo nano /etc/motd\\n```\\n\\nAdd a text such as\\n\\n```bash\\nThe programs included with the Debian GNU/Linux system are free software;\\nthe exact distribution terms for each program are described in the\\nindividual files in /usr/share/doc/*/copyright.\\n\\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\\npermitted by applicable law.\\n\\n\\n______     _              ______      _\\n| ___ \\\\   (_)             |  _  \\\\    | |\\n| |_/ /_ _ _  ___  _ __   | | | |__ _| |_ __ _\\n|  __/ _` | |/ _ \\\\| \'_ \\\\  | | | / _` | __/ _` |\\n| | | (_| | | (_) | | | | | |/ / (_| | || (_| |\\n\\\\_|  \\\\__,_|_|\\\\___/|_| |_| |___/ \\\\__,_|\\\\__\\\\__,_|\\n```\\n\\nSave the file and open a new SSH session and log in, we should be greeted with our beautiful banner\\n\\n[Text to ASCII Art Generator]: http://patorjk.com/software/taag/"},{"id":"trunk-based-deployment","metadata":{"permalink":"/blog/trunk-based-deployment","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2022-04-13-trunk-deployment.md","source":"@site/blog/2022-04-13-trunk-deployment.md","title":"Trunk-Based Development","description":"Trunk-based development is a version control management practice where developers merge small, frequent updates to a","date":"2022-04-13T00:00:00.000Z","formattedDate":"April 13, 2022","tags":[{"label":"CI/CD","permalink":"/blog/tags/ci-cd"}],"readingTime":7.5,"hasTruncateMarker":true,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"trunk-based-deployment","title":"Trunk-Based Development","authors":["jiaqi"],"tags":["CI/CD"]},"unlisted":false,"prevItem":{"title":"Add Custom ASCII Banner Logo to SSH Login Screen","permalink":"/blog/ssh-ascii-banner"},"nextItem":{"title":"Amazon S3 concepts","permalink":"/blog/aws-s3-concepts"}},"content":"Trunk-based development is a version control management practice where developers merge small, frequent updates to a\\ncore \\"trunk\\" or main branch. Since it streamlines merging and integration phases, it helps achieve CI/CD and increases\\nsoftware delivery and organizational performance.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the early days of software development, programmers didn\'t have the luxury of modern version control systems. Rather,\\nthey developed two versions of their software concurrently as a means of tracking changes and reversing them if\\nnecessary. Over time, this process proved to be labor-intensive, costly, and inefficient.\\n\\nAs version control systems matured, various development styles emerged, enabling programmers to find bugs more easily,\\ncode in parallel with their colleagues, and accelerate release cadence. Today, most programmers leverage one of two\\ndevelopment models to deliver quality software\\n\\n1. Gitflow\\n2. trunk-based development\\n\\nGitflow, which was popularized first, is a stricter development model where only certain individuals can approve changes\\nto the main code. This maintains code quality and minimizes the number of bugs. Trunk-based development is a more open\\nmodel since all developers have access to the main code. This enables teams to iterate quickly and implement\\n[CI/CD](https://www.atlassian.com/continuous-delivery).\\n\\nWhat is Trunk-Based Development\\n-------------------------------\\n\\nTrunk-based development is a\\n[version control management](https://www.atlassian.com/git/tutorials/what-is-version-control) practice where developers\\nmerge small, frequent updates to a  core \\"trunk\\" or main branch. It\'s a common practice among\\n[DevOps](https://www.atlassian.com/devops/what-is-devops) teams and part of the\\n[DevOps lifecycle](https://www.atlassian.com/devops/what-is-devops/devops-best-practices) since it streamlines merging\\nand integration phases. In fact, trunk-based development is a required practice of CI/CD. Developers can create\\nshort-lived branches with a few small commits compared to other long-lived feature branching strategies. As codebase\\ncomplexity and team size grow, trunk-based development helps keep production releases flowing.\\n\\nGitflow vs. Trunk-Based Development\\n-----------------------------------\\n\\n[Gitflow](https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow) is an alternative Git branching\\nmodel that uses long-lived feature branches and multiple primary branches. Gitflow has more, longer-lived branches and\\nlarger commits than trunk-based development. Under this model, developers create a feature branch and delay merging it\\nto the main trunk branch until the feature is complete. These long-lived feature branches require more collaboration to\\nmerge as they have a higher risk of deviating from the trunk branch and introducing conflicting updates.\\n\\nGitflow also has separate primary branch lines for development, hotfixes, features, and releases. There are different\\nstrategies for merging commits between these branches. Since there are more branches to juggle and manage, there is\\noften more complexity that requires additional planning sessions and review from the team.\\n\\nTrunk-based development is far more simplified since it focuses on the main branch as the source of fixes and releases.\\nIn trunk-based development the main branch is assumed to always be stable, without issues, and ready to deploy.\\n\\nBenefits of Trunk-Based Development\\n-----------------------------------\\n\\nTrunk-based development is a required practice for\\n[continuous integration](https://www.atlassian.com/continuous-delivery/continuous-integration). If build and test\\nprocesses are automated but developers work on isolated, lengthy feature branches that are infrequently integrated into\\na shared branch, continuous integration is not living up to its potential.\\n\\nTrunk-based development eases the friction of code integration. When developers finish new work, they must merge the new\\ncode into the main branch. Yet they should not merge changes to the truck until they have verified that they can build\\nsuccessfully. During this phase, conflicts may arise if modifications have been made since the new work began. In\\nparticular, these conflicts are increasingly complex as development teams grow and the code base scales. This happens\\nwhen developers create separate branches that deviate from the source branch and other developers are simultaneously\\nmerging overlapping code. Luckily, the trunk-based development model reduces these conflicts.\\n\\n### Allows Continuous Code Integration\\n\\nIn the trunk-based development model, there is a repository with a steady stream of commits flowing into the main\\nbranch. Adding an automated test suite and code coverage monitoring for this stream of commits enables continuous\\nintegration. When new code is merged into the trunk, automated integration and code coverage tests run to validate the\\ncode quality.\\n\\n### Ensures Continuous Code Review\\n\\nThe rapid, small commits of trunk-based development make code review a more efficient process. With small branches,\\ndevelopers can quickly see and review small changes. This is far easier compared to a long-lived feature branch where a\\nreviewer reads pages of code or manually inspects a large surface area of code changes.\\n\\n### Enables Consecutive Production Code Releases\\n\\nTeams should make frequent, daily merges to the main branch. Trunk-based development strives to keep the trunk branch\\n\\"green\\", meaning it\'s ready to deploy at any commit. Automated tests, code converge, and code reviews provides a\\ntrunk-based development project with the assurances it\'s ready to deploy to production at any time. This gives team\\nagility to frequently deploy to production and set further goals of daily production releases.\\n\\n:::info\\n\\nAs CI/CD grew in popularity, branching models were refined and optimized, leading to the rise of trunk-based\\ndevelopment. Now, trunk-based development is a requirement of continuous integration. With continuous integration,\\ndevelopers perform trunk-based development in conjunction with automated tests that run after each committee to a\\ntrunk. This ensures the project works at all times.\\n\\n:::\\n\\nTrunk-Based Development Best Practices\\n--------------------------------------\\n\\nTrunk-based development ensures teams release code quickly and consistently. The following is a list of exercises and\\npractices that will help refine your team\'s cadence and develop an optimized release schedule.\\n\\n### Develop in Small Batches\\n\\nTrunk-based development follows a quick rhythm to deliver code to production. If trunk-based development was like music\\nit would be a rapid staccato -- short, succinct notes in rapid succession, with the repository commits being the notes.\\n**Keeping commits and branches small** allows for a more rapid tempo of merges and deployments.\\n\\nSmall changes of a couple of commits or modification of a few lines of code minimize cognitive overhead. It\'s much\\neasier for teams to have meaningful conversations and make quick decisions when reviewing a limited area of code versus\\na sprawling set of changes.\\n\\n### Feature Flags\\n\\nFeature flags nicely compliment trunk-based development by enabling developers to wrap new changes in an inactive code\\npath and activate it at a later time. This allows developers to forgo creating a separate repository feature branch and\\ninstead commit new feature code directly to the main branch within a feature flag path.\\n\\n### Implement Comprehensive Automated Testing\\n\\n**Automated testing is necessary for any modern software project intending to achieve CI/CD**. There are multiple types\\nof automated tests that run at different stages of the release pipeline. Short running unit and integration tests are\\nexecuted during development and upon code merge. Longer running, full stack, end-to-end tests are run in later pipeline\\nphases against a full staging or production environment.\\n\\nAutomated tests help trunk-based development by maintaining a small batch rhythm as developers merge new commits. The\\nautomated test suite reviews the code for any issues and automatically approves or denies it. This helps developers\\nrapidly create commits and run them through automated tests to see if they introduce any new issues.\\n\\n### Perform Asynchronous Code Reviews\\n\\nIn trunk-based development, code review should be performed immediately and not put into an asynchronous system for\\nlater review. Automated tests provide a layer of preemptive code review. When developers are ready to review a team\\nmember\'s pull request, they can first check that the automated tests passed and the code coverage has increased. This\\ngives the reviewer immediate reassurance that the new code meets certain specifications. The reviewer can then focus on\\noptimizations.\\n\\n### Have Three or Fewer Active Branches in the Application\'s Code Repository\\n\\nOnce a branch merges, it is best practice to delete it. **A repository with a large amount of active branches has some\\nunfortunate side effects**. While it can be beneficial for teams to see what work is in progress by examining active\\nbranches, this benefit is lost if there are stale and inactive branches still around. Some developers use Git user\\ninterfaces that may become unwieldy to work with when loading a large number of remote branches.\\n\\n### Merge Branches to the Trunk at Least Once a Day\\n\\nHigh-performing, trunk-based development teams should close out and merge any open and merge-ready branches at least on\\na daily basis. This exercise helps keep rhythm and sets a cadence for release tracking. The team can then tag the main\\ntrunk at the end of day as a release commit, which has the helpful side effect of generating a daily agile release\\nincrement.\\n\\n### Reduced Number of Code Freezes and Integration Phases\\n\\n**Agile CI/CD teams shouldn\'t need planned code freezes or pauses for integration phases** -- although an organization\\nmay need them for other reasons. The \\"continuous\\" in CI/CD implies that updates are constantly flowing. Trunk-based\\ndevelopment teams should try to avoid blocking code freezes and plan accordingly to ensure the release pipeline is not\\nstalled.\\n\\n### Build Fast and Execute Immediately\\n\\nIn order to maintain a quick release cadence, build and test execution times should be optimized. CI/CD build tools\\nshould use caching layers where appropriate to avoid expensive computations for static. **Tests should be optimized to\\nuse appropriate stubs for third-party services**."},{"id":"aws-s3-concepts","metadata":{"permalink":"/blog/aws-s3-concepts","editUrl":"https://github.com/QubitPi/hashicorp-aws/tree/master/docs/blog/2020-09-19-amazon-s3/index.md","source":"@site/blog/2020-09-19-amazon-s3/index.md","title":"Amazon S3 concepts","description":"Buckets","date":"2020-09-19T00:00:00.000Z","formattedDate":"September 19, 2020","tags":[{"label":"AWS","permalink":"/blog/tags/aws"}],"readingTime":7.445,"hasTruncateMarker":false,"authors":[{"name":"Jiaqi Liu","title":"Maintainer of hashicorp-aws","url":"https://github.com/QubitPi","imageURL":"https://avatars.githubusercontent.com/u/16126939?v=4","key":"jiaqi"}],"frontMatter":{"slug":"aws-s3-concepts","title":"Amazon S3 concepts","authors":["jiaqi"],"tags":["AWS"]},"unlisted":false,"prevItem":{"title":"Trunk-Based Development","permalink":"/blog/trunk-based-deployment"}},"content":"Buckets\\n-------\\n\\nA bucket is a container for objects stored in Amazon S3. Every object is contained in a bucket. For example, if the\\nobject named `photos/puppy.jpg` is stored in the `awsexamplebucket1` bucket in the US West (Oregon) Region, then it\\nis addressable using the URL `https://awsexamplebucket1.s3.us-west-2.amazonaws.com/photos/puppy.jpg`.\\n\\nBuckets serve several purposes:\\n\\n- They organize the Amazon S3 namespace at the highest level.\\n- They identify the account responsible for storage and data transfer charges.\\n- They play a role in access control.\\n- They serve as the unit of aggregation for usage reporting.\\n\\nYou can configure buckets so that they are created in a specific AWS Region. For more information, see\\n[Accessing a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro). You can\\nalso configure a bucket so that every time an object is added to it, Amazon S3 generates a unique version ID and assigns\\nit to the object. For more information, see\\n[Using Versioning](https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html).\\n\\nFor more information about buckets, see\\n[Working with Amazon S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html).\\n\\nObjects\\n-------\\n\\nObjects are the fundamental entities stored in Amazon S3. Objects consist of object data and metadata. The data portion\\nis opaque to Amazon S3. The metadata is a set of name-value pairs that describe the object. These include some default\\nmetadata, such as the date last modified, and standard HTTP metadata, such as `Content-Type`. You can also specify\\ncustom metadata at the time the object is stored.\\n\\nAn object is uniquely identified within a bucket by a key (name) and a version ID. For more information, see\\n[Keys](https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#BasicsKeys) and\\n[Using Versioning](https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html).\\n\\nKeys\\n----\\n\\nA key is the unique identifier for an object within a bucket. Every object in a bucket has exactly one key. The\\ncombination of a bucket, key, and version ID uniquely identify each object. So you can think of Amazon S3 as a basic\\ndata map between \\"bucket + key + version\\" and the object itself. Every object in Amazon S3 can be uniquely addressed\\nthrough the combination of the web service endpoint, bucket name, key, and optionally, a version. For example, in the\\nURL `https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl`, \\"`doc`\\" is the name of the bucket and\\n\\"`2006-03-01/AmazonS3.wsdl`\\" is the key.\\n\\nFor more information about object keys, see\\n[Object Keys](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-keys).\\n\\nRegions\\n-------\\n\\nYou can choose the geographical AWS Region where Amazon S3 will store the buckets that you create. You might choose a\\nRegion to optimize latency, minimize costs, or address regulatory requirements. Objects stored in a Region never leave\\nthe Region unless you explicitly transfer them to another Region. For example, objects stored in the Europe (Ireland)\\nRegion never leave it.\\n\\nAmazon S3 Data Consistency Model\\n--------------------------------\\n\\nAmazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one\\ncaveat. The caveat is that if you make a HEAD or GET request to a key name before the object is created, then create\\nthe object shortly after that, a subsequent GET might not return the object due to eventual consistency.\\n\\nAmazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions.\\n\\n**Updates to a single key are atomic**. For example, if you PUT to an existing key, a subsequent read might return the\\nold data or the updated data, but it never returns corrupted or partial data.\\n\\nAmazon S3 achieves high availability by replicating data across multiple servers within AWS data centers. If a PUT\\nrequest is successful, your data is safely stored. However, information about the changes must replicate across Amazon\\nS3, which can take some time, and so you might observe the following behaviors:\\n\\n- A process writes a new object to Amazon S3 and immediately lists keys within its bucket. Until the change is fully\\n  propagated, the object might not appear in the list.\\n- A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon\\n  S3 might return the previous data.\\n- A process deletes an existing object and immediately tries to read it. Until the deletion is fully propagated, Amazon\\n  S3 might return the deleted data.\\n- A process deletes an existing object and immediately lists keys within its bucket. Until the deletion is fully\\n  propagated, Amazon S3 might list the deleted object.\\n\\n**Updates are key-based. There is no way to make atomic updates across keys**. For example, you cannot make the update\\nof one key dependent on the update of another key unless you design this functionality into your application.\\n\\nBuckets have a similar consistency model, with the same caveats. For example, if you delete a bucket and immediately\\nlist all buckets, Amazon S3 might still appear in the list.\\n\\nThe following table describes the characteristics of an eventually consistent read and a consistent read:\\n\\n| Eventually consistent read |         Consistent read         |\\n|:--------------------------:|:-------------------------------:|\\n| Stale reads possible       | No stale reads                  |\\n| Lowest read latency        | Potential higher read latency   |\\n| Highest read throughput    | Potential lower read throughput |\\n\\n### Concurrent applications\\n\\nIn this example, both W1 (write 1) and W2 (write 2) complete before the start of R1 (read 1) and R2 (read 2). For a\\nconsistent read, R1 and R2 both return `color = ruby`. For an eventually consistent read, R1 and R2 might return\\n`color = red` or `color = ruby` depending on the amount of time that has elapsed.\\n\\n![Consistency Example 1](./consistency1.png)\\n\\nIn the next example, W2 does not complete before the start of R1. Therefore, R1 might return `color = ruby` or\\n`color = garnet` for either a consistent read or an eventually consistent read. Also, depending on the amount of time\\nthat has elapsed, an eventually consistent read might return no results.\\n\\nFor a consistent read, R2 returns `color = garnet`. For an eventually consistent read, R2 might return\\n`color = ruby` or color = garnet depending on the amount of time that has elapsed.\\n\\n![Consistency Example 2](./consistency2.png)\\n\\nIn the last example, Client 2 performs W2 before Amazon S3 returns a success for W1, so the outcome of the final value\\nis unknown (`color = garnet` or `color = brick`). Any subsequent reads (consistent read or eventually consistent)\\nmight return either value. Also, depending on the amount of time that has elapsed, an eventually consistent read might\\nreturn no results.\\n\\n![Consistency Example 3](./consistency3.png)\\n\\n## Prefix - Listing Keys Hierarchically Using a Prefix and Delimiter\\n\\nA \\"prefix\\", or \\"key prefix\\" is a **logical** grouping of the objects in a bucket. The prefix value is similar to a\\ndirectory name that enables you to store similar data under the same directory in a bucket. **Note that Amazon S3 does\\nnot have the concept of \\"directory\\"**. There is only objects whose keys are prefixed in a way that looks like a\\n\\"directory\\". For example\\n\\n```console\\nzoo/west-side/fish/little-fish.png\\n```\\n\\nThere is no such directories called `zoo`, `west-side`, `fish`. In this case the :ref:`key <concept-keys>` for the\\nimage `little-fish.png` is `zoo/west-side/fish/`, a string essentially.\\n\\nThe prefix and delimiter parameters limit the kind of results returned by a list operation. The prefix limits the\\nresults to only those keys that begin with the specified prefix. The delimiter causes a list operation to roll up all\\nthe keys that share a common prefix into a single summary list result.\\n\\nThe purpose of the prefix and delimiter parameters is to help you organize and then browse your keys hierarchically. To\\ndo this, first pick a delimiter for your bucket, such as slash (/), that doesn\'t occur in any of your anticipated key\\nnames. Next, construct your key names by concatenating all containing levels of the hierarchy, separating each level\\nwith the delimiter.\\n\\nFor example, if you were storing information about cities, you might naturally organize them by continent, then by\\ncountry, then by state. Because these names do not usually contain punctuation, you might select slash (/) as the\\ndelimiter. The following examples use a slash (/) delimiter.\\n\\n- Europe/France/Nouvelle-Aquitaine/Bordeaux\\n- North America/Canada/Quebec/Montreal\\n- North America/USA/Washington/Bellevue\\n- North America/USA/Washington/Seattle\\n\\nBy using `Prefix` and `Delimiter` with the list operation, you can use the hierarchy you\'ve created to list your\\ndata. For example, to list all the states in USA, set `Delimiter=\'/\'` and `Prefix=\'North America/USA/\'`.\\n\\nA list request with a delimiter lets you browse your hierarchy at just one level, skipping over and summarizing the\\n(possibly millions of) keys nested at deeper levels. For example, assume you have a bucket (`ExampleBucket`) with the\\nfollowing keys.\\n\\n```console\\nsample.jpg\\n\\nphotos/2006/January/sample.jpg\\n\\nphotos/2006/February/sample2.jpg\\n\\nphotos/2006/February/sample3.jpg\\n\\nphotos/2006/February/sample4.jpg\\n```\\n\\nThe sample bucket has only the `sample.jpg` object at the root level. To list only the root level objects in the\\nbucket you send a GET request on the bucket with `\\"/\\"` delimiter character. In response, Amazon S3 returns the\\n`sample.jpg` object key because it does not contain the `\\"/\\"` delimiter character. All other keys contain the\\ndelimiter character. Amazon S3 groups these keys and return a single `CommonPrefixes` element with prefix value\\n`photos/` that is a substring from the beginning of these keys to the first occurrence of the specified delimiter."}]}')}}]);